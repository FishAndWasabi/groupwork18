futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,	int ret = 0, cmp;
	u32 prev;

	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

	__asm__ __volatile__ (

#define __access_ok(addr, size) \
	((get_fs().seg & (addr | size | (addr+size))) == 0)

#define access_ok(type, addr, size)			\
#define access_ok(addr, size)				\
({							\
	__chk_user_ptr(addr);				\
	__access_ok(((unsigned long)(addr)), (size));	\

SYSCALL_DEFINE3(osf_sigaction, int, sig,
	if (act) {
		old_sigset_t mask;
		if (!access_ok(VERIFY_READ, act, sizeof(*act)) ||
		if (!access_ok(act, sizeof(*act)) ||
		    __get_user(new_ka.sa.sa_handler, &act->sa_handler) ||
		    __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||
		    __get_user(mask, &act->sa_mask))

SYSCALL_DEFINE3(osf_sigaction, int, sig,	ret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);

	if (!ret && oact) {
		if (!access_ok(VERIFY_WRITE, oact, sizeof(*oact)) ||
		if (!access_ok(oact, sizeof(*oact)) ||
		    __put_user(old_ka.sa.sa_handler, &oact->sa_handler) ||
		    __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||
		    __put_user(old_ka.sa.sa_mask.sig[0], &oact->sa_mask))

do_sigreturn(struct sigcontext __user *sc)	sigset_t set;

	/* Verify that it's a good sigcontext before using it */
	if (!access_ok(VERIFY_READ, sc, sizeof(*sc)))
	if (!access_ok(sc, sizeof(*sc)))
		goto give_sigsegv;
	if (__get_user(set.sig[0], &sc->sc_mask))
		goto give_sigsegv;

do_rt_sigreturn(struct rt_sigframe __user *frame)	sigset_t set;

	/* Verify that it's a good ucontext_t before using it */
	if (!access_ok(VERIFY_READ, &frame->uc, sizeof(frame->uc)))
	if (!access_ok(&frame->uc, sizeof(frame->uc)))
		goto give_sigsegv;
	if (__copy_from_user(&set, &frame->uc.uc_sigmask, sizeof(set)))
		goto give_sigsegv;

setup_frame(struct ksignal *ksig, sigset_t *set, struct pt_regs *regs)
	oldsp = rdusp();
	frame = get_sigframe(ksig, oldsp, sizeof(*frame));
	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	err |= setup_sigcontext(&frame->sc, regs, set->sig[0], oldsp);

setup_rt_frame(struct ksignal *ksig, sigset_t *set, struct pt_regs *regs)
	oldsp = rdusp();
	frame = get_sigframe(ksig, oldsp, sizeof(*frame));
	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	err |= copy_siginfo_to_user(&frame->info, &ksig->info);

csum_partial_copy_from_user(const void __user *src, void *dst, int len,	unsigned long doff = 7 & (unsigned long) dst;

	if (len) {
		if (!access_ok(VERIFY_READ, src, len)) {
		if (!access_ok(src, len)) {
			if (errp) *errp = -EFAULT;
			memset(dst, 0, len);
			return sum;

futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr, u32 expval,	int ret = 0;
	u32 existval;

	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

#ifndef CONFIG_ARC_HAS_LLSC

SYSCALL_DEFINE3(arc_usr_cmpxchg, int *, uaddr, int, expected, int, new)	/* Z indicates to userspace if operation succeded */
	regs->status32 &= ~STATUS_Z_MASK;

	ret = access_ok(VERIFY_WRITE, uaddr, sizeof(*uaddr));
	ret = access_ok(uaddr, sizeof(*uaddr));
	if (!ret)
		 goto fail;


SYSCALL_DEFINE0(rt_sigreturn)
	sf = (struct rt_sigframe __force __user *)(regs->sp);

	if (!access_ok(VERIFY_READ, sf, sizeof(*sf)))
	if (!access_ok(sf, sizeof(*sf)))
		goto badframe;

	if (__get_user(magic, &sf->sigret_magic))

static inline void __user *get_sigframe(struct ksignal *ksig,	frame = (void __user *)((sp - framesize) & ~7);

	/* Check that we can actually write to the signal frame */
	if (!access_ok(VERIFY_WRITE, frame, framesize))
	if (!access_ok(frame, framesize))
		frame = NULL;

	return frame;

futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,	int ret;
	u32 val;

	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

	smp_mb();

futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,	int ret = 0;
	u32 val;

	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

	preempt_disable();

static inline void set_fs(mm_segment_t fs)
#endif /* CONFIG_MMU */

#define access_ok(type, addr, size)	(__range_ok(addr, size) == 0)
#define access_ok(addr, size)	(__range_ok(addr, size) == 0)

#define user_addr_max() \
	(uaccess_kernel() ? ~0UL : get_fs())

raw_copy_to_user(void __user *to, const void *from, unsigned long n)
static inline unsigned long __must_check clear_user(void __user *to, unsigned long n)
{
	if (access_ok(VERIFY_WRITE, to, n))
	if (access_ok(to, n))
		n = __clear_user(to, n);
	return n;
}

user_backtrace(struct frame_tail __user *tail,	struct frame_tail buftail;
	unsigned long err;

	if (!access_ok(VERIFY_READ, tail, sizeof(buftail)))
	if (!access_ok(tail, sizeof(buftail)))
		return NULL;

	pagefault_disable();

asmlinkage int sys_sigreturn(struct pt_regs *regs)
	frame = (struct sigframe __user *)regs->ARM_sp;

	if (!access_ok(VERIFY_READ, frame, sizeof (*frame)))
	if (!access_ok(frame, sizeof (*frame)))
		goto badframe;

	if (restore_sigframe(regs, frame))

asmlinkage int sys_rt_sigreturn(struct pt_regs *regs)
	frame = (struct rt_sigframe __user *)regs->ARM_sp;

	if (!access_ok(VERIFY_READ, frame, sizeof (*frame)))
	if (!access_ok(frame, sizeof (*frame)))
		goto badframe;

	if (restore_sigframe(regs, &frame->sig))

get_sigframe(struct ksignal *ksig, struct pt_regs *regs, int framesize)	/*
	 * Check that we can actually write to the signal frame.
	 */
	if (!access_ok(VERIFY_WRITE, frame, framesize))
	if (!access_ok(frame, framesize))
		frame = NULL;

	return frame;

static int swp_handler(struct pt_regs *regs, unsigned int instr)		 destreg, EXTRACT_REG_NUM(instr, RT2_OFFSET), data);

	/* Check access in reasonable access range for both SWP and SWPB */
	if (!access_ok(VERIFY_WRITE, (address & ~3), 4)) {
	if (!access_ok((address & ~3), 4)) {
		pr_debug("SWP{B} emulation: access to %p not allowed!\n",
			 (void *)address);
		res = -EFAULT;

asmlinkage long sys_oabi_epoll_wait(int epfd,			maxevents > (INT_MAX/sizeof(*kbuf)) ||
			maxevents > (INT_MAX/sizeof(*events)))
		return -EINVAL;
	if (!access_ok(VERIFY_WRITE, events, sizeof(*events) * maxevents))
	if (!access_ok(events, sizeof(*events) * maxevents))
		return -EFAULT;
	kbuf = kmalloc_array(maxevents, sizeof(*kbuf), GFP_KERNEL);
	if (!kbuf)

asmlinkage long sys_oabi_semtimedop(int semid,
	if (nsops < 1 || nsops > SEMOPM)
		return -EINVAL;
	if (!access_ok(VERIFY_READ, tsops, sizeof(*tsops) * nsops))
	if (!access_ok(tsops, sizeof(*tsops) * nsops))
		return -EFAULT;
	sops = kmalloc_array(nsops, sizeof(*sops), GFP_KERNEL);
	if (!sops)

do_cache_op(unsigned long start, unsigned long end, int flags)	if (end < start || flags)
		return -EINVAL;

	if (!access_ok(VERIFY_READ, start, end - start))
	if (!access_ok(start, end - start))
		return -EFAULT;

	return __do_cache_op(start, end);

static struct frame_tail* user_backtrace(struct frame_tail *tail)	struct frame_tail buftail[2];

	/* Also check accessibility of one struct frame_tail beyond */
	if (!access_ok(VERIFY_READ, tail, sizeof(buftail)))
	if (!access_ok(tail, sizeof(buftail)))
		return NULL;
	if (__copy_from_user_inatomic(buftail, tail, sizeof(buftail)))
		return NULL;

futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *_uaddr,	u32 val, tmp;
	u32 __user *uaddr;

	if (!access_ok(VERIFY_WRITE, _uaddr, sizeof(u32)))
	if (!access_ok(_uaddr, sizeof(u32)))
		return -EFAULT;

	uaddr = __uaccess_mask_ptr(_uaddr);

static inline unsigned long __range_ok(const void __user *addr, unsigned long si	return ret;
}

#define access_ok(type, addr, size)	__range_ok(addr, size)
#define access_ok(addr, size)	__range_ok(addr, size)
#define user_addr_max			get_fs

#define _ASM_EXTABLE(from, to)						\

do {									\({									\
	__typeof__(*(ptr)) __user *__p = (ptr);				\
	might_fault();							\
	if (access_ok(VERIFY_READ, __p, sizeof(*__p))) {		\
	if (access_ok(__p, sizeof(*__p))) {				\
		__p = uaccess_mask_ptr(__p);				\
		__get_user_err((x), __p, (err));			\
	} else {							\

do {									\({									\
	__typeof__(*(ptr)) __user *__p = (ptr);				\
	might_fault();							\
	if (access_ok(VERIFY_WRITE, __p, sizeof(*__p))) {		\
	if (access_ok(__p, sizeof(*__p))) {				\
		__p = uaccess_mask_ptr(__p);				\
		__put_user_err((x), __p, (err));			\
	} else	{							\

extern unsigned long __must_check __arch_copy_in_user(void __user *to, const voiextern unsigned long __must_check __arch_clear_user(void __user *to, unsigned long n);
static inline unsigned long __must_check __clear_user(void __user *to, unsigned long n)
{
	if (access_ok(VERIFY_WRITE, to, n))
	if (access_ok(to, n))
		n = __arch_clear_user(__uaccess_mask_ptr(to), n);
	return n;
}

static int swp_handler(struct pt_regs *regs, u32 instr)
	/* Check access in reasonable access range for both SWP and SWPB */
	user_ptr = (const void __user *)(unsigned long)(address & ~3);
	if (!access_ok(VERIFY_WRITE, user_ptr, 4)) {
	if (!access_ok(user_ptr, 4)) {
		pr_debug("SWP{B} emulation: access to 0x%08x not allowed!\n",
			address);
		goto fault;

user_backtrace(struct frame_tail __user *tail,	unsigned long lr;

	/* Also check accessibility of one struct frame_tail beyond */
	if (!access_ok(VERIFY_READ, tail, sizeof(buftail)))
	if (!access_ok(tail, sizeof(buftail)))
		return NULL;

	pagefault_disable();

compat_user_backtrace(struct compat_frame_tail __user *tail,	unsigned long err;

	/* Also check accessibility of one struct frame_tail beyond */
	if (!access_ok(VERIFY_READ, tail, sizeof(buftail)))
	if (!access_ok(tail, sizeof(buftail)))
		return NULL;

	pagefault_disable();

static int parse_user_sigframe(struct user_ctxs *user,			offset = 0;
			limit = extra_size;

			if (!access_ok(VERIFY_READ, base, limit))
			if (!access_ok(base, limit))
				goto invalid;

			continue;

SYSCALL_DEFINE0(rt_sigreturn)
	frame = (struct rt_sigframe __user *)regs->sp;

	if (!access_ok(VERIFY_READ, frame, sizeof (*frame)))
	if (!access_ok(frame, sizeof (*frame)))
		goto badframe;

	if (restore_sigframe(regs, frame))

static int get_sigframe(struct rt_sigframe_user_layout *user,	/*
	 * Check that we can actually write to the signal frame.
	 */
	if (!access_ok(VERIFY_WRITE, user->sigframe, sp_top - sp))
	if (!access_ok(user->sigframe, sp_top - sp))
		return -EFAULT;

	return 0;

COMPAT_SYSCALL_DEFINE0(sigreturn)
	frame = (struct compat_sigframe __user *)regs->compat_sp;

	if (!access_ok(VERIFY_READ, frame, sizeof (*frame)))
	if (!access_ok(frame, sizeof (*frame)))
		goto badframe;

	if (compat_restore_sigframe(regs, frame))

COMPAT_SYSCALL_DEFINE0(rt_sigreturn)
	frame = (struct compat_rt_sigframe __user *)regs->compat_sp;

	if (!access_ok(VERIFY_READ, frame, sizeof (*frame)))
	if (!access_ok(frame, sizeof (*frame)))
		goto badframe;

	if (compat_restore_sigframe(regs, &frame->sig))

static void __user *compat_get_sigframe(struct ksignal *ksig,	/*
	 * Check that we can actually write to the signal frame.
	 */
	if (!access_ok(VERIFY_WRITE, frame, framesize))
	if (!access_ok(frame, framesize))
		frame = NULL;

	return frame;

do_compat_cache_op(unsigned long start, unsigned long end, int flags)	if (end < start || flags)
		return -EINVAL;

	if (!access_ok(VERIFY_READ, (const void __user *)start, end - start))
	if (!access_ok((const void __user *)start, end - start))
		return -EFAULT;

	return __do_compat_cache_op(start, end);

asmlinkage int do_rt_sigreturn(struct pt_regs *regs)
	frame = (struct rt_sigframe __user *) ((unsigned long) regs->sp + 8);

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__copy_from_user(&set, &frame->uc.uc_sigmask, sizeof(set)))
		goto badframe;

static int setup_rt_frame(struct ksignal *ksig, sigset_t *set,
	frame = get_sigframe(ksig, regs, sizeof(*frame));

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	err |= __put_user(&frame->info, &frame->pinfo);

static int ldb_asm(uint32_t addr, uint32_t *valp)	uint32_t val;
	int err;

	if (!access_ok(VERIFY_READ, (void *)addr, 1))
	if (!access_ok((void *)addr, 1))
		return 1;

	asm volatile (

static int stb_asm(uint32_t addr, uint32_t val){
	int err;

	if (!access_ok(VERIFY_WRITE, (void *)addr, 1))
	if (!access_ok((void *)addr, 1))
		return 1;

	asm volatile (

#include <linux/version.h>
#include <asm/segment.h>

#define VERIFY_READ	0
#define VERIFY_WRITE	1

static inline int access_ok(int type, const void *addr, unsigned long size)
static inline int access_ok(const void *addr, unsigned long size)
{
	unsigned long limit = current_thread_info()->addr_limit.seg;


static inline int access_ok(int type, const void *addr, unsigned long size)		((unsigned long)(addr + size) < limit));
}

static inline int verify_area(int type, const void *addr, unsigned long size)
{
	return access_ok(type, addr, size) ? 0 : -EFAULT;
}

#define __addr_ok(addr) (access_ok(VERIFY_READ, addr, 0))
#define __addr_ok(addr) (access_ok(addr, 0))

extern int __put_user_bad(void);


extern int __put_user_bad(void);	long __pu_err = -EFAULT;					\
	typeof(*(ptr)) *__pu_addr = (ptr);				\
	typeof(*(ptr)) __pu_val = (typeof(*(ptr)))(x);			\
	if (access_ok(VERIFY_WRITE, __pu_addr, size) && __pu_addr)	\
	if (access_ok(__pu_addr, size) && __pu_addr)	\
		__put_user_size(__pu_val, __pu_addr, (size), __pu_err);	\
	__pu_err;							\
})

do {								\({								\
	int __gu_err = -EFAULT;					\
	const __typeof__(*(ptr)) __user *__gu_ptr = (ptr);	\
	if (access_ok(VERIFY_READ, __gu_ptr, size) && __gu_ptr)	\
	if (access_ok(__gu_ptr, size) && __gu_ptr)	\
		__get_user_size(x, __gu_ptr, size, __gu_err);	\
	__gu_err;						\
})

do_rt_sigreturn(void)	struct pt_regs *regs = current_pt_regs();
	struct rt_sigframe *frame = (struct rt_sigframe *)(regs->usp);

	if (verify_area(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__copy_from_user(&set, &frame->uc.uc_sigmask, sizeof(set)))
		goto badframe;

unsigned long raw_copy_from_user(void *to, const void *from,
			unsigned long n)
{
	if (access_ok(VERIFY_READ, from, n))
	if (access_ok(from, n))
		__copy_user_zeroing(to, from, n);
	else
		memset(to, 0, n);

EXPORT_SYMBOL(raw_copy_from_user);unsigned long raw_copy_to_user(void *to, const void *from,
			unsigned long n)
{
	if (access_ok(VERIFY_WRITE, to, n))
	if (access_ok(to, n))
		__copy_user(to, from, n);
	return n;
}

long strncpy_from_user(char *dst, const char *src, long count){
	long res = -EFAULT;

	if (access_ok(VERIFY_READ, src, 1))
	if (access_ok(src, 1))
		__do_strncpy_from_user(dst, src, count, res);
	return res;
}

do {							\unsigned long
clear_user(void __user *to, unsigned long n)
{
	if (access_ok(VERIFY_WRITE, to, n))
	if (access_ok(to, n))
		__do_clear_user(to, n);
	return n;
}

asmlinkage int sys_rt_sigreturn(void)	sigset_t set;
	int er0;

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__copy_from_user(&set, &frame->uc.uc_sigmask, sizeof(set)))
		goto badframe;

static int setup_rt_frame(struct ksignal *ksig, sigset_t *set,
	frame = get_sigframe(ksig, regs, sizeof(*frame));

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	if (ksig->ka.sa.sa_flags & SA_SIGINFO)

futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr, u32 oldval,	int prev;
	int ret;

	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

	__asm__ __volatile__ (


/*
 * access_ok: - Checks if a user space pointer is valid
 * @type: Type of access: %VERIFY_READ or %VERIFY_WRITE.  Note that
 *        %VERIFY_WRITE is a superset of %VERIFY_READ - if it is safe
 *        to write to a block, it is always safe to read from it.
 * @addr: User space pointer to start of block to check
 * @size: Size of block to check
 *

static int setup_rt_frame(struct ksignal *ksig, sigset_t *set,
	frame = get_sigframe(ksig, regs, sizeof(struct rt_sigframe));

	if (!access_ok(VERIFY_WRITE, frame, sizeof(struct rt_sigframe)))
	if (!access_ok(frame, sizeof(struct rt_sigframe)))
		return -EFAULT;

	if (copy_siginfo_to_user(&frame->info, &ksig->info))

asmlinkage int sys_rt_sigreturn(void)	current->restart_block.fn = do_no_restart_syscall;

	frame = (struct rt_sigframe __user *)pt_psp(regs);
	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__copy_from_user(&blocked, &frame->uc.uc_sigmask, sizeof(blocked)))
		goto badframe;

__kernel_size_t __clear_user_hexagon(void __user *dest, unsigned long count)
unsigned long clear_user_hexagon(void __user *dest, unsigned long count)
{
	if (!access_ok(VERIFY_WRITE, dest, count))
	if (!access_ok(dest, count))
		return count;
	else
		return __clear_user_hexagon(dest, count);

static inline intfutex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,
			      u32 oldval, u32 newval)
{
	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

	{

static inline int __access_ok(const void __user *p, unsigned long size)	return likely(addr <= seg) &&
	 (seg == KERNEL_DS.seg || likely(REGION_OFFSET(addr) < RGN_MAP_LIMIT));
}
#define access_ok(type, addr, size)	__access_ok((addr), (size))
#define access_ok(addr, size)	__access_ok((addr), (size))

/*
 * These are the main single-value transfer routines.  They automatically

ptrace_getregs (struct task_struct *child, struct pt_all_user_regs __user *ppr)	char nat = 0;
	int i;

	if (!access_ok(VERIFY_WRITE, ppr, sizeof(struct pt_all_user_regs)))
	if (!access_ok(ppr, sizeof(struct pt_all_user_regs)))
		return -EIO;

	pt = task_pt_regs(child);

ptrace_setregs (struct task_struct *child, struct pt_all_user_regs __user *ppr)
	memset(&fpval, 0, sizeof(fpval));

	if (!access_ok(VERIFY_READ, ppr, sizeof(struct pt_all_user_regs)))
	if (!access_ok(ppr, sizeof(struct pt_all_user_regs)))
		return -EIO;

	pt = task_pt_regs(child);

ia64_rt_sigreturn (struct sigscratch *scr)		 */
		retval = (long) &ia64_strace_leave_kernel;

	if (!access_ok(VERIFY_READ, sc, sizeof(*sc)))
	if (!access_ok(sc, sizeof(*sc)))
		goto give_sigsegv;

	if (GET_SIGSET(&set, &sc->sc_mask))

setup_frame(struct ksignal *ksig, sigset_t *set, struct sigscratch *scr)	}
	frame = (void __user *) ((new_sp - sizeof(*frame)) & -STACK_ALIGN);

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame))) {
	if (!access_ok(frame, sizeof(*frame))) {
		force_sigsegv(ksig->sig, current);
		return 1;
	}

#include <asm/segment.h>

/* We let the MMU do all checking */
static inline int access_ok(int type, const void __user *addr,
static inline int access_ok(const void __user *addr,
			    unsigned long size)
{
	return 1;


#include <asm/segment.h>

#define access_ok(type,addr,size)	_access_ok((unsigned long)(addr),(size))
#define access_ok(addr,size)	_access_ok((unsigned long)(addr),(size))

/*
 * It is not enough to just have access_ok check for a real RAM address.

asmlinkage int do_sigreturn(struct pt_regs *regs, struct switch_stack *sw)	struct sigframe __user *frame = (struct sigframe __user *)(usp - 4);
	sigset_t set;

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__get_user(set.sig[0], &frame->sc.sc_mask) ||
	    (_NSIG_WORDS > 1 &&

asmlinkage int do_rt_sigreturn(struct pt_regs *regs, struct switch_stack *sw)	struct rt_sigframe __user *frame = (struct rt_sigframe __user *)(usp - 4);
	sigset_t set;

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__copy_from_user(&set, &frame->uc.uc_sigmask, sizeof(set)))
		goto badframe;

futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,	int ret = 0, cmp;
	u32 prev;

	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

	__asm__ __volatile__ ("1:	lwx	%1, %3, r0;		\

static inline int ___range_ok(unsigned long addr, unsigned long size)#define __range_ok(addr, size) \
		___range_ok((unsigned long)(addr), (unsigned long)(size))

#define access_ok(type, addr, size) (__range_ok((addr), (size)) == 0)
#define access_ok(addr, size) (__range_ok((addr), (size)) == 0)

#else

static inline int access_ok(int type, const void __user *addr,
							unsigned long size)
static inline int access_ok(const void __user *addr, unsigned long size)
{
	if (!size)
		goto ok;

	if ((get_fs().seg < ((unsigned long)addr)) ||
			(get_fs().seg < ((unsigned long)addr + size - 1))) {
		pr_devel("ACCESS fail: %s at 0x%08x (size 0x%x), seg 0x%08x\n",
			type ? "WRITE" : "READ ", (__force u32)addr, (u32)size,
		pr_devel("ACCESS fail at 0x%08x (size 0x%x), seg 0x%08x\n",
			(__force u32)addr, (u32)size,
			(u32)get_fs().seg);
		return 0;
	}
ok:
	pr_devel("ACCESS OK: %s at 0x%08x (size 0x%x), seg 0x%08x\n",
			type ? "WRITE" : "READ ", (__force u32)addr, (u32)size,
	pr_devel("ACCESS OK at 0x%08x (size 0x%x), seg 0x%08x\n",
			(__force u32)addr, (u32)size,
			(u32)get_fs().seg);
	return 1;
}

static inline unsigned long __must_check clear_user(void __user *to,							unsigned long n)
{
	might_fault();
	if (unlikely(!access_ok(VERIFY_WRITE, to, n)))
	if (unlikely(!access_ok(to, n)))
		return n;

	return __clear_user(to, n);

extern long __user_bad(void);	const typeof(*(ptr)) __user *__gu_addr = (ptr);			\
	int __gu_err = 0;						\
									\
	if (access_ok(VERIFY_READ, __gu_addr, size)) {			\
	if (access_ok(__gu_addr, size)) {			\
		switch (size) {						\
		case 1:							\
			__get_user_asm("lbu", __gu_addr, __gu_val,	\

extern long __user_bad(void);	typeof(*(ptr)) __user *__pu_addr = (ptr);			\
	int __pu_err = 0;						\
									\
	if (access_ok(VERIFY_WRITE, __pu_addr, size)) {			\
	if (access_ok(__pu_addr, size)) {			\
		switch (size) {						\
		case 1:							\
			__put_user_asm("sb", __pu_addr, __pu_val,	\

extern int __strncpy_user(char *to, const char __user *from, int len);static inline long
strncpy_from_user(char *dst, const char __user *src, long count)
{
	if (!access_ok(VERIFY_READ, src, 1))
	if (!access_ok(src, 1))
		return -EFAULT;
	return __strncpy_user(dst, src, count);
}

extern int __strnlen_user(const char __user *sstr, int len);
static inline long strnlen_user(const char __user *src, long n)
{
	if (!access_ok(VERIFY_READ, src, 1))
	if (!access_ok(src, 1))
		return 0;
	return __strnlen_user(src, n);
}

asmlinkage long sys_rt_sigreturn(struct pt_regs *regs)	/* Always make any pending restarted system calls return -EINTR */
	current->restart_block.fn = do_no_restart_syscall;

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;

	if (__copy_from_user(&set, &frame->uc.uc_sigmask, sizeof(set)))

static int setup_rt_frame(struct ksignal *ksig, sigset_t *set,
	frame = get_sigframe(ksig, regs, sizeof(*frame));

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	if (ksig->ka.sa.sa_flags & SA_SIGINFO)

static inline__wsum csum_and_copy_from_user(const void __user *src, void *dst,
			       int len, __wsum sum, int *err_ptr)
{
	if (access_ok(VERIFY_READ, src, len))
	if (access_ok(src, len))
		return csum_partial_copy_from_user(src, dst, len, sum,
						   err_ptr);
	if (len)

__wsum csum_and_copy_to_user(const void *src, void __user *dst, int len,			     __wsum sum, int *err_ptr)
{
	might_fault();
	if (access_ok(VERIFY_WRITE, dst, len)) {
	if (access_ok(dst, len)) {
		if (uaccess_kernel())
			return __csum_partial_copy_kernel(src,
							  (__force void *)dst,

futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,	int ret = 0;
	u32 val;

	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

	if (cpu_has_llsc && R10000_LLSC_WAR) {

static inline int user_termio_to_kernel_termios(struct ktermios *termios,	unsigned short iflag, oflag, cflag, lflag;
	unsigned int err;

	if (!access_ok(VERIFY_READ, termio, sizeof(struct termio)))
	if (!access_ok(termio, sizeof(struct termio)))
		return -EFAULT;

	err = __get_user(iflag, &termio->c_iflag);

static inline int kernel_termios_to_user_termio(struct termio __user *termio,{
	int err;

	if (!access_ok(VERIFY_WRITE, termio, sizeof(struct termio)))
	if (!access_ok(termio, sizeof(struct termio)))
		return -EFAULT;

	err = __put_user(termios->c_iflag, &termio->c_iflag);

static inline bool eva_kernel_access(void)
/*
 * access_ok: - Checks if a user space pointer is valid
 * @type: Type of access: %VERIFY_READ or %VERIFY_WRITE.  Note that
 *	  %VERIFY_WRITE is a superset of %VERIFY_READ - if it is safe
 *	  to write to a block, it is always safe to read from it.
 * @addr: User space pointer to start of block to check
 * @size: Size of block to check
 *

static inline int __access_ok(const void __user *p, unsigned long size)	return (get_fs().seg & (addr | (addr + size) | __ua_size(size))) == 0;
}

#define access_ok(type, addr, size)					\
#define access_ok(addr, size)					\
	likely(__access_ok((addr), (size)))

/*

do {									\	const __typeof__(*(ptr)) __user * __gu_ptr = (ptr);		\
									\
	might_fault();							\
	if (likely(access_ok(VERIFY_READ,  __gu_ptr, size))) {		\
	if (likely(access_ok( __gu_ptr, size))) {		\
		if (eva_kernel_access())				\
			__get_kernel_common((x), size, __gu_ptr);	\
		else							\

do {									\	int __pu_err = -EFAULT;						\
									\
	might_fault();							\
	if (likely(access_ok(VERIFY_WRITE,  __pu_addr, size))) {	\
	if (likely(access_ok( __pu_addr, size))) {	\
		if (eva_kernel_access())				\
			__put_kernel_common(__pu_addr, size);		\
		else							\

__clear_user(void __user *addr, __kernel_size_t size)({									\
	void __user * __cl_addr = (addr);				\
	unsigned long __cl_size = (n);					\
	if (__cl_size && access_ok(VERIFY_WRITE,			\
					__cl_addr, __cl_size))		\
	if (__cl_size && access_ok(__cl_addr, __cl_size))		\
		__cl_size = __clear_user(__cl_addr, __cl_size);		\
	__cl_size;							\
})

int mipsr2_decoder(struct pt_regs *regs, u32 inst, unsigned long *fcr31)	case lwl_op:
		rt = regs->regs[MIPSInst_RT(inst)];
		vaddr = regs->regs[MIPSInst_RS(inst)] + MIPSInst_SIMM(inst);
		if (!access_ok(VERIFY_READ, (void __user *)vaddr, 4)) {
		if (!access_ok((void __user *)vaddr, 4)) {
			current->thread.cp0_baduaddr = vaddr;
			err = SIGSEGV;
			break;

int mipsr2_decoder(struct pt_regs *regs, u32 inst, unsigned long *fcr31)	case lwr_op:
		rt = regs->regs[MIPSInst_RT(inst)];
		vaddr = regs->regs[MIPSInst_RS(inst)] + MIPSInst_SIMM(inst);
		if (!access_ok(VERIFY_READ, (void __user *)vaddr, 4)) {
		if (!access_ok((void __user *)vaddr, 4)) {
			current->thread.cp0_baduaddr = vaddr;
			err = SIGSEGV;
			break;

int mipsr2_decoder(struct pt_regs *regs, u32 inst, unsigned long *fcr31)	case swl_op:
		rt = regs->regs[MIPSInst_RT(inst)];
		vaddr = regs->regs[MIPSInst_RS(inst)] + MIPSInst_SIMM(inst);
		if (!access_ok(VERIFY_WRITE, (void __user *)vaddr, 4)) {
		if (!access_ok((void __user *)vaddr, 4)) {
			current->thread.cp0_baduaddr = vaddr;
			err = SIGSEGV;
			break;

int mipsr2_decoder(struct pt_regs *regs, u32 inst, unsigned long *fcr31)	case swr_op:
		rt = regs->regs[MIPSInst_RT(inst)];
		vaddr = regs->regs[MIPSInst_RS(inst)] + MIPSInst_SIMM(inst);
		if (!access_ok(VERIFY_WRITE, (void __user *)vaddr, 4)) {
		if (!access_ok((void __user *)vaddr, 4)) {
			current->thread.cp0_baduaddr = vaddr;
			err = SIGSEGV;
			break;

int mipsr2_decoder(struct pt_regs *regs, u32 inst, unsigned long *fcr31)
		rt = regs->regs[MIPSInst_RT(inst)];
		vaddr = regs->regs[MIPSInst_RS(inst)] + MIPSInst_SIMM(inst);
		if (!access_ok(VERIFY_READ, (void __user *)vaddr, 8)) {
		if (!access_ok((void __user *)vaddr, 8)) {
			current->thread.cp0_baduaddr = vaddr;
			err = SIGSEGV;
			break;

int mipsr2_decoder(struct pt_regs *regs, u32 inst, unsigned long *fcr31)
		rt = regs->regs[MIPSInst_RT(inst)];
		vaddr = regs->regs[MIPSInst_RS(inst)] + MIPSInst_SIMM(inst);
		if (!access_ok(VERIFY_READ, (void __user *)vaddr, 8)) {
		if (!access_ok((void __user *)vaddr, 8)) {
			current->thread.cp0_baduaddr = vaddr;
			err = SIGSEGV;
			break;

int mipsr2_decoder(struct pt_regs *regs, u32 inst, unsigned long *fcr31)
		rt = regs->regs[MIPSInst_RT(inst)];
		vaddr = regs->regs[MIPSInst_RS(inst)] + MIPSInst_SIMM(inst);
		if (!access_ok(VERIFY_WRITE, (void __user *)vaddr, 8)) {
		if (!access_ok((void __user *)vaddr, 8)) {
			current->thread.cp0_baduaddr = vaddr;
			err = SIGSEGV;
			break;

int mipsr2_decoder(struct pt_regs *regs, u32 inst, unsigned long *fcr31)
		rt = regs->regs[MIPSInst_RT(inst)];
		vaddr = regs->regs[MIPSInst_RS(inst)] + MIPSInst_SIMM(inst);
		if (!access_ok(VERIFY_WRITE, (void __user *)vaddr, 8)) {
		if (!access_ok((void __user *)vaddr, 8)) {
			current->thread.cp0_baduaddr = vaddr;
			err = SIGSEGV;
			break;

int mipsr2_decoder(struct pt_regs *regs, u32 inst, unsigned long *fcr31)			err = SIGBUS;
			break;
		}
		if (!access_ok(VERIFY_READ, (void __user *)vaddr, 4)) {
		if (!access_ok((void __user *)vaddr, 4)) {
			current->thread.cp0_baduaddr = vaddr;
			err = SIGBUS;
			break;

int mipsr2_decoder(struct pt_regs *regs, u32 inst, unsigned long *fcr31)			err = SIGBUS;
			break;
		}
		if (!access_ok(VERIFY_WRITE, (void __user *)vaddr, 4)) {
		if (!access_ok((void __user *)vaddr, 4)) {
			current->thread.cp0_baduaddr = vaddr;
			err = SIGBUS;
			break;

int mipsr2_decoder(struct pt_regs *regs, u32 inst, unsigned long *fcr31)			err = SIGBUS;
			break;
		}
		if (!access_ok(VERIFY_READ, (void __user *)vaddr, 8)) {
		if (!access_ok((void __user *)vaddr, 8)) {
			current->thread.cp0_baduaddr = vaddr;
			err = SIGBUS;
			break;

int mipsr2_decoder(struct pt_regs *regs, u32 inst, unsigned long *fcr31)			err = SIGBUS;
			break;
		}
		if (!access_ok(VERIFY_WRITE, (void __user *)vaddr, 8)) {
		if (!access_ok((void __user *)vaddr, 8)) {
			current->thread.cp0_baduaddr = vaddr;
			err = SIGBUS;
			break;

int ptrace_getregs(struct task_struct *child, struct user_pt_regs __user *data)	struct pt_regs *regs;
	int i;

	if (!access_ok(VERIFY_WRITE, data, 38 * 8))
	if (!access_ok(data, 38 * 8))
		return -EIO;

	regs = task_pt_regs(child);

int ptrace_setregs(struct task_struct *child, struct user_pt_regs __user *data)	struct pt_regs *regs;
	int i;

	if (!access_ok(VERIFY_READ, data, 38 * 8))
	if (!access_ok(data, 38 * 8))
		return -EIO;

	regs = task_pt_regs(child);

int ptrace_get_watch_regs(struct task_struct *child,
	if (!cpu_has_watch || boot_cpu_data.watch_reg_use_cnt == 0)
		return -EIO;
	if (!access_ok(VERIFY_WRITE, addr, sizeof(struct pt_watch_regs)))
	if (!access_ok(addr, sizeof(struct pt_watch_regs)))
		return -EIO;

#ifdef CONFIG_32BIT

int ptrace_set_watch_regs(struct task_struct *child,
	if (!cpu_has_watch || boot_cpu_data.watch_reg_use_cnt == 0)
		return -EIO;
	if (!access_ok(VERIFY_READ, addr, sizeof(struct pt_watch_regs)))
	if (!access_ok(addr, sizeof(struct pt_watch_regs)))
		return -EIO;
	/* Check the values. */
	for (i = 0; i < boot_cpu_data.watch_reg_use_cnt; i++) {

int ptrace_getfpregs(struct task_struct *child, __u32 __user *data){
	int i;

	if (!access_ok(VERIFY_WRITE, data, 33 * 8))
	if (!access_ok(data, 33 * 8))
		return -EIO;

	if (tsk_used_math(child)) {

int ptrace_setfpregs(struct task_struct *child, __u32 __user *data)	u32 value;
	int i;

	if (!access_ok(VERIFY_READ, data, 33 * 8))
	if (!access_ok(data, 33 * 8))
		return -EIO;

	init_fp_ctx(child);

SYSCALL_DEFINE3(sigaction, int, sig, const struct sigaction __user *, act,	if (act) {
		old_sigset_t mask;

		if (!access_ok(VERIFY_READ, act, sizeof(*act)))
		if (!access_ok(act, sizeof(*act)))
			return -EFAULT;
		err |= __get_user(new_ka.sa.sa_handler, &act->sa_handler);
		err |= __get_user(new_ka.sa.sa_flags, &act->sa_flags);

SYSCALL_DEFINE3(sigaction, int, sig, const struct sigaction __user *, act,	ret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);

	if (!ret && oact) {
		if (!access_ok(VERIFY_WRITE, oact, sizeof(*oact)))
		if (!access_ok(oact, sizeof(*oact)))
			return -EFAULT;
		err |= __put_user(old_ka.sa.sa_flags, &oact->sa_flags);
		err |= __put_user(old_ka.sa.sa_handler, &oact->sa_handler);

asmlinkage void sys_sigreturn(void)
	regs = current_pt_regs();
	frame = (struct sigframe __user *)regs->regs[29];
	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__copy_from_user(&blocked, &frame->sf_mask, sizeof(blocked)))
		goto badframe;

asmlinkage void sys_rt_sigreturn(void)
	regs = current_pt_regs();
	frame = (struct rt_sigframe __user *)regs->regs[29];
	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__copy_from_user(&set, &frame->rs_uc.uc_sigmask, sizeof(set)))
		goto badframe;

static int setup_frame(void *sig_return, struct ksignal *ksig,	int err = 0;

	frame = get_sigframe(ksig, regs, sizeof(*frame));
	if (!access_ok(VERIFY_WRITE, frame, sizeof (*frame)))
	if (!access_ok(frame, sizeof (*frame)))
		return -EFAULT;

	err |= setup_sigcontext(regs, &frame->sf_sc);

static int setup_rt_frame(void *sig_return, struct ksignal *ksig,	int err = 0;

	frame = get_sigframe(ksig, regs, sizeof(*frame));
	if (!access_ok(VERIFY_WRITE, frame, sizeof (*frame)))
	if (!access_ok(frame, sizeof (*frame)))
		return -EFAULT;

	/* Create siginfo.  */

SYSCALL_DEFINE3(32_sigaction, long, sig, const struct compat_sigaction __user *,		old_sigset_t mask;
		s32 handler;

		if (!access_ok(VERIFY_READ, act, sizeof(*act)))
		if (!access_ok(act, sizeof(*act)))
			return -EFAULT;
		err |= __get_user(handler, &act->sa_handler);
		new_ka.sa.sa_handler = (void __user *)(s64)handler;

SYSCALL_DEFINE3(32_sigaction, long, sig, const struct compat_sigaction __user *,	ret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);

	if (!ret && oact) {
		if (!access_ok(VERIFY_WRITE, oact, sizeof(*oact)))
		if (!access_ok(oact, sizeof(*oact)))
			return -EFAULT;
		err |= __put_user(old_ka.sa.sa_flags, &oact->sa_flags);
		err |= __put_user((u32)(u64)old_ka.sa.sa_handler,

asmlinkage void sysn32_rt_sigreturn(void)
	regs = current_pt_regs();
	frame = (struct rt_sigframe_n32 __user *)regs->regs[29];
	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__copy_conv_sigset_from_user(&set, &frame->rs_uc.uc_sigmask))
		goto badframe;

static int setup_rt_frame_n32(void *sig_return, struct ksignal *ksig,	int err = 0;

	frame = get_sigframe(ksig, regs, sizeof(*frame));
	if (!access_ok(VERIFY_WRITE, frame, sizeof (*frame)))
	if (!access_ok(frame, sizeof (*frame)))
		return -EFAULT;

	/* Create siginfo.  */

static int setup_frame_32(void *sig_return, struct ksignal *ksig,	int err = 0;

	frame = get_sigframe(ksig, regs, sizeof(*frame));
	if (!access_ok(VERIFY_WRITE, frame, sizeof (*frame)))
	if (!access_ok(frame, sizeof (*frame)))
		return -EFAULT;

	err |= setup_sigcontext32(regs, &frame->sf_sc);

asmlinkage void sys32_rt_sigreturn(void)
	regs = current_pt_regs();
	frame = (struct rt_sigframe32 __user *)regs->regs[29];
	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__copy_conv_sigset_from_user(&set, &frame->rs_uc.uc_sigmask))
		goto badframe;

static int setup_rt_frame_32(void *sig_return, struct ksignal *ksig,	int err = 0;

	frame = get_sigframe(ksig, regs, sizeof(*frame));
	if (!access_ok(VERIFY_WRITE, frame, sizeof (*frame)))
	if (!access_ok(frame, sizeof (*frame)))
		return -EFAULT;

	/* Convert (siginfo_t -> compat_siginfo_t) and copy to user. */

asmlinkage void sys32_sigreturn(void)
	regs = current_pt_regs();
	frame = (struct sigframe32 __user *)regs->regs[29];
	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__copy_conv_sigset_from_user(&blocked, &frame->sf_mask))
		goto badframe;

static inline int mips_atomic_set(unsigned long addr, unsigned long new)	if (unlikely(addr & 3))
		return -EINVAL;

	if (unlikely(!access_ok(VERIFY_WRITE, (const void __user *)addr, 4)))
	if (unlikely(!access_ok((const void __user *)addr, 4)))
		return -EINVAL;

	if (cpu_has_llsc && R10000_LLSC_WAR) {

static void emulate_load_store_insn(struct pt_regs *regs,		if (insn.dsp_format.func == lx_op) {
			switch (insn.dsp_format.op) {
			case lwx_op:
				if (!access_ok(VERIFY_READ, addr, 4))
				if (!access_ok(addr, 4))
					goto sigbus;
				LoadW(addr, value, res);
				if (res)

static void emulate_load_store_insn(struct pt_regs *regs,				regs->regs[insn.dsp_format.rd] = value;
				break;
			case lhx_op:
				if (!access_ok(VERIFY_READ, addr, 2))
				if (!access_ok(addr, 2))
					goto sigbus;
				LoadHW(addr, value, res);
				if (res)

static void emulate_load_store_insn(struct pt_regs *regs,			set_fs(USER_DS);
			switch (insn.spec3_format.func) {
			case lhe_op:
				if (!access_ok(VERIFY_READ, addr, 2)) {
				if (!access_ok(addr, 2)) {
					set_fs(seg);
					goto sigbus;
				}

static void emulate_load_store_insn(struct pt_regs *regs,				regs->regs[insn.spec3_format.rt] = value;
				break;
			case lwe_op:
				if (!access_ok(VERIFY_READ, addr, 4)) {
				if (!access_ok(addr, 4)) {
					set_fs(seg);
					goto sigbus;
				}

static void emulate_load_store_insn(struct pt_regs *regs,				regs->regs[insn.spec3_format.rt] = value;
				break;
			case lhue_op:
				if (!access_ok(VERIFY_READ, addr, 2)) {
				if (!access_ok(addr, 2)) {
					set_fs(seg);
					goto sigbus;
				}

static void emulate_load_store_insn(struct pt_regs *regs,				regs->regs[insn.spec3_format.rt] = value;
				break;
			case she_op:
				if (!access_ok(VERIFY_WRITE, addr, 2)) {
				if (!access_ok(addr, 2)) {
					set_fs(seg);
					goto sigbus;
				}

static void emulate_load_store_insn(struct pt_regs *regs,				}
				break;
			case swe_op:
				if (!access_ok(VERIFY_WRITE, addr, 4)) {
				if (!access_ok(addr, 4)) {
					set_fs(seg);
					goto sigbus;
				}

static void emulate_load_store_insn(struct pt_regs *regs,#endif
		break;
	case lh_op:
		if (!access_ok(VERIFY_READ, addr, 2))
		if (!access_ok(addr, 2))
			goto sigbus;

		if (IS_ENABLED(CONFIG_EVA)) {

static void emulate_load_store_insn(struct pt_regs *regs,		break;

	case lw_op:
		if (!access_ok(VERIFY_READ, addr, 4))
		if (!access_ok(addr, 4))
			goto sigbus;

		if (IS_ENABLED(CONFIG_EVA)) {

static void emulate_load_store_insn(struct pt_regs *regs,		break;

	case lhu_op:
		if (!access_ok(VERIFY_READ, addr, 2))
		if (!access_ok(addr, 2))
			goto sigbus;

		if (IS_ENABLED(CONFIG_EVA)) {

static void emulate_load_store_insn(struct pt_regs *regs,		 * would blow up, so for now we don't handle unaligned 64-bit
		 * instructions on 32-bit kernels.
		 */
		if (!access_ok(VERIFY_READ, addr, 4))
		if (!access_ok(addr, 4))
			goto sigbus;

		LoadWU(addr, value, res);

static void emulate_load_store_insn(struct pt_regs *regs,		 * would blow up, so for now we don't handle unaligned 64-bit
		 * instructions on 32-bit kernels.
		 */
		if (!access_ok(VERIFY_READ, addr, 8))
		if (!access_ok(addr, 8))
			goto sigbus;

		LoadDW(addr, value, res);

static void emulate_load_store_insn(struct pt_regs *regs,		goto sigill;

	case sh_op:
		if (!access_ok(VERIFY_WRITE, addr, 2))
		if (!access_ok(addr, 2))
			goto sigbus;

		compute_return_epc(regs);

static void emulate_load_store_insn(struct pt_regs *regs,		break;

	case sw_op:
		if (!access_ok(VERIFY_WRITE, addr, 4))
		if (!access_ok(addr, 4))
			goto sigbus;

		compute_return_epc(regs);

static void emulate_load_store_insn(struct pt_regs *regs,		 * would blow up, so for now we don't handle unaligned 64-bit
		 * instructions on 32-bit kernels.
		 */
		if (!access_ok(VERIFY_WRITE, addr, 8))
		if (!access_ok(addr, 8))
			goto sigbus;

		compute_return_epc(regs);

static void emulate_load_store_insn(struct pt_regs *regs,
		switch (insn.msa_mi10_format.func) {
		case msa_ld_op:
			if (!access_ok(VERIFY_READ, addr, sizeof(*fpr)))
			if (!access_ok(addr, sizeof(*fpr)))
				goto sigbus;

			do {

static void emulate_load_store_insn(struct pt_regs *regs,			break;

		case msa_st_op:
			if (!access_ok(VERIFY_WRITE, addr, sizeof(*fpr)))
			if (!access_ok(addr, sizeof(*fpr)))
				goto sigbus;

			/*

static void emulate_load_store_microMIPS(struct pt_regs *regs,			if (reg == 31)
				goto sigbus;

			if (!access_ok(VERIFY_READ, addr, 8))
			if (!access_ok(addr, 8))
				goto sigbus;

			LoadW(addr, value, res);

static void emulate_load_store_microMIPS(struct pt_regs *regs,			if (reg == 31)
				goto sigbus;

			if (!access_ok(VERIFY_WRITE, addr, 8))
			if (!access_ok(addr, 8))
				goto sigbus;

			value = regs->regs[reg];

static void emulate_load_store_microMIPS(struct pt_regs *regs,			if (reg == 31)
				goto sigbus;

			if (!access_ok(VERIFY_READ, addr, 16))
			if (!access_ok(addr, 16))
				goto sigbus;

			LoadDW(addr, value, res);

static void emulate_load_store_microMIPS(struct pt_regs *regs,			if (reg == 31)
				goto sigbus;

			if (!access_ok(VERIFY_WRITE, addr, 16))
			if (!access_ok(addr, 16))
				goto sigbus;

			value = regs->regs[reg];

static void emulate_load_store_microMIPS(struct pt_regs *regs,			if ((rvar > 9) || !reg)
				goto sigill;
			if (reg & 0x10) {
				if (!access_ok
				    (VERIFY_READ, addr, 4 * (rvar + 1)))
				if (!access_ok(addr, 4 * (rvar + 1)))
					goto sigbus;
			} else {
				if (!access_ok(VERIFY_READ, addr, 4 * rvar))
				if (!access_ok(addr, 4 * rvar))
					goto sigbus;
			}
			if (rvar == 9)

static void emulate_load_store_microMIPS(struct pt_regs *regs,			if ((rvar > 9) || !reg)
				goto sigill;
			if (reg & 0x10) {
				if (!access_ok
				    (VERIFY_WRITE, addr, 4 * (rvar + 1)))
				if (!access_ok(addr, 4 * (rvar + 1)))
					goto sigbus;
			} else {
				if (!access_ok(VERIFY_WRITE, addr, 4 * rvar))
				if (!access_ok(addr, 4 * rvar))
					goto sigbus;
			}
			if (rvar == 9)

static void emulate_load_store_microMIPS(struct pt_regs *regs,			if ((rvar > 9) || !reg)
				goto sigill;
			if (reg & 0x10) {
				if (!access_ok
				    (VERIFY_READ, addr, 8 * (rvar + 1)))
				if (!access_ok(addr, 8 * (rvar + 1)))
					goto sigbus;
			} else {
				if (!access_ok(VERIFY_READ, addr, 8 * rvar))
				if (!access_ok(addr, 8 * rvar))
					goto sigbus;
			}
			if (rvar == 9)

static void emulate_load_store_microMIPS(struct pt_regs *regs,			if ((rvar > 9) || !reg)
				goto sigill;
			if (reg & 0x10) {
				if (!access_ok
				    (VERIFY_WRITE, addr, 8 * (rvar + 1)))
				if (!access_ok(addr, 8 * (rvar + 1)))
					goto sigbus;
			} else {
				if (!access_ok(VERIFY_WRITE, addr, 8 * rvar))
				if (!access_ok(addr, 8 * rvar))
					goto sigbus;
			}
			if (rvar == 9)

static void emulate_load_store_microMIPS(struct pt_regs *regs,		case mm_lwm16_op:
			reg = insn.mm16_m_format.rlist;
			rvar = reg + 1;
			if (!access_ok(VERIFY_READ, addr, 4 * rvar))
			if (!access_ok(addr, 4 * rvar))
				goto sigbus;

			for (i = 16; rvar; rvar--, i++) {

static void emulate_load_store_microMIPS(struct pt_regs *regs,		case mm_swm16_op:
			reg = insn.mm16_m_format.rlist;
			rvar = reg + 1;
			if (!access_ok(VERIFY_WRITE, addr, 4 * rvar))
			if (!access_ok(addr, 4 * rvar))
				goto sigbus;

			for (i = 16; rvar; rvar--, i++) {

static void emulate_load_store_microMIPS(struct pt_regs *regs,	}

loadHW:
	if (!access_ok(VERIFY_READ, addr, 2))
	if (!access_ok(addr, 2))
		goto sigbus;

	LoadHW(addr, value, res);

static void emulate_load_store_microMIPS(struct pt_regs *regs,	goto success;

loadHWU:
	if (!access_ok(VERIFY_READ, addr, 2))
	if (!access_ok(addr, 2))
		goto sigbus;

	LoadHWU(addr, value, res);

static void emulate_load_store_microMIPS(struct pt_regs *regs,	goto success;

loadW:
	if (!access_ok(VERIFY_READ, addr, 4))
	if (!access_ok(addr, 4))
		goto sigbus;

	LoadW(addr, value, res);

static void emulate_load_store_microMIPS(struct pt_regs *regs,	 * would blow up, so for now we don't handle unaligned 64-bit
	 * instructions on 32-bit kernels.
	 */
	if (!access_ok(VERIFY_READ, addr, 4))
	if (!access_ok(addr, 4))
		goto sigbus;

	LoadWU(addr, value, res);

static void emulate_load_store_microMIPS(struct pt_regs *regs,	 * would blow up, so for now we don't handle unaligned 64-bit
	 * instructions on 32-bit kernels.
	 */
	if (!access_ok(VERIFY_READ, addr, 8))
	if (!access_ok(addr, 8))
		goto sigbus;

	LoadDW(addr, value, res);

static void emulate_load_store_microMIPS(struct pt_regs *regs,	goto sigill;

storeHW:
	if (!access_ok(VERIFY_WRITE, addr, 2))
	if (!access_ok(addr, 2))
		goto sigbus;

	value = regs->regs[reg];

static void emulate_load_store_microMIPS(struct pt_regs *regs,	goto success;

storeW:
	if (!access_ok(VERIFY_WRITE, addr, 4))
	if (!access_ok(addr, 4))
		goto sigbus;

	value = regs->regs[reg];

static void emulate_load_store_microMIPS(struct pt_regs *regs,	 * would blow up, so for now we don't handle unaligned 64-bit
	 * instructions on 32-bit kernels.
	 */
	if (!access_ok(VERIFY_WRITE, addr, 8))
	if (!access_ok(addr, 8))
		goto sigbus;

	value = regs->regs[reg];

static void emulate_load_store_MIPS16e(struct pt_regs *regs, void __user * addr)		goto sigbus;

	case MIPS16e_lh_op:
		if (!access_ok(VERIFY_READ, addr, 2))
		if (!access_ok(addr, 2))
			goto sigbus;

		LoadHW(addr, value, res);

static void emulate_load_store_MIPS16e(struct pt_regs *regs, void __user * addr)		break;

	case MIPS16e_lhu_op:
		if (!access_ok(VERIFY_READ, addr, 2))
		if (!access_ok(addr, 2))
			goto sigbus;

		LoadHWU(addr, value, res);

static void emulate_load_store_MIPS16e(struct pt_regs *regs, void __user * addr)	case MIPS16e_lw_op:
	case MIPS16e_lwpc_op:
	case MIPS16e_lwsp_op:
		if (!access_ok(VERIFY_READ, addr, 4))
		if (!access_ok(addr, 4))
			goto sigbus;

		LoadW(addr, value, res);

static void emulate_load_store_MIPS16e(struct pt_regs *regs, void __user * addr)		 * would blow up, so for now we don't handle unaligned 64-bit
		 * instructions on 32-bit kernels.
		 */
		if (!access_ok(VERIFY_READ, addr, 4))
		if (!access_ok(addr, 4))
			goto sigbus;

		LoadWU(addr, value, res);

static void emulate_load_store_MIPS16e(struct pt_regs *regs, void __user * addr)		 * would blow up, so for now we don't handle unaligned 64-bit
		 * instructions on 32-bit kernels.
		 */
		if (!access_ok(VERIFY_READ, addr, 8))
		if (!access_ok(addr, 8))
			goto sigbus;

		LoadDW(addr, value, res);

static void emulate_load_store_MIPS16e(struct pt_regs *regs, void __user * addr)		goto sigill;

	case MIPS16e_sh_op:
		if (!access_ok(VERIFY_WRITE, addr, 2))
		if (!access_ok(addr, 2))
			goto sigbus;

		MIPS16e_compute_return_epc(regs, &oldinst);

static void emulate_load_store_MIPS16e(struct pt_regs *regs, void __user * addr)	case MIPS16e_sw_op:
	case MIPS16e_swsp_op:
	case MIPS16e_i8_op:	/* actually - MIPS16e_swrasp_func */
		if (!access_ok(VERIFY_WRITE, addr, 4))
		if (!access_ok(addr, 4))
			goto sigbus;

		MIPS16e_compute_return_epc(regs, &oldinst);

static void emulate_load_store_MIPS16e(struct pt_regs *regs, void __user * addr)		 * would blow up, so for now we don't handle unaligned 64-bit
		 * instructions on 32-bit kernels.
		 */
		if (!access_ok(VERIFY_WRITE, addr, 8))
		if (!access_ok(addr, 8))
			goto sigbus;

		MIPS16e_compute_return_epc(regs, &oldinst);

static int cop1Emulate(struct pt_regs *xcp, struct mips_fpu_struct *ctx,				     MIPSInst_SIMM(ir));
		MIPS_FPU_EMU_INC_STATS(loads);

		if (!access_ok(VERIFY_READ, dva, sizeof(u64))) {
		if (!access_ok(dva, sizeof(u64))) {
			MIPS_FPU_EMU_INC_STATS(errors);
			*fault_addr = dva;
			return SIGBUS;

static int cop1Emulate(struct pt_regs *xcp, struct mips_fpu_struct *ctx,				      MIPSInst_SIMM(ir));
		MIPS_FPU_EMU_INC_STATS(stores);
		DIFROMREG(dval, MIPSInst_RT(ir));
		if (!access_ok(VERIFY_WRITE, dva, sizeof(u64))) {
		if (!access_ok(dva, sizeof(u64))) {
			MIPS_FPU_EMU_INC_STATS(errors);
			*fault_addr = dva;
			return SIGBUS;

static int cop1Emulate(struct pt_regs *xcp, struct mips_fpu_struct *ctx,		wva = (u32 __user *) (xcp->regs[MIPSInst_RS(ir)] +
				      MIPSInst_SIMM(ir));
		MIPS_FPU_EMU_INC_STATS(loads);
		if (!access_ok(VERIFY_READ, wva, sizeof(u32))) {
		if (!access_ok(wva, sizeof(u32))) {
			MIPS_FPU_EMU_INC_STATS(errors);
			*fault_addr = wva;
			return SIGBUS;

static int cop1Emulate(struct pt_regs *xcp, struct mips_fpu_struct *ctx,				      MIPSInst_SIMM(ir));
		MIPS_FPU_EMU_INC_STATS(stores);
		SIFROMREG(wval, MIPSInst_RT(ir));
		if (!access_ok(VERIFY_WRITE, wva, sizeof(u32))) {
		if (!access_ok(wva, sizeof(u32))) {
			MIPS_FPU_EMU_INC_STATS(errors);
			*fault_addr = wva;
			return SIGBUS;

static int fpux_emu(struct pt_regs *xcp, struct mips_fpu_struct *ctx,				xcp->regs[MIPSInst_FT(ir)]);

			MIPS_FPU_EMU_INC_STATS(loads);
			if (!access_ok(VERIFY_READ, va, sizeof(u32))) {
			if (!access_ok(va, sizeof(u32))) {
				MIPS_FPU_EMU_INC_STATS(errors);
				*fault_addr = va;
				return SIGBUS;

static int fpux_emu(struct pt_regs *xcp, struct mips_fpu_struct *ctx,			MIPS_FPU_EMU_INC_STATS(stores);

			SIFROMREG(val, MIPSInst_FS(ir));
			if (!access_ok(VERIFY_WRITE, va, sizeof(u32))) {
			if (!access_ok(va, sizeof(u32))) {
				MIPS_FPU_EMU_INC_STATS(errors);
				*fault_addr = va;
				return SIGBUS;

static int fpux_emu(struct pt_regs *xcp, struct mips_fpu_struct *ctx,				xcp->regs[MIPSInst_FT(ir)]);

			MIPS_FPU_EMU_INC_STATS(loads);
			if (!access_ok(VERIFY_READ, va, sizeof(u64))) {
			if (!access_ok(va, sizeof(u64))) {
				MIPS_FPU_EMU_INC_STATS(errors);
				*fault_addr = va;
				return SIGBUS;

static int fpux_emu(struct pt_regs *xcp, struct mips_fpu_struct *ctx,
			MIPS_FPU_EMU_INC_STATS(stores);
			DIFROMREG(val, MIPSInst_FS(ir));
			if (!access_ok(VERIFY_WRITE, va, sizeof(u64))) {
			if (!access_ok(va, sizeof(u64))) {
				MIPS_FPU_EMU_INC_STATS(errors);
				*fault_addr = va;
				return SIGBUS;

SYSCALL_DEFINE3(cacheflush, unsigned long, addr, unsigned long, bytes,{
	if (bytes == 0)
		return 0;
	if (!access_ok(VERIFY_WRITE, (void __user *) addr, bytes))
	if (!access_ok((void __user *) addr, bytes))
		return -EFAULT;

	__flush_icache_user_range(addr, addr + bytes);

int __get_user_pages_fast(unsigned long start, int nr_pages, int write,	addr = start;
	len = (unsigned long) nr_pages << PAGE_SHIFT;
	end = start + len;
	if (unlikely(!access_ok(write ? VERIFY_WRITE : VERIFY_READ,
					(void __user *)start, len)))
	if (unlikely(!access_ok((void __user *)start, len)))
		return 0;

	/*

struct stackframe {static inline int get_mem(unsigned long addr, unsigned long *result)
{
	unsigned long *address = (unsigned long *) addr;
	if (!access_ok(VERIFY_READ, address, sizeof(unsigned long)))
	if (!access_ok(address, sizeof(unsigned long)))
		return -1;
	if (__copy_from_user_inatomic(result, address, sizeof(unsigned long)))
		return -3;

static ssize_t sbprof_tb_read(struct file *filp, char *buf,	char *dest    =	 buf;
	long  cur_off = *offp;

	if (!access_ok(VERIFY_WRITE, buf, size))
	if (!access_ok(buf, size))
		return -EFAULT;

	mutex_lock(&sbp.lock);

futex_atomic_cmpxchg_inatomic(u32 * uval, u32 __user * uaddr,	int ret = 0;
	u32 val, tmp, flags;

	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

	smp_mb();

#include <asm/types.h>
#include <linux/mm.h>

#define VERIFY_READ	0
#define VERIFY_WRITE	1

#define __asmeq(x, y)  ".ifnc " x "," y " ; .err ; .endif\n\t"

/*

static inline void set_fs(mm_segment_t fs)
#define __range_ok(addr, size) (size <= get_fs() && addr <= (get_fs() -size))

#define access_ok(type, addr, size)	\
#define access_ok(addr, size)	\
	__range_ok((unsigned long)addr, (unsigned long)size)
/*
 * Single-value transfer routines.  They automatically use the right

static inline void set_fs(mm_segment_t fs)({									\
	const __typeof__(*(ptr)) __user *__p = (ptr);			\
	might_fault();							\
	if (access_ok(VERIFY_READ, __p, sizeof(*__p))) {		\
	if (access_ok(__p, sizeof(*__p))) {		\
		__get_user_err((x), __p, (err));			\
	} else {							\
		(x) = 0; (err) = -EFAULT;				\

do {									\({									\
	__typeof__(*(ptr)) __user *__p = (ptr);				\
	might_fault();							\
	if (access_ok(VERIFY_WRITE, __p, sizeof(*__p))) {		\
	if (access_ok(__p, sizeof(*__p))) {		\
		__put_user_err((x), __p, (err));			\
	} else	{							\
		(err) = -EFAULT;					\

extern unsigned long __arch_copy_to_user(void __user * to, const void *from,#define INLINE_COPY_TO_USER
static inline unsigned long clear_user(void __user * to, unsigned long n)
{
	if (access_ok(VERIFY_WRITE, to, n))
	if (access_ok(to, n))
		n = __arch_clear_user(to, n);
	return n;
}

user_backtrace(struct perf_callchain_entry_ctx *entry, unsigned long fp)		(unsigned long *)(fp - (unsigned long)sizeof(buftail));

	/* Check accessibility of one struct frame_tail beyond */
	if (!access_ok(VERIFY_READ, user_frame_tail, sizeof(buftail)))
	if (!access_ok(user_frame_tail, sizeof(buftail)))
		return 0;
	if (__copy_from_user_inatomic
		(&buftail, user_frame_tail, sizeof(buftail)))

user_backtrace_opt_size(struct perf_callchain_entry_ctx *entry,		(unsigned long *)(fp - (unsigned long)sizeof(buftail));

	/* Check accessibility of one struct frame_tail beyond */
	if (!access_ok(VERIFY_READ, user_frame_tail, sizeof(buftail)))
	if (!access_ok(user_frame_tail, sizeof(buftail)))
		return 0;
	if (__copy_from_user_inatomic
		(&buftail, user_frame_tail, sizeof(buftail)))

perf_callchain_user(struct perf_callchain_entry_ctx *entry,		user_frame_tail =
			(unsigned long *)(fp - (unsigned long)sizeof(fp));

		if (!access_ok(VERIFY_READ, user_frame_tail, sizeof(fp)))
		if (!access_ok(user_frame_tail, sizeof(fp)))
			return;

		if (__copy_from_user_inatomic

perf_callchain_user(struct perf_callchain_entry_ctx *entry,				(unsigned long *)(fp -
					(unsigned long)sizeof(buftail));

			if (!access_ok
				(VERIFY_READ, user_frame_tail, sizeof(buftail)))
			if (!access_ok(user_frame_tail, sizeof(buftail)))
				return;

			if (__copy_from_user_inatomic

perf_callchain_user(struct perf_callchain_entry_ctx *entry,					(unsigned long *)(fp - (unsigned long)
						sizeof(buftail_opt_size));

				if (!access_ok(VERIFY_READ, user_frame_tail,
				if (!access_ok(user_frame_tail,
					       sizeof(buftail_opt_size)))
					return;


asmlinkage long sys_rt_sigreturn(struct pt_regs *regs)
	frame = (struct rt_sigframe __user *)regs->sp;

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;

	if (restore_sigframe(regs, frame))

setup_rt_frame(struct ksignal *ksig, sigset_t * set, struct pt_regs *regs)	    get_sigframe(ksig, regs, sizeof(*frame));
	int err = 0;

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	__put_user_error(0, &frame->uc.uc_flags, err);

static inline int do_16(unsigned long inst, struct pt_regs *regs)		unaligned_addr += shift;

	if (load) {
		if (!access_ok(VERIFY_READ, (void *)unaligned_addr, len))
		if (!access_ok((void *)unaligned_addr, len))
			return -EACCES;

		get_data(unaligned_addr, &target_val, len);
		*idx_to_addr(regs, target_idx) = target_val;
	} else {
		if (!access_ok(VERIFY_WRITE, (void *)unaligned_addr, len))
		if (!access_ok((void *)unaligned_addr, len))
			return -EACCES;
		target_val = *idx_to_addr(regs, target_idx);
		set_data((void *)unaligned_addr, target_val, len);

static inline int do_32(unsigned long inst, struct pt_regs *regs)
	if (load) {

		if (!access_ok(VERIFY_READ, (void *)unaligned_addr, len))
		if (!access_ok((void *)unaligned_addr, len))
			return -EACCES;

		get_data(unaligned_addr, &target_val, len);

static inline int do_32(unsigned long inst, struct pt_regs *regs)			*idx_to_addr(regs, RT(inst)) = target_val;
	} else {

		if (!access_ok(VERIFY_WRITE, (void *)unaligned_addr, len))
		if (!access_ok((void *)unaligned_addr, len))
			return -EACCES;

		target_val = *idx_to_addr(regs, RT(inst));

	(((signed long)(((long)get_fs().seg) &	\
		((long)(addr) | (((long)(addr)) + (len)) | (len)))) == 0)

#define access_ok(type, addr, len)		\
#define access_ok(addr, len)		\
	likely(__access_ok((unsigned long)(addr), (unsigned long)(len)))

# define __EX_TABLE_SECTION	".section __ex_table,\"a\"\n"

static inline unsigned long __must_check __clear_user(void __user *to,static inline unsigned long __must_check clear_user(void __user *to,
						    unsigned long n)
{
	if (!access_ok(VERIFY_WRITE, to, n))
	if (!access_ok(to, n))
		return n;
	return __clear_user(to, n);
}

do {									\	long __gu_err = -EFAULT;					\
	const __typeof__(*(ptr)) __user *__gu_ptr = (ptr);		\
	unsigned long __gu_val = 0;					\
	if (access_ok(VERIFY_READ,  __gu_ptr, sizeof(*__gu_ptr)))	\
	if (access_ok( __gu_ptr, sizeof(*__gu_ptr)))	\
		__get_user_common(__gu_val, sizeof(*__gu_ptr),		\
			__gu_ptr, __gu_err);				\
	(x) = (__force __typeof__(x))__gu_val;				\

do {									\	long __pu_err = -EFAULT;					\
	__typeof__(*(ptr)) __user *__pu_ptr = (ptr);			\
	__typeof__(*(ptr)) __pu_val = (__typeof(*ptr))(x);		\
	if (access_ok(VERIFY_WRITE, __pu_ptr, sizeof(*__pu_ptr))) {	\
	if (access_ok(__pu_ptr, sizeof(*__pu_ptr))) {	\
		switch (sizeof(*__pu_ptr)) {				\
		case 1:							\
			__put_user_asm(__pu_val, "stb", __pu_ptr, __pu_err); \

asmlinkage int do_rt_sigreturn(struct switch_stack *sw)	sigset_t set;
	int rval;

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;

	if (__copy_from_user(&set, &frame->uc.uc_sigmask, sizeof(set)))

futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,	int ret = 0;
	u32 prev;

	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

	__asm__ __volatile__ (				\

/* Ensure that addr is below task's addr_limit */
#define __addr_ok(addr) ((unsigned long) addr < get_fs())

#define access_ok(type, addr, size) \
#define access_ok(addr, size) \
	__range_ok((unsigned long)addr, (unsigned long)size)

/*

extern long __put_user_bad(void);({									\
	long __pu_err = -EFAULT;					\
	__typeof__(*(ptr)) *__pu_addr = (ptr);				\
	if (access_ok(VERIFY_WRITE, __pu_addr, size))			\
	if (access_ok(__pu_addr, size))			\
		__put_user_size((x), __pu_addr, (size), __pu_err);	\
	__pu_err;							\
})

struct __large_struct {({									\
	long __gu_err = -EFAULT, __gu_val = 0;				\
	const __typeof__(*(ptr)) * __gu_addr = (ptr);			\
	if (access_ok(VERIFY_READ, __gu_addr, size))			\
	if (access_ok(__gu_addr, size))			\
		__get_user_size(__gu_val, __gu_addr, (size), __gu_err);	\
	(x) = (__force __typeof__(*(ptr)))__gu_val;			\
	__gu_err;							\

extern unsigned long __clear_user(void *addr, unsigned long size);static inline __must_check unsigned long
clear_user(void *addr, unsigned long size)
{
	if (likely(access_ok(VERIFY_WRITE, addr, size)))
	if (likely(access_ok(addr, size)))
		size = __clear_user(addr, size);
	return size;
}

static int restore_sigcontext(struct pt_regs *regs,
	/*
	 * Restore the regs from &sc->regs.
	 * (sc is already checked for VERIFY_READ since the sigframe was
	 * (sc is already checked since the sigframe was
	 *  checked in sys_sigreturn previously)
	 */
	err |= __copy_from_user(regs, sc->regs.gpr, 32 * sizeof(unsigned long));

asmlinkage long _sys_rt_sigreturn(struct pt_regs *regs)	if (((long)frame) & 3)
		goto badframe;

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__copy_from_user(&set, &frame->uc.uc_sigmask, sizeof(set)))
		goto badframe;

static int setup_rt_frame(struct ksignal *ksig, sigset_t *set,
	frame = get_sigframe(ksig, regs, sizeof(*frame));

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	/* Create siginfo.  */

futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,	if (uaccess_kernel() && !uaddr)
		return -EFAULT;

	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

	/* HPPA has no cmpxchg in hardware and therefore the

 * that put_user is the same as __put_user, etc.
 */

#define access_ok(type, uaddr, size)	\
#define access_ok(uaddr, size)	\
	( (uaddr) == (uaddr) )

#define put_user __put_user

futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,	int ret = 0;
	u32 prev;

	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

        __asm__ __volatile__ (

static inline int __access_ok(unsigned long addr, unsigned long size,
#endif

#define access_ok(type, addr, size)		\
#define access_ok(addr, size)		\
	(__chk_user_ptr(addr), (void)(type),		\
	 __access_ok((__force unsigned long)(addr), (size), get_fs()))


do {								\	long __pu_err = -EFAULT;					\
	__typeof__(*(ptr)) __user *__pu_addr = (ptr);			\
	might_fault();							\
	if (access_ok(VERIFY_WRITE, __pu_addr, size))			\
	if (access_ok(__pu_addr, size))			\
		__put_user_size((x), __pu_addr, (size), __pu_err);	\
	__pu_err;							\
})

do {								\	__long_type(*(ptr)) __gu_val = 0;				\
	__typeof__(*(ptr)) __user *__gu_addr = (ptr);		\
	might_fault();							\
	if (access_ok(VERIFY_READ, __gu_addr, (size))) {		\
	if (access_ok(__gu_addr, (size))) {		\
		barrier_nospec();					\
		__get_user_size(__gu_val, __gu_addr, (size), __gu_err);	\
	}								\

extern unsigned long __clear_user(void __user *addr, unsigned long size);static inline unsigned long clear_user(void __user *addr, unsigned long size)
{
	might_fault();
	if (likely(access_ok(VERIFY_WRITE, addr, size)))
	if (likely(access_ok(addr, size)))
		return __clear_user(addr, size);
	return size;
}

static int emulate_spe(struct pt_regs *regs, unsigned int reg,
	/* Verify the address of the operand */
	if (unlikely(user_mode(regs) &&
		     !access_ok((flags & ST ? VERIFY_WRITE : VERIFY_READ),
				addr, nb)))
		     !access_ok(addr, nb)))
		return -EFAULT;

	/* userland only */

static ssize_t validate_flash_write(struct file *file, const char __user *buf,		args_buf->status = VALIDATE_INCOMPLETE;
	}

	if (!access_ok(VERIFY_READ, buf, count)) {
	if (!access_ok(buf, count)) {
		rc = -EFAULT;
		goto done;
	}

static ssize_t rtas_log_read(struct file * file, char __user * buf,
	count = rtas_error_log_buffer_max;

	if (!access_ok(VERIFY_WRITE, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;

	tmp = kmalloc(count, GFP_KERNEL);

void __user *get_sigframe(struct ksignal *ksig, unsigned long sp,	newsp = (oldsp - frame_size) & ~0xFUL;

	/* Check access */
	if (!access_ok(VERIFY_WRITE, (void __user *)newsp, oldsp - newsp))
	if (!access_ok((void __user *)newsp, oldsp - newsp))
		return NULL;

        return (void __user *)newsp;

static int do_setcontext(struct ucontext __user *ucp, struct pt_regs *regs, int#else
	if (__get_user(mcp, &ucp->uc_regs))
		return -EFAULT;
	if (!access_ok(VERIFY_READ, mcp, sizeof(*mcp)))
	if (!access_ok(mcp, sizeof(*mcp)))
		return -EFAULT;
#endif
	set_current_blocked(&set);

SYSCALL_DEFINE3(swapcontext, struct ucontext __user *, old_ctx,		 */
		mctx = (struct mcontext __user *)
			((unsigned long) &old_ctx->uc_mcontext & ~0xfUL);
		if (!access_ok(VERIFY_WRITE, old_ctx, ctx_size)
		if (!access_ok(old_ctx, ctx_size)
		    || save_user_regs(regs, mctx, NULL, 0, ctx_has_vsx_region)
		    || put_sigset_t(&old_ctx->uc_sigmask, &current->blocked)
		    || __put_user(to_user_ptr(mctx), &old_ctx->uc_regs))

SYSCALL_DEFINE3(swapcontext, struct ucontext __user *, old_ctx,	}
	if (new_ctx == NULL)
		return 0;
	if (!access_ok(VERIFY_READ, new_ctx, ctx_size) ||
	if (!access_ok(new_ctx, ctx_size) ||
	    fault_in_pages_readable((u8 __user *)new_ctx, ctx_size))
		return -EFAULT;


SYSCALL_DEFINE0(rt_sigreturn)
	rt_sf = (struct rt_sigframe __user *)
		(regs->gpr[1] + __SIGNAL_FRAMESIZE + 16);
	if (!access_ok(VERIFY_READ, rt_sf, sizeof(*rt_sf)))
	if (!access_ok(rt_sf, sizeof(*rt_sf)))
		goto bad;

#ifdef CONFIG_PPC_TRANSACTIONAL_MEM

SYSCALL_DEFINE3(debug_setcontext, struct ucontext __user *, ctx,	current->thread.debug.dbcr0 = new_dbcr0;
#endif

	if (!access_ok(VERIFY_READ, ctx, sizeof(*ctx)) ||
	if (!access_ok(ctx, sizeof(*ctx)) ||
	    fault_in_pages_readable((u8 __user *)ctx, sizeof(*ctx)))
		return -EFAULT;


SYSCALL_DEFINE0(sigreturn)	{
		sr = (struct mcontext __user *)from_user_ptr(sigctx.regs);
		addr = sr;
		if (!access_ok(VERIFY_READ, sr, sizeof(*sr))
		if (!access_ok(sr, sizeof(*sr))
		    || restore_user_regs(regs, sr, 1))
			goto badframe;
	}

static long restore_sigcontext(struct task_struct *tsk, sigset_t *set, int sig,	err |= __get_user(v_regs, &sc->v_regs);
	if (err)
		return err;
	if (v_regs && !access_ok(VERIFY_READ, v_regs, 34 * sizeof(vector128)))
	if (v_regs && !access_ok(v_regs, 34 * sizeof(vector128)))
		return -EFAULT;
	/* Copy 33 vec registers (vr0..31 and vscr) from the stack */
	if (v_regs != NULL && (msr & MSR_VEC) != 0) {

static long restore_tm_sigcontexts(struct task_struct *tsk,	err |= __get_user(tm_v_regs, &tm_sc->v_regs);
	if (err)
		return err;
	if (v_regs && !access_ok(VERIFY_READ, v_regs, 34 * sizeof(vector128)))
	if (v_regs && !access_ok(v_regs, 34 * sizeof(vector128)))
		return -EFAULT;
	if (tm_v_regs && !access_ok(VERIFY_READ,
				    tm_v_regs, 34 * sizeof(vector128)))
	if (tm_v_regs && !access_ok(tm_v_regs, 34 * sizeof(vector128)))
		return -EFAULT;
	/* Copy 33 vec registers (vr0..31 and vscr) from the stack */
	if (v_regs != NULL && tm_v_regs != NULL && (msr & MSR_VEC) != 0) {

SYSCALL_DEFINE3(swapcontext, struct ucontext __user *, old_ctx,		ctx_has_vsx_region = 1;

	if (old_ctx != NULL) {
		if (!access_ok(VERIFY_WRITE, old_ctx, ctx_size)
		if (!access_ok(old_ctx, ctx_size)
		    || setup_sigcontext(&old_ctx->uc_mcontext, current, 0, NULL, 0,
					ctx_has_vsx_region)
		    || __copy_to_user(&old_ctx->uc_sigmask,

SYSCALL_DEFINE3(swapcontext, struct ucontext __user *, old_ctx,	}
	if (new_ctx == NULL)
		return 0;
	if (!access_ok(VERIFY_READ, new_ctx, ctx_size)
	if (!access_ok(new_ctx, ctx_size)
	    || __get_user(tmp, (u8 __user *) new_ctx)
	    || __get_user(tmp, (u8 __user *) new_ctx + ctx_size - 1))
		return -EFAULT;

SYSCALL_DEFINE0(rt_sigreturn)	/* Always make any pending restarted system calls return -EINTR */
	current->restart_block.fn = do_no_restart_syscall;

	if (!access_ok(VERIFY_READ, uc, sizeof(*uc)))
	if (!access_ok(uc, sizeof(*uc)))
		goto badframe;

	if (__copy_from_user(&set, &uc->uc_sigmask, sizeof(set)))

ppc_select(int n, fd_set __user *inp, fd_set __user *outp, fd_set __user *exp, s	if ( (unsigned long)n >= 4096 )
	{
		unsigned long __user *buffer = (unsigned long __user *)n;
		if (!access_ok(VERIFY_READ, buffer, 5*sizeof(unsigned long))
		if (!access_ok(buffer, 5*sizeof(unsigned long))
		    || __get_user(n, buffer)
		    || __get_user(inp, ((fd_set __user * __user *)(buffer+1)))
		    || __get_user(outp, ((fd_set  __user * __user *)(buffer+2)))

static void p9_hmi_special_emu(struct pt_regs *regs)	addr = (__force const void __user *)ea;

	/* Check it */
	if (!access_ok(VERIFY_READ, addr, 16)) {
	if (!access_ok(addr, 16)) {
		pr_devel("HMI vec emu: bad access %i:%s[%d] nip=%016lx"
			 " instr=%08x addr=%016lx\n",
			 smp_processor_id(), current->comm, current->pid,

static ssize_t kvm_htab_read(struct file *file, char __user *buf,	int first_pass;
	unsigned long hpte[2];

	if (!access_ok(VERIFY_WRITE, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;
	if (kvm_is_radix(kvm))
		return 0;

static ssize_t kvm_htab_write(struct file *file, const char __user *buf,	int mmu_ready;
	int pshift;

	if (!access_ok(VERIFY_READ, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;
	if (kvm_is_radix(kvm))
		return -EINVAL;

__wsum csum_and_copy_from_user(const void __user *src, void *dst,		goto out;
	}

	if (unlikely((len < 0) || !access_ok(VERIFY_READ, src, len))) {
	if (unlikely((len < 0) || !access_ok(src, len))) {
		*err_ptr = -EFAULT;
		csum = (__force unsigned int)sum;
		goto out;

__wsum csum_and_copy_to_user(const void *src, void __user *dst, int len,		goto out;
	}

	if (unlikely((len < 0) || !access_ok(VERIFY_WRITE, dst, len))) {
	if (unlikely((len < 0) || !access_ok(dst, len))) {
		*err_ptr = -EFAULT;
		csum = -1; /* invalid checksum */
		goto out;

static bool bad_stack_expansion(struct pt_regs *regs, unsigned long address,			return false;

		if ((flags & FAULT_FLAG_WRITE) && (flags & FAULT_FLAG_USER) &&
		    access_ok(VERIFY_READ, nip, sizeof(*nip))) {
		    access_ok(nip, sizeof(*nip))) {
			unsigned int inst;
			int res;


SYSCALL_DEFINE3(subpage_prot, unsigned long, addr,		return 0;
	}

	if (!access_ok(VERIFY_READ, map, (len >> PAGE_SHIFT) * sizeof(u32)))
	if (!access_ok(map, (len >> PAGE_SHIFT) * sizeof(u32)))
		return -EFAULT;

	down_write(&mm->mmap_sem);

static unsigned int user_getsp32(unsigned int sp, int is_first)	unsigned int stack_frame[2];
	void __user *p = compat_ptr(sp);

	if (!access_ok(VERIFY_READ, p, sizeof(stack_frame)))
	if (!access_ok(p, sizeof(stack_frame)))
		return 0;

	/*

static unsigned long user_getsp64(unsigned long sp, int is_first){
	unsigned long stack_frame[3];

	if (!access_ok(VERIFY_READ, (void __user *)sp, sizeof(stack_frame)))
	if (!access_ok((void __user *)sp, sizeof(stack_frame)))
		return 0;

	if (__copy_from_user_inatomic(stack_frame, (void __user *)sp,

static ssize_t spufs_mbox_read(struct file *file, char __user *buf,	if (len < 4)
		return -EINVAL;

	if (!access_ok(VERIFY_WRITE, buf, len))
	if (!access_ok(buf, len))
		return -EFAULT;

	udata = (void __user *)buf;

static ssize_t spufs_ibox_read(struct file *file, char __user *buf,	if (len < 4)
		return -EINVAL;

	if (!access_ok(VERIFY_WRITE, buf, len))
	if (!access_ok(buf, len))
		return -EFAULT;

	udata = (void __user *)buf;

static ssize_t spufs_wbox_write(struct file *file, const char __user *buf,		return -EINVAL;

	udata = (void __user *)buf;
	if (!access_ok(VERIFY_READ, buf, len))
	if (!access_ok(buf, len))
		return -EFAULT;

	if (__get_user(wbox_data, udata))

static ssize_t spufs_mbox_info_read(struct file *file, char __user *buf,	int ret;
	struct spu_context *ctx = file->private_data;

	if (!access_ok(VERIFY_WRITE, buf, len))
	if (!access_ok(buf, len))
		return -EFAULT;

	ret = spu_acquire_saved(ctx);

static ssize_t spufs_ibox_info_read(struct file *file, char __user *buf,	struct spu_context *ctx = file->private_data;
	int ret;

	if (!access_ok(VERIFY_WRITE, buf, len))
	if (!access_ok(buf, len))
		return -EFAULT;

	ret = spu_acquire_saved(ctx);

static ssize_t spufs_wbox_info_read(struct file *file, char __user *buf,	struct spu_context *ctx = file->private_data;
	int ret;

	if (!access_ok(VERIFY_WRITE, buf, len))
	if (!access_ok(buf, len))
		return -EFAULT;

	ret = spu_acquire_saved(ctx);

static ssize_t spufs_dma_info_read(struct file *file, char __user *buf,	struct spu_context *ctx = file->private_data;
	int ret;

	if (!access_ok(VERIFY_WRITE, buf, len))
	if (!access_ok(buf, len))
		return -EFAULT;

	ret = spu_acquire_saved(ctx);

static ssize_t __spufs_proxydma_info_read(struct spu_context *ctx,	if (len < ret)
		return -EINVAL;

	if (!access_ok(VERIFY_WRITE, buf, len))
	if (!access_ok(buf, len))
		return -EFAULT;

	info.proxydma_info_type = ctx->csa.prob.dma_querytype_RW;

static ssize_t lpc_debug_read(struct file *filp, char __user *ubuf,	u32 data, pos, len, todo;
	int rc;

	if (!access_ok(VERIFY_WRITE, ubuf, count))
	if (!access_ok(ubuf, count))
		return -EFAULT;

	todo = count;

static ssize_t lpc_debug_write(struct file *filp, const char __user *ubuf,	u32 data, pos, len, todo;
	int rc;

	if (!access_ok(VERIFY_READ, ubuf, count))
	if (!access_ok(ubuf, count))
		return -EFAULT;

	todo = count;

static ssize_t scanlog_read(struct file *file, char __user *buf,		return -EINVAL;
	}

	if (!access_ok(VERIFY_WRITE, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;

	for (;;) {

futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,	u32 val;
	uintptr_t tmp;

	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

	__enable_user_access();

static inline void set_fs(mm_segment_t fs)#define user_addr_max()	(get_fs())


#define VERIFY_READ	0
#define VERIFY_WRITE	1

/**
 * access_ok: - Checks if a user space pointer is valid
 * @type: Type of access: %VERIFY_READ or %VERIFY_WRITE.  Note that
 *        %VERIFY_WRITE is a superset of %VERIFY_READ - if it is safe
 *        to write to a block, it is always safe to read from it.
 * @addr: User space pointer to start of block to check
 * @size: Size of block to check
 *

static inline void set_fs(mm_segment_t fs) * checks that the pointer is in the user space range - after calling
 * this function, memory access functions may still return -EFAULT.
 */
#define access_ok(type, addr, size) ({					\
#define access_ok(addr, size) ({					\
	__chk_user_ptr(addr);						\
	likely(__access_ok((unsigned long __force)(addr), (size)));	\
})

do {								\({								\
	const __typeof__(*(ptr)) __user *__p = (ptr);		\
	might_fault();						\
	access_ok(VERIFY_READ, __p, sizeof(*__p)) ?		\
	access_ok(__p, sizeof(*__p)) ?		\
		__get_user((x), __p) :				\
		((x) = 0, -EFAULT);				\
})

do {								\({								\
	__typeof__(*(ptr)) __user *__p = (ptr);			\
	might_fault();						\
	access_ok(VERIFY_WRITE, __p, sizeof(*__p)) ?		\
	access_ok(__p, sizeof(*__p)) ?		\
		__put_user((x), __p) :				\
		-EFAULT;					\
})

static inlineunsigned long __must_check clear_user(void __user *to, unsigned long n)
{
	might_fault();
	return access_ok(VERIFY_WRITE, to, n) ?
	return access_ok(to, n) ?
		__clear_user(to, n) : n;
}


SYSCALL_DEFINE0(rt_sigreturn)
	frame = (struct rt_sigframe __user *)regs->sp;

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;

	if (__copy_from_user(&set, &frame->uc.uc_sigmask, sizeof(set)))

static int setup_rt_frame(struct ksignal *ksig, sigset_t *set,	long err = 0;

	frame = get_sigframe(ksig, regs, sizeof(*frame));
	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	err |= copy_siginfo_to_user(&frame->info, &ksig->info);

static inline int __range_ok(unsigned long addr, unsigned long size)	__range_ok((unsigned long)(addr), (size));	\
})

#define access_ok(type, addr, size) __access_ok(addr, size)
#define access_ok(addr, size) __access_ok(addr, size)

unsigned long __must_check
raw_copy_from_user(void *to, const void __user *from, unsigned long n);

static inline __wsum csum_and_copy_to_user(const void *src,					   int len, __wsum sum,
					   int *err_ptr)
{
	if (access_ok(VERIFY_WRITE, dst, len))
	if (access_ok(dst, len))
		return csum_partial_copy_generic((__force const void *)src,
						dst, len, sum, NULL, err_ptr);


static inline intfutex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,
			      u32 oldval, u32 newval)
{
	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

	return atomic_futex_op_cmpxchg_inatomic(uval, uaddr, oldval, newval);

 */
#define __access_ok(addr, size)		\
	(__addr_ok((addr) + (size)))
#define access_ok(type, addr, size)	\
#define access_ok(addr, size)	\
	(__chk_user_ptr(addr),		\
	 __access_ok((unsigned long __force)(addr), (size)))


struct __large_struct { unsigned long buf[100]; };	long __gu_err = -EFAULT;					\
	unsigned long __gu_val = 0;					\
	const __typeof__(*(ptr)) *__gu_addr = (ptr);			\
	if (likely(access_ok(VERIFY_READ, __gu_addr, (size))))		\
	if (likely(access_ok(__gu_addr, (size))))		\
		__get_user_size(__gu_val, __gu_addr, (size), __gu_err);	\
	(x) = (__force __typeof__(*(ptr)))__gu_val;			\
	__gu_err;							\

struct __large_struct { unsigned long buf[100]; };	long __pu_err = -EFAULT;				\
	__typeof__(*(ptr)) __user *__pu_addr = (ptr);		\
	__typeof__(*(ptr)) __pu_val = x;			\
	if (likely(access_ok(VERIFY_WRITE, __pu_addr, size)))	\
	if (likely(access_ok(__pu_addr, size)))	\
		__put_user_size(__pu_val, __pu_addr, (size),	\
				__pu_err);			\
	__pu_err;						\

__kernel_size_t __clear_user(void *addr, __kernel_size_t size);	void __user * __cl_addr = (addr);				\
	unsigned long __cl_size = (n);					\
									\
	if (__cl_size && access_ok(VERIFY_WRITE,			\
		((unsigned long)(__cl_addr)), __cl_size))		\
	if (__cl_size && access_ok(__cl_addr, __cl_size))		\
		__cl_size = __clear_user(__cl_addr, __cl_size);		\
									\
	__cl_size;							\

asmlinkage int sys_sigreturn(void)        /* Always make any pending restarted system calls return -EINTR */
	current->restart_block.fn = do_no_restart_syscall;

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;

	if (__get_user(set.sig[0], &frame->sc.oldmask)

asmlinkage int sys_rt_sigreturn(void)	/* Always make any pending restarted system calls return -EINTR */
	current->restart_block.fn = do_no_restart_syscall;

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;

	if (__copy_from_user(&set, &frame->uc.uc_sigmask, sizeof(set)))

static int setup_frame(struct ksignal *ksig, sigset_t *set,
	frame = get_sigframe(&ksig->ka, regs->regs[15], sizeof(*frame));

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	err |= setup_sigcontext(&frame->sc, regs, set->sig[0]);

static int setup_rt_frame(struct ksignal *ksig, sigset_t *set,
	frame = get_sigframe(&ksig->ka, regs->regs[15], sizeof(*frame));

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	err |= copy_siginfo_to_user(&frame->info, &ksig->info);

asmlinkage int sys_sigreturn(unsigned long r2, unsigned long r3,	/* Always make any pending restarted system calls return -EINTR */
	current->restart_block.fn = do_no_restart_syscall;

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;

	if (__get_user(set.sig[0], &frame->sc.oldmask)

asmlinkage int sys_rt_sigreturn(unsigned long r2, unsigned long r3,	/* Always make any pending restarted system calls return -EINTR */
	current->restart_block.fn = do_no_restart_syscall;

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;

	if (__copy_from_user(&set, &frame->uc.uc_sigmask, sizeof(set)))

static int setup_frame(struct ksignal *ksig, sigset_t *set, struct pt_regs *regs
	frame = get_sigframe(&ksig->ka, regs->regs[REG_SP], sizeof(*frame));

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	err |= setup_sigcontext(&frame->sc, regs, set->sig[0]);

static int setup_rt_frame(struct ksignal *kig, sigset_t *set,
	frame = get_sigframe(&ksig->ka, regs->regs[REG_SP], sizeof(*frame));

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	err |= __put_user(&frame->info, &frame->pinfo);

static int read_opcode(reg_size_t pc, insn_size_t *result_opcode, int from_user_		/* SHmedia */
		aligned_pc = pc & ~3;
		if (from_user_mode) {
			if (!access_ok(VERIFY_READ, aligned_pc, sizeof(insn_size_t))) {
			if (!access_ok(aligned_pc, sizeof(insn_size_t))) {
				get_user_error = -EFAULT;
			} else {
				get_user_error = __get_user(opcode, (insn_size_t *)aligned_pc);

static int misaligned_load(struct pt_regs *regs,	if (user_mode(regs)) {
		__u64 buffer;

		if (!access_ok(VERIFY_READ, (unsigned long) address, 1UL<<width_shift)) {
		if (!access_ok((unsigned long) address, 1UL<<width_shift)) {
			return -1;
		}


static int misaligned_store(struct pt_regs *regs,	if (user_mode(regs)) {
		__u64 buffer;

		if (!access_ok(VERIFY_WRITE, (unsigned long) address, 1UL<<width_shift)) {
		if (!access_ok((unsigned long) address, 1UL<<width_shift)) {
			return -1;
		}


static int misaligned_fpu_load(struct pt_regs *regs,		__u64 buffer;
		__u32 buflo, bufhi;

		if (!access_ok(VERIFY_READ, (unsigned long) address, 1UL<<width_shift)) {
		if (!access_ok((unsigned long) address, 1UL<<width_shift)) {
			return -1;
		}


static int misaligned_fpu_store(struct pt_regs *regs,		/* Initialise these to NaNs. */
		__u32 buflo=0xffffffffUL, bufhi=0xffffffffUL;

		if (!access_ok(VERIFY_WRITE, (unsigned long) address, 1UL<<width_shift)) {
		if (!access_ok((unsigned long) address, 1UL<<width_shift)) {
			return -1;
		}


void do_reserved_inst(unsigned long error_code, struct pt_regs *regs)	/* SHmedia : check for defect.  This requires executable vmas
	   to be readable too. */
	aligned_pc = pc & ~3;
	if (!access_ok(VERIFY_READ, aligned_pc, sizeof(insn_size_t)))
	if (!access_ok(aligned_pc, sizeof(insn_size_t)))
		get_user_error = -EFAULT;
	else
		get_user_error = __get_user(opcode, (insn_size_t *)aligned_pc);

int __get_user_pages_fast(unsigned long start, int nr_pages, int write,	addr = start;
	len = (unsigned long) nr_pages << PAGE_SHIFT;
	end = start + len;
	if (unlikely(!access_ok(write ? VERIFY_WRITE : VERIFY_READ,
					(void __user *)start, len)))
	if (unlikely(!access_ok((void __user *)start, len)))
		return 0;

	/*

user_backtrace(unsigned long *stackaddr, struct pt_regs *regs)	unsigned long buf_stack;

	/* Also check accessibility of address */
	if (!access_ok(VERIFY_READ, stackaddr, sizeof(unsigned long)))
	if (!access_ok(stackaddr, sizeof(unsigned long)))
		return NULL;

	if (__copy_from_user_inatomic(&buf_stack, stackaddr, sizeof(unsigned long)))

static inline __wsumcsum_partial_copy_to_user(const void *src, void __user *dst, int len,
			  __wsum sum, int *err)
{
	if (!access_ok (VERIFY_WRITE, dst, len)) {
	if (!access_ok(dst, len)) {
		*err = -EFAULT;
		return sum;
	} else {

#define __user_ok(addr, size) ({ (void)(size); (addr) < STACK_TOP; })
#define __kernel_ok (uaccess_kernel())
#define __access_ok(addr, size) (__user_ok((addr) & get_fs().seg, (size)))
#define access_ok(type, addr, size) \
#define access_ok(addr, size) \
	({ (void)(type); __access_ok((unsigned long)(addr), size); })

/*

static inline int __access_ok(const void __user * addr, unsigned long size)	return 1;
}

static inline int access_ok(int type, const void __user * addr, unsigned long size)
static inline int access_ok(const void __user * addr, unsigned long size)
{
	return 1;
}

int restore_fpu_state(struct pt_regs *regs, __siginfo_fpu_t __user *fpu)	set_used_math();
	clear_tsk_thread_flag(current, TIF_USEDFPU);

	if (!access_ok(VERIFY_READ, fpu, sizeof(*fpu)))
	if (!access_ok(fpu, sizeof(*fpu)))
		return -EFAULT;

	err = __copy_from_user(&current->thread.float_regs[0], &fpu->si_float_regs[0],

static inline int ok_for_user(struct pt_regs *regs, unsigned int insn,			      enum direction dir)
{
	unsigned int reg;
	int check = (dir == load) ? VERIFY_READ : VERIFY_WRITE;
	int size = ((insn >> 19) & 3) == 3 ? 8 : 4;

	if ((regs->pc | regs->npc) & 3)

static inline int ok_for_user(struct pt_regs *regs, unsigned int insn,
	reg = (insn >> 25) & 0x1f;
	if (reg >= 16) {
		if (!access_ok(check, WINREG_ADDR(reg - 16), size))
		if (!access_ok(WINREG_ADDR(reg - 16), size))
			return -EFAULT;
	}
	reg = (insn >> 14) & 0x1f;
	if (reg >= 16) {
		if (!access_ok(check, WINREG_ADDR(reg - 16), size))
		if (!access_ok(WINREG_ADDR(reg - 16), size))
			return -EFAULT;
	}
	if (!(insn & 0x2000)) {
		reg = (insn & 0x1f);
		if (reg >= 16) {
			if (!access_ok(check, WINREG_ADDR(reg - 16), size))
			if (!access_ok(WINREG_ADDR(reg - 16), size))
				return -EFAULT;
		}
	}

long arch_ptrace(struct task_struct *child, long request,
#ifdef PTRACE_GETREGS
	case PTRACE_GETREGS: { /* Get all gp regs from the child. */
		if (!access_ok(VERIFY_WRITE, p, MAX_REG_OFFSET)) {
		if (!access_ok(p, MAX_REG_OFFSET)) {
			ret = -EIO;
			break;
		}

long arch_ptrace(struct task_struct *child, long request,#ifdef PTRACE_SETREGS
	case PTRACE_SETREGS: { /* Set all gp regs in the child. */
		unsigned long tmp = 0;
		if (!access_ok(VERIFY_READ, p, MAX_REG_OFFSET)) {
		if (!access_ok(p, MAX_REG_OFFSET)) {
			ret = -EIO;
			break;
		}

asmlinkage int __sys_rt_sigreturn(struct pt_regs *regs)
	frame = (struct rt_sigframe __user *)regs->UCreg_sp;

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;

	if (restore_sigframe(regs, &frame->sig))

static inline void __user *get_sigframe(struct k_sigaction *ka,	/*
	 * Check that we can actually write to the signal frame.
	 */
	if (!access_ok(VERIFY_WRITE, frame, framesize))
	if (!access_ok(frame, framesize))
		frame = NULL;

	return frame;

static bool write_ok_or_segv(unsigned long ptr, size_t size)	 * sig_on_uaccess_err, this could go away.
	 */

	if (!access_ok(VERIFY_WRITE, (void __user *)ptr, size)) {
	if (!access_ok((void __user *)ptr, size)) {
		struct thread_struct *thread = &current->thread;

		thread->error_code	= X86_PF_USER | X86_PF_WRITE;

static int aout_core_dump(struct coredump_params *cprm)
	/* make sure we actually have a data and stack area to dump */
	set_fs(USER_DS);
	if (!access_ok(VERIFY_READ, (void *) (unsigned long)START_DATA(dump),
	if (!access_ok((void *) (unsigned long)START_DATA(dump),
		       dump.u_dsize << PAGE_SHIFT))
		dump.u_dsize = 0;
	if (!access_ok(VERIFY_READ, (void *) (unsigned long)START_STACK(dump),
	if (!access_ok((void *) (unsigned long)START_STACK(dump),
		       dump.u_ssize << PAGE_SHIFT))
		dump.u_ssize = 0;


asmlinkage long sys32_sigreturn(void)	struct sigframe_ia32 __user *frame = (struct sigframe_ia32 __user *)(regs->sp-8);
	sigset_t set;

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__get_user(set.sig[0], &frame->sc.oldmask)
	    || (_COMPAT_NSIG_WORDS > 1

asmlinkage long sys32_rt_sigreturn(void)
	frame = (struct rt_sigframe_ia32 __user *)(regs->sp - 4);

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__copy_from_user(&set, &frame->uc.uc_sigmask, sizeof(set)))
		goto badframe;

int ia32_setup_frame(int sig, struct ksignal *ksig,
	frame = get_sigframe(ksig, regs, sizeof(*frame), &fpstate);

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	if (__put_user(sig, &frame->sig))

int ia32_setup_rt_frame(int sig, struct ksignal *ksig,
	frame = get_sigframe(ksig, regs, sizeof(*frame), &fpstate);

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	put_user_try {

static int cp_stat64(struct stat64 __user *ubuf, struct kstat *stat)	typeof(ubuf->st_gid) gid = 0;
	SET_UID(uid, from_kuid_munged(current_user_ns(), stat->uid));
	SET_GID(gid, from_kgid_munged(current_user_ns(), stat->gid));
	if (!access_ok(VERIFY_WRITE, ubuf, sizeof(struct stat64)) ||
	if (!access_ok(ubuf, sizeof(struct stat64)) ||
	    __put_user(huge_encode_dev(stat->dev), &ubuf->st_dev) ||
	    __put_user(stat->ino, &ubuf->__st_ino) ||
	    __put_user(stat->ino, &ubuf->st_ino) ||

static inline __wsum csum_and_copy_to_user(const void *src,	__wsum ret;

	might_sleep();
	if (access_ok(VERIFY_WRITE, dst, len)) {
	if (access_ok(dst, len)) {
		stac();
		ret = csum_partial_copy_generic(src, (__force void *)dst,
						len, sum, NULL, err_ptr);

void sync_initial_page_table(void);/*
 * Define this if things work differently on an i386 and an i486:
 * it will (on an i486) warn about kernel memory accesses that are
 * done without a 'access_ok(VERIFY_WRITE,..)'
 * done without a 'access_ok( ..)'
 */
#undef TEST_ACCESS_OK


static inline bool __chk_range_not_ok(unsigned long addr, unsigned long size, un
/**
 * access_ok: - Checks if a user space pointer is valid
 * @type: Type of access: %VERIFY_READ or %VERIFY_WRITE.  Note that
 *        %VERIFY_WRITE is a superset of %VERIFY_READ - if it is safe
 *        to write to a block, it is always safe to read from it.
 * @addr: User space pointer to start of block to check
 * @size: Size of block to check
 *

static inline bool __chk_range_not_ok(unsigned long addr, unsigned long size, un * checks that the pointer is in the user space range - after calling
 * this function, memory access functions may still return -EFAULT.
 */
#define access_ok(type, addr, size)					\
#define access_ok(addr, size)					\
({									\
	WARN_ON_IN_IRQ();						\
	likely(!__range_not_ok(addr, size, user_addr_max()));		\

extern void __cmpxchg_wrong_size(void)
#define user_atomic_cmpxchg_inatomic(uval, ptr, old, new)		\
({									\
	access_ok(VERIFY_WRITE, (ptr), sizeof(*(ptr))) ?		\
	access_ok((ptr), sizeof(*(ptr))) ?		\
		__user_atomic_cmpxchg_inatomic((uval), (ptr),		\
				(old), (new), sizeof(*(ptr))) :		\
		-EFAULT;						\

int copy_fpstate_to_sigframe(void __user *buf, void __user *buf_fx, int size)	ia32_fxstate &= (IS_ENABLED(CONFIG_X86_32) ||
			 IS_ENABLED(CONFIG_IA32_EMULATION));

	if (!access_ok(VERIFY_WRITE, buf, size))
	if (!access_ok(buf, size))
		return -EACCES;

	if (!static_cpu_has(X86_FEATURE_FPU))

static int __fpu__restore_sig(void __user *buf, void __user *buf_fx, int size)		return 0;
	}

	if (!access_ok(VERIFY_READ, buf, size))
	if (!access_ok(buf, size))
		return -EACCES;

	fpu__initialize(fpu);

__setup_frame(int sig, struct ksignal *ksig, sigset_t *set,
	frame = get_sigframe(&ksig->ka, regs, sizeof(*frame), &fpstate);

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	if (__put_user(sig, &frame->sig))

static int __setup_rt_frame(int sig, struct ksignal *ksig,
	frame = get_sigframe(&ksig->ka, regs, sizeof(*frame), &fpstate);

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	put_user_try {

static int __setup_rt_frame(int sig, struct ksignal *ksig,
	frame = get_sigframe(&ksig->ka, regs, sizeof(struct rt_sigframe), &fp);

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	if (ksig->ka.sa.sa_flags & SA_SIGINFO) {

static int x32_setup_rt_frame(struct ksignal *ksig,
	frame = get_sigframe(&ksig->ka, regs, sizeof(*frame), &fpstate);

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return -EFAULT;

	if (ksig->ka.sa.sa_flags & SA_SIGINFO) {

SYSCALL_DEFINE0(sigreturn)
	frame = (struct sigframe __user *)(regs->sp - 8);

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__get_user(set.sig[0], &frame->sc.oldmask) || (_NSIG_WORDS > 1
		&& __copy_from_user(&set.sig[1], &frame->extramask,

SYSCALL_DEFINE0(rt_sigreturn)	unsigned long uc_flags;

	frame = (struct rt_sigframe __user *)(regs->sp - sizeof(long));
	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__copy_from_user(&set, &frame->uc.uc_sigmask, sizeof(set)))
		goto badframe;

asmlinkage long sys32_x32_rt_sigreturn(void)
	frame = (struct rt_sigframe_x32 __user *)(regs->sp - 8);

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;
	if (__copy_from_user(&set, &frame->uc.uc_sigmask, sizeof(set)))
		goto badframe;

copy_stack_frame(const void __user *fp, struct stack_frame_user *frame){
	int ret;

	if (!access_ok(VERIFY_READ, fp, sizeof(*frame)))
	if (!access_ok(fp, sizeof(*frame)))
		return 0;

	ret = 1;

void save_v86_state(struct kernel_vm86_regs *regs, int retval)	set_flags(regs->pt.flags, VEFLAGS, X86_EFLAGS_VIF | vm86->veflags_mask);
	user = vm86->user_vm86;

	if (!access_ok(VERIFY_WRITE, user, vm86->vm86plus.is_vm86pus ?
	if (!access_ok(user, vm86->vm86plus.is_vm86pus ?
		       sizeof(struct vm86plus_struct) :
		       sizeof(struct vm86_struct))) {
		pr_alert("could not access userspace vm86 info\n");

static long do_sys_vm86(struct vm86plus_struct __user *user_vm86, bool plus)	if (vm86->saved_sp0)
		return -EPERM;

	if (!access_ok(VERIFY_READ, user_vm86, plus ?
	if (!access_ok(user_vm86, plus ?
		       sizeof(struct vm86_struct) :
		       sizeof(struct vm86plus_struct)))
		return -EFAULT;

csum_partial_copy_from_user(const void __user *src, void *dst,	might_sleep();
	*errp = 0;

	if (!likely(access_ok(VERIFY_READ, src, len)))
	if (!likely(access_ok(src, len)))
		goto out_err;

	/*

csum_partial_copy_to_user(const void *src, void __user *dst,
	might_sleep();

	if (unlikely(!access_ok(VERIFY_WRITE, dst, len))) {
	if (unlikely(!access_ok(dst, len))) {
		*errp = -EFAULT;
		return 0;
	}

unsigned longclear_user(void __user *to, unsigned long n)
{
	might_fault();
	if (access_ok(VERIFY_WRITE, to, n))
	if (access_ok(to, n))
		__do_clear_user(to, n);
	return n;
}

EXPORT_SYMBOL(__clear_user);
unsigned long clear_user(void __user *to, unsigned long n)
{
	if (access_ok(VERIFY_WRITE, to, n))
	if (access_ok(to, n))
		return __clear_user(to, n);
	return n;
}

static inline bool seg_writable(struct desc_struct *d)#define instruction_address	(*(struct address *)&I387->soft.fip)
#define operand_address		(*(struct address *)&I387->soft.foo)

#define FPU_access_ok(x,y,z)	if ( !access_ok(x,y,z) ) \
#define FPU_access_ok(y,z)	if ( !access_ok(y,z) ) \
				math_abort(FPU_info,SIGSEGV)
#define FPU_abort		math_abort(FPU_info, SIGSEGV)


static inline bool seg_writable(struct desc_struct *d)/* A simpler test than access_ok() can probably be done for
   FPU_code_access_ok() because the only possible error is to step
   past the upper boundary of a legal code area. */
#define	FPU_code_access_ok(z) FPU_access_ok(VERIFY_READ,(void __user *)FPU_EIP,z)
#define	FPU_code_access_ok(z) FPU_access_ok((void __user *)FPU_EIP,z)
#endif

#define FPU_get_user(x,y)       get_user((x),(y))

int FPU_load_store(u_char type, fpu_addr_modes addr_modes,		break;
	case 024:		/* fldcw */
		RE_ENTRANT_CHECK_OFF;
		FPU_access_ok(VERIFY_READ, data_address, 2);
		FPU_access_ok(data_address, 2);
		FPU_get_user(control_word,
			     (unsigned short __user *)data_address);
		RE_ENTRANT_CHECK_ON;

int FPU_load_store(u_char type, fpu_addr_modes addr_modes,		break;
	case 034:		/* fstcw m16int */
		RE_ENTRANT_CHECK_OFF;
		FPU_access_ok(VERIFY_WRITE, data_address, 2);
		FPU_access_ok(data_address, 2);
		FPU_put_user(control_word,
			     (unsigned short __user *)data_address);
		RE_ENTRANT_CHECK_ON;

int FPU_load_store(u_char type, fpu_addr_modes addr_modes,		break;
	case 036:		/* fstsw m2byte */
		RE_ENTRANT_CHECK_OFF;
		FPU_access_ok(VERIFY_WRITE, data_address, 2);
		FPU_access_ok(data_address, 2);
		FPU_put_user(status_word(),
			     (unsigned short __user *)data_address);
		RE_ENTRANT_CHECK_ON;

int FPU_load_extended(long double __user *s, int stnr)	FPU_REG *sti_ptr = &st(stnr);

	RE_ENTRANT_CHECK_OFF;
	FPU_access_ok(VERIFY_READ, s, 10);
	FPU_access_ok(s, 10);
	__copy_from_user(sti_ptr, s, 10);
	RE_ENTRANT_CHECK_ON;


int FPU_load_double(double __user *dfloat, FPU_REG *loaded_data)	unsigned m64, l64;

	RE_ENTRANT_CHECK_OFF;
	FPU_access_ok(VERIFY_READ, dfloat, 8);
	FPU_access_ok(dfloat, 8);
	FPU_get_user(m64, 1 + (unsigned long __user *)dfloat);
	FPU_get_user(l64, (unsigned long __user *)dfloat);
	RE_ENTRANT_CHECK_ON;

int FPU_load_single(float __user *single, FPU_REG *loaded_data)	int exp, tag, negative;

	RE_ENTRANT_CHECK_OFF;
	FPU_access_ok(VERIFY_READ, single, 4);
	FPU_access_ok(single, 4);
	FPU_get_user(m32, (unsigned long __user *)single);
	RE_ENTRANT_CHECK_ON;


int FPU_load_int64(long long __user *_s)	FPU_REG *st0_ptr = &st(0);

	RE_ENTRANT_CHECK_OFF;
	FPU_access_ok(VERIFY_READ, _s, 8);
	FPU_access_ok(_s, 8);
	if (copy_from_user(&s, _s, 8))
		FPU_abort;
	RE_ENTRANT_CHECK_ON;

int FPU_load_int32(long __user *_s, FPU_REG *loaded_data)	int negative;

	RE_ENTRANT_CHECK_OFF;
	FPU_access_ok(VERIFY_READ, _s, 4);
	FPU_access_ok(_s, 4);
	FPU_get_user(s, _s);
	RE_ENTRANT_CHECK_ON;


int FPU_load_int16(short __user *_s, FPU_REG *loaded_data)	int s, negative;

	RE_ENTRANT_CHECK_OFF;
	FPU_access_ok(VERIFY_READ, _s, 2);
	FPU_access_ok(_s, 2);
	/* Cast as short to get the sign extended. */
	FPU_get_user(s, _s);
	RE_ENTRANT_CHECK_ON;

int FPU_load_bcd(u_char __user *s)	int sign;

	RE_ENTRANT_CHECK_OFF;
	FPU_access_ok(VERIFY_READ, s, 10);
	FPU_access_ok(s, 10);
	RE_ENTRANT_CHECK_ON;
	for (pos = 8; pos >= 0; pos--) {
		l *= 10;

int FPU_store_extended(FPU_REG *st0_ptr, u_char st0_tag,
	if (st0_tag != TAG_Empty) {
		RE_ENTRANT_CHECK_OFF;
		FPU_access_ok(VERIFY_WRITE, d, 10);
		FPU_access_ok(d, 10);

		FPU_put_user(st0_ptr->sigl, (unsigned long __user *)d);
		FPU_put_user(st0_ptr->sigh,

int FPU_store_extended(FPU_REG *st0_ptr, u_char st0_tag,		/* The masked response */
		/* Put out the QNaN indefinite */
		RE_ENTRANT_CHECK_OFF;
		FPU_access_ok(VERIFY_WRITE, d, 10);
		FPU_access_ok(d, 10);
		FPU_put_user(0, (unsigned long __user *)d);
		FPU_put_user(0xc0000000, 1 + (unsigned long __user *)d);
		FPU_put_user(0xffff, 4 + (short __user *)d);

int FPU_store_double(FPU_REG *st0_ptr, u_char st0_tag, double __user *dfloat)			/* The masked response */
			/* Put out the QNaN indefinite */
			RE_ENTRANT_CHECK_OFF;
			FPU_access_ok(VERIFY_WRITE, dfloat, 8);
			FPU_access_ok(dfloat, 8);
			FPU_put_user(0, (unsigned long __user *)dfloat);
			FPU_put_user(0xfff80000,
				     1 + (unsigned long __user *)dfloat);

int FPU_store_double(FPU_REG *st0_ptr, u_char st0_tag, double __user *dfloat)		l[1] |= 0x80000000;

	RE_ENTRANT_CHECK_OFF;
	FPU_access_ok(VERIFY_WRITE, dfloat, 8);
	FPU_access_ok(dfloat, 8);
	FPU_put_user(l[0], (unsigned long __user *)dfloat);
	FPU_put_user(l[1], 1 + (unsigned long __user *)dfloat);
	RE_ENTRANT_CHECK_ON;

int FPU_store_single(FPU_REG *st0_ptr, u_char st0_tag, float __user *single)			/* The masked response */
			/* Put out the QNaN indefinite */
			RE_ENTRANT_CHECK_OFF;
			FPU_access_ok(VERIFY_WRITE, single, 4);
			FPU_access_ok(single, 4);
			FPU_put_user(0xffc00000,
				     (unsigned long __user *)single);
			RE_ENTRANT_CHECK_ON;

int FPU_store_single(FPU_REG *st0_ptr, u_char st0_tag, float __user *single)		templ |= 0x80000000;

	RE_ENTRANT_CHECK_OFF;
	FPU_access_ok(VERIFY_WRITE, single, 4);
	FPU_access_ok(single, 4);
	FPU_put_user(templ, (unsigned long __user *)single);
	RE_ENTRANT_CHECK_ON;


int FPU_store_int64(FPU_REG *st0_ptr, u_char st0_tag, long long __user *d)	}

	RE_ENTRANT_CHECK_OFF;
	FPU_access_ok(VERIFY_WRITE, d, 8);
	FPU_access_ok(d, 8);
	if (copy_to_user(d, &tll, 8))
		FPU_abort;
	RE_ENTRANT_CHECK_ON;

int FPU_store_int32(FPU_REG *st0_ptr, u_char st0_tag, long __user *d)	}

	RE_ENTRANT_CHECK_OFF;
	FPU_access_ok(VERIFY_WRITE, d, 4);
	FPU_access_ok(d, 4);
	FPU_put_user(t.sigl, (unsigned long __user *)d);
	RE_ENTRANT_CHECK_ON;


int FPU_store_int16(FPU_REG *st0_ptr, u_char st0_tag, short __user *d)	}

	RE_ENTRANT_CHECK_OFF;
	FPU_access_ok(VERIFY_WRITE, d, 2);
	FPU_access_ok(d, 2);
	FPU_put_user((short)t.sigl, d);
	RE_ENTRANT_CHECK_ON;


int FPU_store_bcd(FPU_REG *st0_ptr, u_char st0_tag, u_char __user *d)		if (control_word & CW_Invalid) {
			/* Produce the QNaN "indefinite" */
			RE_ENTRANT_CHECK_OFF;
			FPU_access_ok(VERIFY_WRITE, d, 10);
			FPU_access_ok(d, 10);
			for (i = 0; i < 7; i++)
				FPU_put_user(0, d + i);	/* These bytes "undefined" */
			FPU_put_user(0xc0, d + 7);	/* This byte "undefined" */

int FPU_store_bcd(FPU_REG *st0_ptr, u_char st0_tag, u_char __user *d)	}

	RE_ENTRANT_CHECK_OFF;
	FPU_access_ok(VERIFY_WRITE, d, 10);
	FPU_access_ok(d, 10);
	RE_ENTRANT_CHECK_ON;
	for (i = 0; i < 9; i++) {
		b = FPU_div_small(&ll, 10);

u_char __user *fldenv(fpu_addr_modes addr_modes, u_char __user *s)	    ((addr_modes.default_mode == PM16)
	     ^ (addr_modes.override.operand_size == OP_SIZE_PREFIX))) {
		RE_ENTRANT_CHECK_OFF;
		FPU_access_ok(VERIFY_READ, s, 0x0e);
		FPU_access_ok(s, 0x0e);
		FPU_get_user(control_word, (unsigned short __user *)s);
		FPU_get_user(partial_status, (unsigned short __user *)(s + 2));
		FPU_get_user(tag_word, (unsigned short __user *)(s + 4));

u_char __user *fldenv(fpu_addr_modes addr_modes, u_char __user *s)		}
	} else {
		RE_ENTRANT_CHECK_OFF;
		FPU_access_ok(VERIFY_READ, s, 0x1c);
		FPU_access_ok(s, 0x1c);
		FPU_get_user(control_word, (unsigned short __user *)s);
		FPU_get_user(partial_status, (unsigned short __user *)(s + 4));
		FPU_get_user(tag_word, (unsigned short __user *)(s + 8));

void frstor(fpu_addr_modes addr_modes, u_char __user *data_address)
	/* Copy all registers in stack order. */
	RE_ENTRANT_CHECK_OFF;
	FPU_access_ok(VERIFY_READ, s, 80);
	FPU_access_ok(s, 80);
	__copy_from_user(register_base + offset, s, other);
	if (offset)
		__copy_from_user(register_base, s + other, offset);

u_char __user *fstenv(fpu_addr_modes addr_modes, u_char __user *d)	    ((addr_modes.default_mode == PM16)
	     ^ (addr_modes.override.operand_size == OP_SIZE_PREFIX))) {
		RE_ENTRANT_CHECK_OFF;
		FPU_access_ok(VERIFY_WRITE, d, 14);
		FPU_access_ok(d, 14);
#ifdef PECULIAR_486
		FPU_put_user(control_word & ~0xe080, (unsigned long __user *)d);
#else

u_char __user *fstenv(fpu_addr_modes addr_modes, u_char __user *d)		d += 0x0e;
	} else {
		RE_ENTRANT_CHECK_OFF;
		FPU_access_ok(VERIFY_WRITE, d, 7 * 4);
		FPU_access_ok(d, 7 * 4);
#ifdef PECULIAR_486
		control_word &= ~0xe080;
		/* An 80486 sets nearly all of the reserved bits to 1. */

void fsave(fpu_addr_modes addr_modes, u_char __user *data_address)	d = fstenv(addr_modes, data_address);

	RE_ENTRANT_CHECK_OFF;
	FPU_access_ok(VERIFY_WRITE, d, 80);
	FPU_access_ok(d, 80);

	/* Copy all registers in stack order. */
	if (__copy_to_user(d, register_base + offset, other))

static int get_bt_addr(struct mm_struct *mm,	unsigned long bd_entry;
	unsigned long bt_addr;

	if (!access_ok(VERIFY_READ, (bd_entry_ptr), sizeof(*bd_entry_ptr)))
	if (!access_ok((bd_entry_ptr), sizeof(*bd_entry_ptr)))
		return -EFAULT;

	while (1) {

static __inline__ __wsum csum_and_copy_to_user(const void *src,						     void __user *dst,
						     int len, __wsum sum, int *err_ptr)
{
	if (access_ok(VERIFY_WRITE, dst, len)) {
	if (access_ok(dst, len)) {
		if (copy_to_user(dst, src, len)) {
			*err_ptr = -EFAULT;
			return (__force __wsum)-1;

int setup_signal_stack_sc(unsigned long stack_top, struct ksignal *ksig,	/* This is the same calculation as i386 - ((sp + 4) & 15) == 0 */
	stack_top = ((stack_top + 4) & -16UL) - 4;
	frame = (struct sigframe __user *) stack_top - 1;
	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return 1;

	restorer = frame->retcode;

int setup_signal_stack_si(unsigned long stack_top, struct ksignal *ksig,
	stack_top &= -8UL;
	frame = (struct rt_sigframe __user *) stack_top - 1;
	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		return 1;

	restorer = frame->retcode;

int setup_signal_stack_si(unsigned long stack_top, struct ksignal *ksig,	/* Subtract 128 for a red zone and 8 for proper alignment */
	frame = (struct rt_sigframe __user *) ((unsigned long) frame - 128 - 8);

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto out;

	if (ksig->ka.sa.sa_flags & SA_SIGINFO) {

static __inline__ __wsum csum_and_copy_to_user(const void *src,					       void __user *dst, int len,
					       __wsum sum, int *err_ptr)
{
	if (access_ok(VERIFY_WRITE, dst, len))
	if (access_ok(dst, len))
		return csum_partial_copy_generic(src,dst,len,sum,NULL,err_ptr);

	if (len)

futex_atomic_cmpxchg_inatomic(u32 *uval, u32 __user *uaddr,{
	int ret = 0;

	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

#if !XCHAL_HAVE_S32C1I

#define __user_ok(addr, size) \
		(((size) <= TASK_SIZE)&&((addr) <= TASK_SIZE-(size)))
#define __access_ok(addr, size) (__kernel_ok || __user_ok((addr), (size)))
#define access_ok(type, addr, size) __access_ok((unsigned long)(addr), (size))
#define access_ok(addr, size) __access_ok((unsigned long)(addr), (size))

#define user_addr_max() (uaccess_kernel() ? ~0UL : TASK_SIZE)


extern long __put_user_bad(void);({									\
	long __pu_err = -EFAULT;					\
	__typeof__(*(ptr)) *__pu_addr = (ptr);				\
	if (access_ok(VERIFY_WRITE, __pu_addr, size))			\
	if (access_ok(__pu_addr, size))			\
		__put_user_size((x), __pu_addr, (size), __pu_err);	\
	__pu_err;							\
})

__asm__ __volatile__(					\({									\
	long __gu_err = -EFAULT, __gu_val = 0;				\
	const __typeof__(*(ptr)) *__gu_addr = (ptr);			\
	if (access_ok(VERIFY_READ, __gu_addr, size))			\
	if (access_ok(__gu_addr, size))			\
		__get_user_size(__gu_val, __gu_addr, (size), __gu_err);	\
	(x) = (__force __typeof__(*(ptr)))__gu_val;			\
	__gu_err;							\

__xtensa_clear_user(void *addr, unsigned long size)static inline unsigned long
clear_user(void *addr, unsigned long size)
{
	if (access_ok(VERIFY_WRITE, addr, size))
	if (access_ok(addr, size))
		return __xtensa_clear_user(addr, size);
	return size ? -EFAULT : 0;
}

extern long __strncpy_user(char *, const char *, long);static inline long
strncpy_from_user(char *dst, const char *src, long count)
{
	if (access_ok(VERIFY_READ, src, 1))
	if (access_ok(src, 1))
		return __strncpy_user(dst, src, count);
	return -EFAULT;
}

asmlinkage long xtensa_rt_sigreturn(long a0, long a1, long a2, long a3,
	frame = (struct rt_sigframe __user *) regs->areg[1];

	if (!access_ok(VERIFY_READ, frame, sizeof(*frame)))
	if (!access_ok(frame, sizeof(*frame)))
		goto badframe;

	if (__copy_from_user(&set, &frame->uc.uc_sigmask, sizeof(set)))

static int setup_frame(struct ksignal *ksig, sigset_t *set,	if (regs->depc > 64)
		panic ("Double exception sys_sigreturn\n");

	if (!access_ok(VERIFY_WRITE, frame, sizeof(*frame))) {
	if (!access_ok(frame, sizeof(*frame))) {
		return -EFAULT;
	}


void xtensa_backtrace_user(struct pt_regs *regs, unsigned int depth,		pc = MAKE_PC_FROM_RA(a0, pc);

		/* Check if the region is OK to access. */
		if (!access_ok(VERIFY_READ, &SPILL_SLOT(a1, 0), 8))
		if (!access_ok(&SPILL_SLOT(a1, 0), 8))
			return;
		/* Copy a1, a0 from user space stack frame. */
		if (__get_user(a0, &SPILL_SLOT(a1, 0)) ||

static ssize_t acpi_aml_read(struct file *file, char __user *buf,
	if (!count)
		return 0;
	if (!access_ok(VERIFY_WRITE, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;

	while (count > 0) {

static ssize_t acpi_aml_write(struct file *file, const char __user *buf,
	if (!count)
		return 0;
	if (!access_ok(VERIFY_READ, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;

	while (count > 0) {

static ssize_t read_nvram(struct file *file, char __user *buf,	unsigned int i;
	char __user *p = buf;

	if (!access_ok(VERIFY_WRITE, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;
	if (*ppos >= nvram_len)
		return 0;

static ssize_t write_nvram(struct file *file, const char __user *buf,	const char __user *p = buf;
	char c;

	if (!access_ok(VERIFY_READ, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;
	if (*ppos >= nvram_len)
		return 0;

static ssize_t read_port(struct file *file, char __user *buf,	unsigned long i = *ppos;
	char __user *tmp = buf;

	if (!access_ok(VERIFY_WRITE, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;
	while (count-- > 0 && i < 65536) {
		if (__put_user(inb(i), tmp) < 0)

static ssize_t write_port(struct file *file, const char __user *buf,	unsigned long i = *ppos;
	const char __user *tmp = buf;

	if (!access_ok(VERIFY_READ, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;
	while (count-- > 0 && i < 65536) {
		char c;

static ssize_t flash_write(struct file *file, const char __user *buf,	if (count > gbFlashSize - p)
		count = gbFlashSize - p;
			
	if (!access_ok(VERIFY_READ, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;

	/*

static long cmm_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)	      _IOC_DIR(cmd), _IOC_READ, _IOC_WRITE, size, cmd);

	if (_IOC_DIR(cmd) & _IOC_READ) {
		if (!access_ok(VERIFY_WRITE, argp, size))
		if (!access_ok(argp, size))
			goto out;
	}
	if (_IOC_DIR(cmd) & _IOC_WRITE) {
		if (!access_ok(VERIFY_READ, argp, size))
		if (!access_ok(argp, size))
			goto out;
	}
	rc = 0;

static int sev_ioctl_do_pek_csr(struct sev_issue_cmd *argp)		goto cmd;

	/* allocate a physically contiguous buffer to store the CSR blob */
	if (!access_ok(VERIFY_WRITE, input.address, input.length) ||
	if (!access_ok(input.address, input.length) ||
	    input.length > SEV_FW_BLOB_MAX_SIZE) {
		ret = -EFAULT;
		goto e_free;

static int sev_ioctl_do_pdh_export(struct sev_issue_cmd *argp)
	/* Allocate a physically contiguous buffer to store the PDH blob. */
	if ((input.pdh_cert_len > SEV_FW_BLOB_MAX_SIZE) ||
	    !access_ok(VERIFY_WRITE, input.pdh_cert_address, input.pdh_cert_len)) {
	    !access_ok(input.pdh_cert_address, input.pdh_cert_len)) {
		ret = -EFAULT;
		goto e_free;
	}

	/* Allocate a physically contiguous buffer to store the cert chain blob. */
	if ((input.cert_chain_len > SEV_FW_BLOB_MAX_SIZE) ||
	    !access_ok(VERIFY_WRITE, input.cert_chain_address, input.cert_chain_len)) {
	    !access_ok(input.cert_chain_address, input.cert_chain_len)) {
		ret = -EFAULT;
		goto e_free;
	}

static int ioctl_queue_iso(struct client *client, union ioctl_arg *arg)		return -EINVAL;

	p = (struct fw_cdev_iso_packet __user *)u64_to_uptr(a->packets);
	if (!access_ok(VERIFY_READ, p, a->size))
	if (!access_ok(p, a->size))
		return -EFAULT;

	end = (void __user *)p + a->size;

copy_ucs2_from_user_len(efi_char16_t **dst, efi_char16_t __user *src,		return 0;
	}

	if (!access_ok(VERIFY_READ, src, 1))
	if (!access_ok(src, 1))
		return -EFAULT;

	buf = memdup_user(src, len);

copy_ucs2_from_user_len(efi_char16_t **dst, efi_char16_t __user *src,static inline int
get_ucs2_strsize_from_user(efi_char16_t __user *src, size_t *len)
{
	if (!access_ok(VERIFY_READ, src, 1))
	if (!access_ok(src, 1))
		return -EFAULT;

	*len = user_ucs2_strsize(src);

copy_ucs2_from_user(efi_char16_t **dst, efi_char16_t __user *src){
	size_t len;

	if (!access_ok(VERIFY_READ, src, 1))
	if (!access_ok(src, 1))
		return -EFAULT;

	len = user_ucs2_strsize(src);

copy_ucs2_to_user_len(efi_char16_t __user *dst, efi_char16_t *src, size_t len)	if (!src)
		return 0;

	if (!access_ok(VERIFY_WRITE, dst, 1))
	if (!access_ok(dst, 1))
		return -EFAULT;

	return copy_to_user(dst, src, len);

int afu_dma_map_region(struct dfl_feature_platform_data *pdata,	if (user_addr + length < user_addr)
		return -EINVAL;

	if (!access_ok(VERIFY_WRITE, (void __user *)(unsigned long)user_addr,
	if (!access_ok((void __user *)(unsigned long)user_addr,
		       length))
		return -EINVAL;


static int fme_pr(struct platform_device *pdev, unsigned long arg)		return -EINVAL;
	}

	if (!access_ok(VERIFY_READ,
		       (void __user *)(unsigned long)port_pr.buffer_address,
	if (!access_ok((void __user *)(unsigned long)port_pr.buffer_address,
		       port_pr.buffer_size))
		return -EFAULT;


static int set_queue_properties_from_user(struct queue_properties *q_properties,	}

	if ((args->ring_base_address) &&
		(!access_ok(VERIFY_WRITE,
			(const void __user *) args->ring_base_address,
		(!access_ok((const void __user *) args->ring_base_address,
			sizeof(uint64_t)))) {
		pr_err("Can't access ring base address\n");
		return -EFAULT;

static int set_queue_properties_from_user(struct queue_properties *q_properties,		return -EINVAL;
	}

	if (!access_ok(VERIFY_WRITE,
			(const void __user *) args->read_pointer_address,
	if (!access_ok((const void __user *) args->read_pointer_address,
			sizeof(uint32_t))) {
		pr_err("Can't access read pointer\n");
		return -EFAULT;
	}

	if (!access_ok(VERIFY_WRITE,
			(const void __user *) args->write_pointer_address,
	if (!access_ok((const void __user *) args->write_pointer_address,
			sizeof(uint32_t))) {
		pr_err("Can't access write pointer\n");
		return -EFAULT;
	}

	if (args->eop_buffer_address &&
		!access_ok(VERIFY_WRITE,
			(const void __user *) args->eop_buffer_address,
		!access_ok((const void __user *) args->eop_buffer_address,
			sizeof(uint32_t))) {
		pr_debug("Can't access eop buffer");
		return -EFAULT;
	}

	if (args->ctx_save_restore_address &&
		!access_ok(VERIFY_WRITE,
			(const void __user *) args->ctx_save_restore_address,
		!access_ok((const void __user *) args->ctx_save_restore_address,
			sizeof(uint32_t))) {
		pr_debug("Can't access ctx save restore buffer");
		return -EFAULT;

static int kfd_ioctl_update_queue(struct file *filp, struct kfd_process *p,	}

	if ((args->ring_base_address) &&
		(!access_ok(VERIFY_WRITE,
			(const void __user *) args->ring_base_address,
		(!access_ok((const void __user *) args->ring_base_address,
			sizeof(uint64_t)))) {
		pr_err("Can't access ring base address\n");
		return -EFAULT;

int armada_gem_pwrite_ioctl(struct drm_device *dev, void *data,
	ptr = (char __user *)(uintptr_t)args->ptr;

	if (!access_ok(VERIFY_READ, ptr, args->size))
	if (!access_ok(ptr, args->size))
		return -EFAULT;

	ret = fault_in_pages_readable(ptr, args->size);

ssize_t drm_read(struct file *filp, char __user *buffer,	struct drm_device *dev = file_priv->minor->dev;
	ssize_t ret;

	if (!access_ok(VERIFY_WRITE, buffer, count))
	if (!access_ok(buffer, count))
		return -EFAULT;

	ret = mutex_lock_interruptible(&file_priv->event_read_lock);

static int etnaviv_ioctl_gem_userptr(struct drm_device *dev, void *data,	struct drm_file *file)
{
	struct drm_etnaviv_gem_userptr *args = data;
	int access;

	if (args->flags & ~(ETNA_USERPTR_READ|ETNA_USERPTR_WRITE) ||
	    args->flags == 0)

static int etnaviv_ioctl_gem_userptr(struct drm_device *dev, void *data,	    args->user_ptr & ~PAGE_MASK)
		return -EINVAL;

	if (args->flags & ETNA_USERPTR_WRITE)
		access = VERIFY_WRITE;
	else
		access = VERIFY_READ;

	if (!access_ok(access, (void __user *)(unsigned long)args->user_ptr,
	if (!access_ok((void __user *)(unsigned long)args->user_ptr,
		       args->user_size))
		return -EFAULT;


i915_gem_pread_ioctl(struct drm_device *dev, void *data,	if (args->size == 0)
		return 0;

	if (!access_ok(VERIFY_WRITE,
		       u64_to_user_ptr(args->data_ptr),
	if (!access_ok(u64_to_user_ptr(args->data_ptr),
		       args->size))
		return -EFAULT;


i915_gem_pwrite_ioctl(struct drm_device *dev, void *data,	if (args->size == 0)
		return 0;

	if (!access_ok(VERIFY_READ,
		       u64_to_user_ptr(args->data_ptr),
		       args->size))
	if (!access_ok(u64_to_user_ptr(args->data_ptr), args->size))
		return -EFAULT;

	obj = i915_gem_object_lookup(file, args->handle);

static int eb_relocate_vma(struct i915_execbuffer *eb, struct i915_vma *vma)	 * to read. However, if the array is not writable the user loses
	 * the updated relocation values.
	 */
	if (unlikely(!access_ok(VERIFY_READ, urelocs, remain*sizeof(*urelocs))))
	if (unlikely(!access_ok(urelocs, remain*sizeof(*urelocs))))
		return -EFAULT;

	do {

static int check_relocations(const struct drm_i915_gem_exec_object2 *entry)
	addr = u64_to_user_ptr(entry->relocs_ptr);
	size *= sizeof(struct drm_i915_gem_relocation_entry);
	if (!access_ok(VERIFY_READ, addr, size))
	if (!access_ok(addr, size))
		return -EFAULT;

	end = addr + size;

get_fence_array(struct drm_i915_gem_execbuffer2 *args,		return ERR_PTR(-EINVAL);

	user = u64_to_user_ptr(args->cliprects_ptr);
	if (!access_ok(VERIFY_READ, user, nfences * sizeof(*user)))
	if (!access_ok(user, nfences * sizeof(*user)))
		return ERR_PTR(-EFAULT);

	fences = kvmalloc_array(nfences, sizeof(*fences),

i915_gem_userptr_ioctl(struct drm_device *dev,	if (offset_in_page(args->user_ptr | args->user_size))
		return -EINVAL;

	if (!access_ok(args->flags & I915_USERPTR_READ_ONLY ? VERIFY_READ : VERIFY_WRITE,
		       (char __user *)(unsigned long)args->user_ptr, args->user_size))
	if (!access_ok((char __user *)(unsigned long)args->user_ptr, args->user_size))
		return -EFAULT;

	if (args->flags & I915_USERPTR_READ_ONLY) {

static int compat_i915_getparam(struct file *file, unsigned int cmd,		return -EFAULT;

	request = compat_alloc_user_space(sizeof(*request));
	if (!access_ok(VERIFY_WRITE, request, sizeof(*request)) ||
	if (!access_ok(request, sizeof(*request)) ||
	    __put_user(req32.param, &request->param) ||
	    __put_user((void __user *)(unsigned long)req32.value,
		       &request->value))

static struct i915_oa_reg *alloc_oa_regs(struct drm_i915_private *dev_priv,	if (!n_regs)
		return NULL;

	if (!access_ok(VERIFY_READ, regs, n_regs * sizeof(u32) * 2))
	if (!access_ok(regs, n_regs * sizeof(u32) * 2))
		return ERR_PTR(-EFAULT);

	/* No is_valid function means we're not allowing any register to be programmed. */

static int query_topology_info(struct drm_i915_private *dev_priv,	if (topo.flags != 0)
		return -EINVAL;

	if (!access_ok(VERIFY_WRITE, u64_to_user_ptr(query_item->data_ptr),
	if (!access_ok(u64_to_user_ptr(query_item->data_ptr),
		       total_length))
		return -EFAULT;


void msm_gem_submit_free(struct msm_gem_submit *submit)static inline unsigned long __must_check
copy_from_user_inatomic(void *to, const void __user *from, unsigned long n)
{
	if (access_ok(VERIFY_READ, from, n))
	if (access_ok(from, n))
		return __copy_from_user_inatomic(to, from, n);
	return -EFAULT;
}

static int qxl_process_single_command(struct qxl_device *qdev,	if (cmd->command_size > PAGE_SIZE - sizeof(union qxl_release_info))
		return -EINVAL;

	if (!access_ok(VERIFY_READ,
		       u64_to_user_ptr(cmd->command),
	if (!access_ok(u64_to_user_ptr(cmd->command),
		       cmd->command_size))
		return -EFAULT;


static ssize_t verify_hdr(struct ib_uverbs_cmd_hdr *hdr,			if (hdr->out_words * 8 < method_elm->resp_size)
				return -ENOSPC;

			if (!access_ok(VERIFY_WRITE,
				       u64_to_user_ptr(ex_hdr->response),
			if (!access_ok(u64_to_user_ptr(ex_hdr->response),
				       (hdr->out_words + ex_hdr->provider_out_words) * 8))
				return -EFAULT;
		} else {

static int pin_rcv_pages(struct hfi1_filedata *fd, struct tid_user_buf *tidbuf)	}

	/* Verify that access is OK for the user buffer */
	if (!access_ok(VERIFY_WRITE, (void __user *)vaddr,
	if (!access_ok((void __user *)vaddr,
		       npages * PAGE_SIZE)) {
		dd_dev_err(dd, "Fail vaddr %p, %u pages, !access_ok\n",
			   (void *)vaddr, npages);

static int qib_tid_update(struct qib_ctxtdata *rcd, struct file *fp,
	/* virtual address of first page in transfer */
	vaddr = ti->tidvaddr;
	if (!access_ok(VERIFY_WRITE, (void __user *) vaddr,
	if (!access_ok((void __user *) vaddr,
		       cnt * PAGE_SIZE)) {
		ret = -EFAULT;
		goto done;

anslcd_write( struct file * file, const char __user * buf,	printk(KERN_DEBUG "LCD: write\n");
#endif

	if (!access_ok(VERIFY_READ, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;

	mutex_lock(&anslcd_mutex);

pmu_read(struct file *file, char __user *buf,
	if (count < 1 || !pp)
		return -EINVAL;
	if (!access_ok(VERIFY_WRITE, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;

	spin_lock_irqsave(&pp->lock, flags);

static int ivtvfb_prep_frame(struct ivtv *itv, int cmd, void __user *source,		IVTVFB_WARN("ivtvfb_prep_frame: Count not a multiple of 4 (%d)\n", count);

	/* Check Source */
	if (!access_ok(VERIFY_READ, source + dest_offset, count)) {
	if (!access_ok(source + dest_offset, count)) {
		IVTVFB_WARN("Invalid userspace pointer %p\n", source);

		IVTVFB_DEBUG_WARN("access_ok() failed for offset 0x%08lx source %p count %d\n",

static int get_v4l2_window32(struct v4l2_window __user *p64,	compat_caddr_t p;
	u32 clipcount;

	if (!access_ok(VERIFY_READ, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    copy_in_user(&p64->w, &p32->w, sizeof(p32->w)) ||
	    assign_in_user(&p64->field, &p32->field) ||
	    assign_in_user(&p64->chromakey, &p32->chromakey) ||

static int __bufsize_v4l2_format(struct v4l2_format32 __user *p32, u32 *size)
static int bufsize_v4l2_format(struct v4l2_format32 __user *p32, u32 *size)
{
	if (!access_ok(VERIFY_READ, p32, sizeof(*p32)))
	if (!access_ok(p32, sizeof(*p32)))
		return -EFAULT;
	return __bufsize_v4l2_format(p32, size);
}

static int get_v4l2_format32(struct v4l2_format __user *p64,			     struct v4l2_format32 __user *p32,
			     void __user *aux_buf, u32 aux_space)
{
	if (!access_ok(VERIFY_READ, p32, sizeof(*p32)))
	if (!access_ok(p32, sizeof(*p32)))
		return -EFAULT;
	return __get_v4l2_format32(p64, p32, aux_buf, aux_space);
}

static int get_v4l2_format32(struct v4l2_format __user *p64,static int bufsize_v4l2_create(struct v4l2_create_buffers32 __user *p32,
			       u32 *size)
{
	if (!access_ok(VERIFY_READ, p32, sizeof(*p32)))
	if (!access_ok(p32, sizeof(*p32)))
		return -EFAULT;
	return __bufsize_v4l2_format(&p32->format, size);
}

static int get_v4l2_create32(struct v4l2_create_buffers __user *p64,			     struct v4l2_create_buffers32 __user *p32,
			     void __user *aux_buf, u32 aux_space)
{
	if (!access_ok(VERIFY_READ, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    copy_in_user(p64, p32,
			 offsetof(struct v4l2_create_buffers32, format)))
		return -EFAULT;

static int __put_v4l2_format32(struct v4l2_format __user *p64,static int put_v4l2_format32(struct v4l2_format __user *p64,
			     struct v4l2_format32 __user *p32)
{
	if (!access_ok(VERIFY_WRITE, p32, sizeof(*p32)))
	if (!access_ok(p32, sizeof(*p32)))
		return -EFAULT;
	return __put_v4l2_format32(p64, p32);
}

static int put_v4l2_format32(struct v4l2_format __user *p64,static int put_v4l2_create32(struct v4l2_create_buffers __user *p64,
			     struct v4l2_create_buffers32 __user *p32)
{
	if (!access_ok(VERIFY_WRITE, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    copy_in_user(p32, p64,
			 offsetof(struct v4l2_create_buffers32, format)) ||
	    assign_in_user(&p32->capabilities, &p64->capabilities) ||

static int get_v4l2_standard32(struct v4l2_standard __user *p64,			       struct v4l2_standard32 __user *p32)
{
	/* other fields are not set by the user, nor used by the driver */
	if (!access_ok(VERIFY_READ, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    assign_in_user(&p64->index, &p32->index))
		return -EFAULT;
	return 0;

static int get_v4l2_standard32(struct v4l2_standard __user *p64,static int put_v4l2_standard32(struct v4l2_standard __user *p64,
			       struct v4l2_standard32 __user *p32)
{
	if (!access_ok(VERIFY_WRITE, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    assign_in_user(&p32->index, &p64->index) ||
	    assign_in_user(&p32->id, &p64->id) ||
	    copy_in_user(p32->name, p64->name, sizeof(p32->name)) ||

static int bufsize_v4l2_buffer(struct v4l2_buffer32 __user *p32, u32 *size)	u32 type;
	u32 length;

	if (!access_ok(VERIFY_READ, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    get_user(type, &p32->type) ||
	    get_user(length, &p32->length))
		return -EFAULT;

static int get_v4l2_buffer32(struct v4l2_buffer __user *p64,	compat_caddr_t p;
	int ret;

	if (!access_ok(VERIFY_READ, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    assign_in_user(&p64->index, &p32->index) ||
	    get_user(type, &p32->type) ||
	    put_user(type, &p64->type) ||

static int get_v4l2_buffer32(struct v4l2_buffer __user *p64,			return -EFAULT;

		uplane32 = compat_ptr(p);
		if (!access_ok(VERIFY_READ, uplane32,
		if (!access_ok(uplane32,
			       num_planes * sizeof(*uplane32)))
			return -EFAULT;


static int put_v4l2_buffer32(struct v4l2_buffer __user *p64,	compat_caddr_t p;
	int ret;

	if (!access_ok(VERIFY_WRITE, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    assign_in_user(&p32->index, &p64->index) ||
	    get_user(type, &p64->type) ||
	    put_user(type, &p32->type) ||

static int get_v4l2_framebuffer32(struct v4l2_framebuffer __user *p64,{
	compat_caddr_t tmp;

	if (!access_ok(VERIFY_READ, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    get_user(tmp, &p32->base) ||
	    put_user_force(compat_ptr(tmp), &p64->base) ||
	    assign_in_user(&p64->capability, &p32->capability) ||

static int put_v4l2_framebuffer32(struct v4l2_framebuffer __user *p64,{
	void *base;

	if (!access_ok(VERIFY_WRITE, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    get_user(base, &p64->base) ||
	    put_user(ptr_to_compat((void __user *)base), &p32->base) ||
	    assign_in_user(&p32->capability, &p64->capability) ||

static int bufsize_v4l2_ext_controls(struct v4l2_ext_controls32 __user *p32,{
	u32 count;

	if (!access_ok(VERIFY_READ, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    get_user(count, &p32->count))
		return -EFAULT;
	if (count > V4L2_CID_MAX_CTRLS)

static int get_v4l2_ext_controls32(struct file *file,	u32 n;
	compat_caddr_t p;

	if (!access_ok(VERIFY_READ, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    assign_in_user(&p64->which, &p32->which) ||
	    get_user(count, &p32->count) ||
	    put_user(count, &p64->count) ||

static int get_v4l2_ext_controls32(struct file *file,	if (get_user(p, &p32->controls))
		return -EFAULT;
	ucontrols = compat_ptr(p);
	if (!access_ok(VERIFY_READ, ucontrols, count * sizeof(*ucontrols)))
	if (!access_ok(ucontrols, count * sizeof(*ucontrols)))
		return -EFAULT;
	if (aux_space < count * sizeof(*kcontrols))
		return -EFAULT;

static int put_v4l2_ext_controls32(struct file *file,	 * with __user causes smatch warnings, so instead declare it
	 * without __user and cast it as a userspace pointer where needed.
	 */
	if (!access_ok(VERIFY_WRITE, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    assign_in_user(&p32->which, &p64->which) ||
	    get_user(count, &p64->count) ||
	    put_user(count, &p32->count) ||

static int put_v4l2_ext_controls32(struct file *file,	if (get_user(p, &p32->controls))
		return -EFAULT;
	ucontrols = compat_ptr(p);
	if (!access_ok(VERIFY_WRITE, ucontrols, count * sizeof(*ucontrols)))
	if (!access_ok(ucontrols, count * sizeof(*ucontrols)))
		return -EFAULT;

	for (n = 0; n < count; n++) {

struct v4l2_event32 {static int put_v4l2_event32(struct v4l2_event __user *p64,
			    struct v4l2_event32 __user *p32)
{
	if (!access_ok(VERIFY_WRITE, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    assign_in_user(&p32->type, &p64->type) ||
	    copy_in_user(&p32->u, &p64->u, sizeof(p64->u)) ||
	    assign_in_user(&p32->pending, &p64->pending) ||

static int get_v4l2_edid32(struct v4l2_edid __user *p64,{
	compat_uptr_t tmp;

	if (!access_ok(VERIFY_READ, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    assign_in_user(&p64->pad, &p32->pad) ||
	    assign_in_user(&p64->start_block, &p32->start_block) ||
	    assign_in_user_cast(&p64->blocks, &p32->blocks) ||

static int put_v4l2_edid32(struct v4l2_edid __user *p64,{
	void *edid;

	if (!access_ok(VERIFY_WRITE, p32, sizeof(*p32)) ||
	if (!access_ok(p32, sizeof(*p32)) ||
	    assign_in_user(&p32->pad, &p64->pad) ||
	    assign_in_user(&p32->start_block, &p64->start_block) ||
	    assign_in_user(&p32->blocks, &p64->blocks) ||

static int vmci_host_setup_notify(struct vmci_ctx *context,	 * about the size.
	 */
	BUILD_BUG_ON(sizeof(bool) != sizeof(u8));
	if (!access_ok(VERIFY_WRITE, (void __user *)uva, sizeof(u8)))
	if (!access_ok((void __user *)uva, sizeof(u8)))
		return VMCI_ERROR_GENERIC;

	/*

static ssize_t proc_bus_pci_read(struct file *file, char __user *buf,		nbytes = size - pos;
	cnt = nbytes;

	if (!access_ok(VERIFY_WRITE, buf, cnt))
	if (!access_ok(buf, cnt))
		return -EINVAL;

	pci_config_pm_runtime_get(dev);

static ssize_t proc_bus_pci_write(struct file *file, const char __user *buf,		nbytes = size - pos;
	cnt = nbytes;

	if (!access_ok(VERIFY_READ, buf, cnt))
	if (!access_ok(buf, cnt))
		return -EINVAL;

	pci_config_pm_runtime_get(dev);

static ssize_t goldfish_pipe_read_write(struct file *filp,	if (unlikely(bufflen == 0))
		return 0;
	/* Check the buffer range for access */
	if (unlikely(!access_ok(is_write ? VERIFY_WRITE : VERIFY_READ,
				buffer, bufflen)))
	if (unlikely(!access_ok(buffer, bufflen)))
		return -EFAULT;

	address = (unsigned long)buffer;

static ssize_t isapnp_proc_bus_read(struct file *file, char __user * buf,		nbytes = size - pos;
	cnt = nbytes;

	if (!access_ok(VERIFY_WRITE, buf, cnt))
	if (!access_ok(buf, cnt))
		return -EINVAL;

	isapnp_cfg_begin(dev->card->number, dev->number);

static long pmcraid_ioctl_passthrough(	u32 ioasc;
	int request_size;
	int buffer_size;
	u8 access, direction;
	u8 direction;
	int rc = 0;

	/* If IOA reset is in progress, wait 10 secs for reset to complete */

static long pmcraid_ioctl_passthrough(	request_size = le32_to_cpu(buffer->ioarcb.data_transfer_length);

	if (buffer->ioarcb.request_flags0 & TRANSFER_DIR_WRITE) {
		access = VERIFY_READ;
		direction = DMA_TO_DEVICE;
	} else {
		access = VERIFY_WRITE;
		direction = DMA_FROM_DEVICE;
	}


int scsi_ioctl(struct scsi_device *sdev, int cmd, void __user *arg)
	switch (cmd) {
	case SCSI_IOCTL_GET_IDLUN:
		if (!access_ok(VERIFY_WRITE, arg, sizeof(struct scsi_idlun)))
		if (!access_ok(arg, sizeof(struct scsi_idlun)))
			return -EFAULT;

		__put_user((sdev->id & 0xff)

sg_read(struct file *filp, char __user *buf, size_t count, loff_t * ppos)	SCSI_LOG_TIMEOUT(3, sg_printk(KERN_INFO, sdp,
				      "sg_read: count=%d\n", (int) count));

	if (!access_ok(VERIFY_WRITE, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;
	if (sfp->force_packid && (count >= SZ_SG_HEADER)) {
		old_hdr = kmalloc(SZ_SG_HEADER, GFP_KERNEL);

sg_write(struct file *filp, const char __user *buf, size_t count, loff_t * ppos)	      scsi_block_when_processing_errors(sdp->device)))
		return -ENXIO;

	if (!access_ok(VERIFY_READ, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;	/* protects following copy_from_user()s + get_user()s */
	if (count < SZ_SG_HEADER)
		return -EIO;

sg_new_write(Sg_fd *sfp, struct file *file, const char __user *buf,
	if (count < SZ_SG_IO_HDR)
		return -EINVAL;
	if (!access_ok(VERIFY_READ, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT; /* protects following copy_from_user()s + get_user()s */

	sfp->cmd_q = 1;	/* when sg_io_hdr seen, set command queuing on */

sg_new_write(Sg_fd *sfp, struct file *file, const char __user *buf,		sg_remove_request(sfp, srp);
		return -EMSGSIZE;
	}
	if (!access_ok(VERIFY_READ, hp->cmdp, hp->cmd_len)) {
	if (!access_ok(hp->cmdp, hp->cmd_len)) {
		sg_remove_request(sfp, srp);
		return -EFAULT;	/* protects following copy_from_user()s + get_user()s */
	}

sg_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg)			return -ENODEV;
		if (!scsi_block_when_processing_errors(sdp->device))
			return -ENXIO;
		if (!access_ok(VERIFY_WRITE, p, SZ_SG_IO_HDR))
		if (!access_ok(p, SZ_SG_IO_HDR))
			return -EFAULT;
		result = sg_new_write(sfp, filp, p, SZ_SG_IO_HDR,
				 1, read_only, 1, &srp);

sg_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg)	case SG_GET_LOW_DMA:
		return put_user((int) sdp->device->host->unchecked_isa_dma, ip);
	case SG_GET_SCSI_ID:
		if (!access_ok(VERIFY_WRITE, p, sizeof (sg_scsi_id_t)))
		if (!access_ok(p, sizeof (sg_scsi_id_t)))
			return -EFAULT;
		else {
			sg_scsi_id_t __user *sg_idp = p;

sg_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg)		sfp->force_packid = val ? 1 : 0;
		return 0;
	case SG_GET_PACK_ID:
		if (!access_ok(VERIFY_WRITE, ip, sizeof (int)))
		if (!access_ok(ip, sizeof (int)))
			return -EFAULT;
		read_lock_irqsave(&sfp->rq_list_lock, iflags);
		list_for_each_entry(srp, &sfp->rq_list, entry) {

sg_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg)		val = (sdp->device ? 1 : 0);
		return put_user(val, ip);
	case SG_GET_REQUEST_TABLE:
		if (!access_ok(VERIFY_WRITE, p, SZ_SG_REQ_INFO * SG_MAX_QUEUE))
		if (!access_ok(p, SZ_SG_REQ_INFO * SG_MAX_QUEUE))
			return -EFAULT;
		else {
			sg_req_info_t *rinfo;

static int compat_chaninfo(struct file *file, unsigned long arg)	chaninfo = compat_alloc_user_space(sizeof(*chaninfo));

	/* Copy chaninfo structure.  Ignore unused members. */
	if (!access_ok(VERIFY_READ, chaninfo32, sizeof(*chaninfo32)) ||
	    !access_ok(VERIFY_WRITE, chaninfo, sizeof(*chaninfo)))
	if (!access_ok(chaninfo32, sizeof(*chaninfo32)) ||
	    !access_ok(chaninfo, sizeof(*chaninfo)))
		return -EFAULT;

	err = 0;

static int compat_rangeinfo(struct file *file, unsigned long arg)	rangeinfo = compat_alloc_user_space(sizeof(*rangeinfo));

	/* Copy rangeinfo structure. */
	if (!access_ok(VERIFY_READ, rangeinfo32, sizeof(*rangeinfo32)) ||
	    !access_ok(VERIFY_WRITE, rangeinfo, sizeof(*rangeinfo)))
	if (!access_ok(rangeinfo32, sizeof(*rangeinfo32)) ||
	    !access_ok(rangeinfo, sizeof(*rangeinfo)))
		return -EFAULT;

	err = 0;

static int get_compat_cmd(struct comedi_cmd __user *cmd,	} temp;

	/* Copy cmd structure. */
	if (!access_ok(VERIFY_READ, cmd32, sizeof(*cmd32)) ||
	    !access_ok(VERIFY_WRITE, cmd, sizeof(*cmd)))
	if (!access_ok(cmd32, sizeof(*cmd32)) ||
	    !access_ok(cmd, sizeof(*cmd)))
		return -EFAULT;

	err = 0;

static int put_compat_cmd(struct comedi32_cmd_struct __user *cmd32,	 * Assume the pointer values are already valid.
	 * (Could use ptr_to_compat() to set them.)
	 */
	if (!access_ok(VERIFY_READ, cmd, sizeof(*cmd)) ||
	    !access_ok(VERIFY_WRITE, cmd32, sizeof(*cmd32)))
	if (!access_ok(cmd, sizeof(*cmd)) ||
	    !access_ok(cmd32, sizeof(*cmd32)))
		return -EFAULT;

	err = 0;

static int get_compat_insn(struct comedi_insn __user *insn,
	/* Copy insn structure.  Ignore the unused members. */
	err = 0;
	if (!access_ok(VERIFY_READ, insn32, sizeof(*insn32)) ||
	    !access_ok(VERIFY_WRITE, insn, sizeof(*insn)))
	if (!access_ok(insn32, sizeof(*insn32)) ||
	    !access_ok(insn, sizeof(*insn)))
		return -EFAULT;

	err |= __get_user(temp.uint, &insn32->insn);

static int compat_insnlist(struct file *file, unsigned long arg)	insnlist32 = compat_ptr(arg);

	/* Get 32-bit insnlist structure.  */
	if (!access_ok(VERIFY_READ, insnlist32, sizeof(*insnlist32)))
	if (!access_ok(insnlist32, sizeof(*insnlist32)))
		return -EFAULT;

	err = 0;

static int compat_insnlist(struct file *file, unsigned long arg)					     insn[n_insns]));

	/* Set native insnlist structure. */
	if (!access_ok(VERIFY_WRITE, &s->insnlist, sizeof(s->insnlist)))
	if (!access_ok(&s->insnlist, sizeof(s->insnlist)))
		return -EFAULT;

	err |= __put_user(n_insns, &s->insnlist.n_insns);

static ssize_t n_hdlc_tty_read(struct tty_struct *tty, struct file *file,		return -EIO;

	/* verify user access to buffer */
	if (!access_ok(VERIFY_WRITE, buf, nr)) {
	if (!access_ok(buf, nr)) {
		printk(KERN_WARNING "%s(%d) n_hdlc_tty_read() can't verify user "
		"buffer\n", __FILE__, __LINE__);
		return -EFAULT;

static ssize_t usb_device_read(struct file *file, char __user *buf,		return -EINVAL;
	if (nbytes <= 0)
		return 0;
	if (!access_ok(VERIFY_WRITE, buf, nbytes))
	if (!access_ok(buf, nbytes))
		return -EFAULT;

	mutex_lock(&usb_bus_idr_lock);

static int proc_control(struct usb_dev_state *ps, void __user *arg)		ctrl.bRequestType, ctrl.bRequest, ctrl.wValue,
		ctrl.wIndex, ctrl.wLength);
	if (ctrl.bRequestType & 0x80) {
		if (ctrl.wLength && !access_ok(VERIFY_WRITE, ctrl.data,
		if (ctrl.wLength && !access_ok(ctrl.data,
					       ctrl.wLength)) {
			ret = -EINVAL;
			goto done;

static int proc_bulk(struct usb_dev_state *ps, void __user *arg)	}
	tmo = bulk.timeout;
	if (bulk.ep & 0x80) {
		if (len1 && !access_ok(VERIFY_WRITE, bulk.data, len1)) {
		if (len1 && !access_ok(bulk.data, len1)) {
			ret = -EINVAL;
			goto done;
		}

static int proc_do_submiturb(struct usb_dev_state *ps, struct usbdevfs_urb *uurb	}

	if (uurb->buffer_length > 0 &&
			!access_ok(is_in ? VERIFY_WRITE : VERIFY_READ,
				uurb->buffer, uurb->buffer_length)) {
			!access_ok(uurb->buffer, uurb->buffer_length)) {
		ret = -EFAULT;
		goto error;
	}

static ssize_t f_hidg_read(struct file *file, char __user *buffer,	if (!count)
		return 0;

	if (!access_ok(VERIFY_WRITE, buffer, count))
	if (!access_ok(buffer, count))
		return -EFAULT;

	spin_lock_irqsave(&hidg->read_spinlock, flags);

static ssize_t f_hidg_write(struct file *file, const char __user *buffer,	unsigned long flags;
	ssize_t status = -ENOMEM;

	if (!access_ok(VERIFY_READ, buffer, count))
	if (!access_ok(buffer, count))
		return -EFAULT;

	spin_lock_irqsave(&hidg->write_spinlock, flags);

static ssize_t queue_dbg_read(struct file *file, char __user *buf,	size_t len, remaining, actual = 0;
	char tmpbuf[38];

	if (!access_ok(VERIFY_WRITE, buf, nbytes))
	if (!access_ok(buf, nbytes))
		return -EFAULT;

	inode_lock(file_inode(file));

static bool log_access_ok(void __user *log_base, u64 addr, unsigned long sz)	    a + (unsigned long)log_base > ULONG_MAX)
		return false;

	return access_ok(VERIFY_WRITE, log_base + a,
	return access_ok(log_base + a,
			 (sz + VHOST_PAGE_SIZE * 8 - 1) / VHOST_PAGE_SIZE / 8);
}


static bool vq_memory_access_ok(void __user *log_base, struct vhost_umem *umem,			return false;


		if (!access_ok(VERIFY_WRITE, (void __user *)a,
		if (!access_ok((void __user *)a,
				    node->size))
			return false;
		else if (log_all && !log_access_ok(log_base,

static bool umem_access_ok(u64 uaddr, u64 size, int access)		return false;

	if ((access & VHOST_ACCESS_RO) &&
	    !access_ok(VERIFY_READ, (void __user *)a, size))
	    !access_ok((void __user *)a, size))
		return false;
	if ((access & VHOST_ACCESS_WO) &&
	    !access_ok(VERIFY_WRITE, (void __user *)a, size))
	    !access_ok((void __user *)a, size))
		return false;
	return true;
}

static bool vq_access_ok(struct vhost_virtqueue *vq, unsigned int num,{
	size_t s = vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;

	return access_ok(VERIFY_READ, desc, num * sizeof *desc) &&
	       access_ok(VERIFY_READ, avail,
	return access_ok(desc, num * sizeof *desc) &&
	       access_ok(avail,
			 sizeof *avail + num * sizeof *avail->ring + s) &&
	       access_ok(VERIFY_WRITE, used,
	       access_ok(used,
			sizeof *used + num * sizeof *used->ring + s);
}


int vhost_vq_init_access(struct vhost_virtqueue *vq)		goto err;
	vq->signalled_used_valid = false;
	if (!vq->iotlb &&
	    !access_ok(VERIFY_READ, &vq->used->idx, sizeof vq->used->idx)) {
	    !access_ok(&vq->used->idx, sizeof vq->used->idx)) {
		r = -EFAULT;
		goto err;
	}

static int ami_get_var_cursorinfo(struct fb_var_cursorinfo *var,	var->yspot = par->crsr.spot_y;
	if (size > var->height * var->width)
		return -ENAMETOOLONG;
	if (!access_ok(VERIFY_WRITE, data, size))
	if (!access_ok(data, size))
		return -EFAULT;
	delta = 1 << par->crsr.fmode;
	lspr = lofsprite + (delta << 1);

static int ami_set_var_cursorinfo(struct fb_var_cursorinfo *var,		return -EINVAL;
	if (!var->height)
		return -EINVAL;
	if (!access_ok(VERIFY_READ, data, var->width * var->height))
	if (!access_ok(data, var->width * var->height))
		return -EFAULT;
	delta = 1 << fmode;
	lofsprite = shfsprite = (u_short *)spritememory;

static int omapfb_memory_read(struct fb_info *fbi,	if (!display || !display->driver->memory_read)
		return -ENOENT;

	if (!access_ok(VERIFY_WRITE, mr->buffer, mr->buffer_size))
	if (!access_ok(mr->buffer, mr->buffer_size))
		return -EFAULT;

	if (mr->w > 4096 || mr->h > 4096)

static long privcmd_ioctl_mmap_batch(			return -EFAULT;
		/* Returns per-frame error in m.arr. */
		m.err = NULL;
		if (!access_ok(VERIFY_WRITE, m.arr, m.num * sizeof(*m.arr)))
		if (!access_ok(m.arr, m.num * sizeof(*m.arr)))
			return -EFAULT;
		break;
	case 2:
		if (copy_from_user(&m, udata, sizeof(struct privcmd_mmapbatch_v2)))
			return -EFAULT;
		/* Returns per-frame error code in m.err. */
		if (!access_ok(VERIFY_WRITE, m.err, m.num * (sizeof(*m.err))))
		if (!access_ok(m.err, m.num * (sizeof(*m.err))))
			return -EFAULT;
		break;
	default:

static long privcmd_ioctl_dm_op(struct file *file, void __user *udata)			goto out;
		}

		if (!access_ok(VERIFY_WRITE, kbufs[i].uptr,
		if (!access_ok(kbufs[i].uptr,
			       kbufs[i].size)) {
			rc = -EFAULT;
			goto out;

static int aout_core_dump(struct coredump_params *cprm)
/* make sure we actually have a data and stack area to dump */
	set_fs(USER_DS);
	if (!access_ok(VERIFY_READ, START_DATA(dump), dump.u_dsize << PAGE_SHIFT))
	if (!access_ok(START_DATA(dump), dump.u_dsize << PAGE_SHIFT))
		dump.u_dsize = 0;
	if (!access_ok(VERIFY_READ, START_STACK(dump), dump.u_ssize << PAGE_SHIFT))
	if (!access_ok(START_STACK(dump), dump.u_ssize << PAGE_SHIFT))
		dump.u_ssize = 0;

	set_fs(KERNEL_DS);

long btrfs_ioctl_send(struct file *mnt_file, struct btrfs_ioctl_send_args *arg)		goto out;
	}

	if (!access_ok(VERIFY_READ, arg->clone_sources,
	if (!access_ok(arg->clone_sources,
			sizeof(*arg->clone_sources) *
			arg->clone_sources_count)) {
		ret = -EFAULT;

static int do_epoll_wait(int epfd, struct epoll_event __user *events,		return -EINVAL;

	/* Verify that the area passed by the user is writeable */
	if (!access_ok(VERIFY_WRITE, events, maxevents * sizeof(struct epoll_event)))
	if (!access_ok(events, maxevents * sizeof(struct epoll_event)))
		return -EFAULT;

	/* Get the "struct file *" for the eventpoll file */

static long fat_dir_ioctl(struct file *filp, unsigned int cmd,		return fat_generic_ioctl(filp, cmd, arg);
	}

	if (!access_ok(VERIFY_WRITE, d1, sizeof(struct __fat_dirent[2])))
	if (!access_ok(d1, sizeof(struct __fat_dirent[2])))
		return -EFAULT;
	/*
	 * Yes, we don't need this put_user() absolutely. However old

static long fat_compat_dir_ioctl(struct file *filp, unsigned cmd,		return fat_generic_ioctl(filp, cmd, (unsigned long)arg);
	}

	if (!access_ok(VERIFY_WRITE, d1, sizeof(struct compat_dirent[2])))
	if (!access_ok(d1, sizeof(struct compat_dirent[2])))
		return -EFAULT;
	/*
	 * Yes, we don't need this put_user() absolutely. However old

static int ioctl_fiemap(struct file *filp, unsigned long arg)	fieinfo.fi_extents_start = ufiemap->fm_extents;

	if (fiemap.fm_extent_count != 0 &&
	    !access_ok(VERIFY_WRITE, fieinfo.fi_extents_start,
	    !access_ok(fieinfo.fi_extents_start,
		       fieinfo.fi_extents_max * sizeof(struct fiemap_extent)))
		return -EFAULT;


static long exact_copy_from_user(void *to, const void __user * from,	const char __user *f = from;
	char c;

	if (!access_ok(VERIFY_READ, from, n))
	if (!access_ok(from, n))
		return n;

	current->kernel_uaccess_faults_ok++;

static ssize_t dlmfs_file_read(struct file *filp,	if (!count)
		return 0;

	if (!access_ok(VERIFY_WRITE, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;

	/* don't read past the lvb */

static ssize_t dlmfs_file_write(struct file *filp,	if (!count)
		return 0;

	if (!access_ok(VERIFY_READ, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;

	/* don't write past the lvb */

static ssize_t write_pmsg(struct file *file, const char __user *buf,	record.size = count;

	/* check outside lock, page in any data. write_user also checks */
	if (!access_ok(VERIFY_READ, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;

	mutex_lock(&pmsg_lock);

int notrace persistent_ram_write_user(struct persistent_ram_zone *prz,	int rem, ret = 0, c = count;
	size_t start;

	if (unlikely(!access_ok(VERIFY_READ, s, count)))
	if (unlikely(!access_ok(s, count)))
		return -EFAULT;
	if (unlikely(c > prz->buffer_size)) {
		s += c - prz->buffer_size;

ssize_t vfs_read(struct file *file, char __user *buf, size_t count, loff_t *pos)		return -EBADF;
	if (!(file->f_mode & FMODE_CAN_READ))
		return -EINVAL;
	if (unlikely(!access_ok(VERIFY_WRITE, buf, count)))
	if (unlikely(!access_ok(buf, count)))
		return -EFAULT;

	ret = rw_verify_area(READ, file, pos, count);

ssize_t vfs_write(struct file *file, const char __user *buf, size_t count, loff_		return -EBADF;
	if (!(file->f_mode & FMODE_CAN_WRITE))
		return -EINVAL;
	if (unlikely(!access_ok(VERIFY_READ, buf, count)))
	if (unlikely(!access_ok(buf, count)))
		return -EFAULT;

	ret = rw_verify_area(WRITE, file, pos, count);

static ssize_t do_loop_readv_writev(struct file *filp, struct iov_iter *iter,	return ret;
}

/* A write operation does a read from user space and vice versa */
#define vrfy_dir(type) ((type) == READ ? VERIFY_WRITE : VERIFY_READ)

/**
 * rw_copy_check_uvector() - Copy an array of &struct iovec from userspace
 *     into the kernel and check that it is valid.

ssize_t rw_copy_check_uvector(int type, const struct iovec __user * uvector,			goto out;
		}
		if (type >= 0
		    && unlikely(!access_ok(vrfy_dir(type), buf, len))) {
		    && unlikely(!access_ok(buf, len))) {
			ret = -EFAULT;
			goto out;
		}

ssize_t compat_rw_copy_check_uvector(int type,	*ret_pointer = iov;

	ret = -EFAULT;
	if (!access_ok(VERIFY_READ, uvector, nr_segs*sizeof(*uvector)))
	if (!access_ok(uvector, nr_segs*sizeof(*uvector)))
		goto out;

	/*

ssize_t compat_rw_copy_check_uvector(int type,		if (len < 0)	/* size_t not fitting in compat_ssize_t .. */
			goto out;
		if (type >= 0 &&
		    !access_ok(vrfy_dir(type), compat_ptr(buf), len)) {
		    !access_ok(compat_ptr(buf), len)) {
			ret = -EFAULT;
			goto out;
		}

static int fillonedir(struct dir_context *ctx, const char *name, int namlen,	}
	buf->result++;
	dirent = buf->dirent;
	if (!access_ok(VERIFY_WRITE, dirent,
	if (!access_ok(dirent,
			(unsigned long)(dirent->d_name + namlen + 1) -
				(unsigned long)dirent))
		goto efault;

SYSCALL_DEFINE3(getdents, unsigned int, fd,	};
	int error;

	if (!access_ok(VERIFY_WRITE, dirent, count))
	if (!access_ok(dirent, count))
		return -EFAULT;

	f = fdget_pos(fd);

int ksys_getdents64(unsigned int fd, struct linux_dirent64 __user *dirent,	};
	int error;

	if (!access_ok(VERIFY_WRITE, dirent, count))
	if (!access_ok(dirent, count))
		return -EFAULT;

	f = fdget_pos(fd);

static int compat_fillonedir(struct dir_context *ctx, const char *name,	}
	buf->result++;
	dirent = buf->dirent;
	if (!access_ok(VERIFY_WRITE, dirent,
	if (!access_ok(dirent,
			(unsigned long)(dirent->d_name + namlen + 1) -
				(unsigned long)dirent))
		goto efault;

COMPAT_SYSCALL_DEFINE3(getdents, unsigned int, fd,	};
	int error;

	if (!access_ok(VERIFY_WRITE, dirent, count))
	if (!access_ok(dirent, count))
		return -EFAULT;

	f = fdget_pos(fd);

typedef struct {#define FDS_BYTES(nr)	(FDS_LONGS(nr)*sizeof(long))

/*
 * We do a VERIFY_WRITE here even though we are only reading this time:
 * we'll write to it eventually..
 *
 * Use "unsigned long" accesses to let user-mode fd_set's be long-aligned.
 */
static inline

SYSCALL_DEFINE6(pselect6, int, n, fd_set __user *, inp, fd_set __user *, outp,	sigset_t __user *up = NULL;

	if (sig) {
		if (!access_ok(VERIFY_READ, sig, sizeof(void *)+sizeof(size_t))
		if (!access_ok(sig, sizeof(void *)+sizeof(size_t))
		    || __get_user(up, (sigset_t __user * __user *)sig)
		    || __get_user(sigsetsize,
				(size_t __user *)(sig+sizeof(void *))))

SYSCALL_DEFINE6(pselect6_time32, int, n, fd_set __user *, inp, fd_set __user *,	sigset_t __user *up = NULL;

	if (sig) {
		if (!access_ok(VERIFY_READ, sig, sizeof(void *)+sizeof(size_t))
		if (!access_ok(sig, sizeof(void *)+sizeof(size_t))
		    || __get_user(up, (sigset_t __user * __user *)sig)
		    || __get_user(sigsetsize,
				(size_t __user *)(sig+sizeof(void *))))

COMPAT_SYSCALL_DEFINE6(pselect6_time64, int, n, compat_ulong_t __user *, inp,	compat_uptr_t up = 0;

	if (sig) {
		if (!access_ok(VERIFY_READ, sig,
		if (!access_ok(sig,
				sizeof(compat_uptr_t)+sizeof(compat_size_t)) ||
				__get_user(up, (compat_uptr_t __user *)sig) ||
				__get_user(sigsetsize,

COMPAT_SYSCALL_DEFINE6(pselect6, int, n, compat_ulong_t __user *, inp,	compat_uptr_t up = 0;

	if (sig) {
		if (!access_ok(VERIFY_READ, sig,
		if (!access_ok(sig,
				sizeof(compat_uptr_t)+sizeof(compat_size_t)) ||
		    	__get_user(up, (compat_uptr_t __user *)sig) ||
		    	__get_user(sigsetsize,

static inline void set_fs(mm_segment_t fs)#define segment_eq(a, b) ((a).seg == (b).seg)
#endif

#define access_ok(type, addr, size) __access_ok((unsigned long)(addr),(size))
#define access_ok(addr, size) __access_ok((unsigned long)(addr),(size))

/*
 * The architecture should really override this if possible, at least

static inline int __access_ok(unsigned long addr, unsigned long size)({								\
	void __user *__p = (ptr);				\
	might_fault();						\
	access_ok(VERIFY_WRITE, __p, sizeof(*ptr)) ?		\
	access_ok(__p, sizeof(*ptr)) ?		\
		__put_user((x), ((__typeof__(*(ptr)) __user *)__p)) :	\
		-EFAULT;					\
})

extern int __put_user_bad(void) __attribute__((noreturn));({								\
	const void __user *__p = (ptr);				\
	might_fault();						\
	access_ok(VERIFY_READ, __p, sizeof(*ptr)) ?		\
	access_ok(__p, sizeof(*ptr)) ?		\
		__get_user((x), (__typeof__(*(ptr)) __user *)__p) :\
		((x) = (__typeof__(*(ptr)))0,-EFAULT);		\
})

__strncpy_from_user(char *dst, const char __user *src, long count)static inline long
strncpy_from_user(char *dst, const char __user *src, long count)
{
	if (!access_ok(VERIFY_READ, src, 1))
	if (!access_ok(src, 1))
		return -EFAULT;
	return __strncpy_from_user(dst, src, count);
}

strncpy_from_user(char *dst, const char __user *src, long count) */
static inline long strnlen_user(const char __user *src, long n)
{
	if (!access_ok(VERIFY_READ, src, 1))
	if (!access_ok(src, 1))
		return 0;
	return __strnlen_user(src, n);
}

static inline __must_check unsigned longclear_user(void __user *to, unsigned long n)
{
	might_fault();
	if (!access_ok(VERIFY_WRITE, to, n))
	if (!access_ok(to, n))
		return n;

	return __clear_user(to, n);

static inline int copy_regset_to_user(struct task_struct *target,	if (!regset->get)
		return -EOPNOTSUPP;

	if (!access_ok(VERIFY_WRITE, data, size))
	if (!access_ok(data, size))
		return -EFAULT;

	return regset->get(target, regset, offset, size, NULL, data);

static inline int copy_regset_from_user(struct task_struct *target,	if (!regset->set)
		return -EOPNOTSUPP;

	if (!access_ok(VERIFY_READ, data, size))
	if (!access_ok(data, size))
		return -EFAULT;

	return regset->set(target, regset, offset, size, NULL, data);

#include <linux/thread_info.h>
#include <linux/kasan-checks.h>

#define VERIFY_READ 0
#define VERIFY_WRITE 1

#define uaccess_kernel() segment_eq(get_fs(), KERNEL_DS)

#include <asm/uaccess.h>

_copy_from_user(void *to, const void __user *from, unsigned long n){
	unsigned long res = n;
	might_fault();
	if (likely(access_ok(VERIFY_READ, from, n))) {
	if (likely(access_ok(from, n))) {
		kasan_check_write(to, n);
		res = raw_copy_from_user(to, from, n);
	}

static inline unsigned long_copy_to_user(void __user *to, const void *from, unsigned long n)
{
	might_fault();
	if (access_ok(VERIFY_WRITE, to, n)) {
	if (access_ok(to, n)) {
		kasan_check_read(from, n);
		n = raw_copy_to_user(to, from, n);
	}

static __always_inline unsigned long __must_checkcopy_in_user(void __user *to, const void __user *from, unsigned long n)
{
	might_fault();
	if (access_ok(VERIFY_WRITE, to, n) && access_ok(VERIFY_READ, from, n))
	if (access_ok(to, n) && access_ok(from, n))
		n = raw_copy_in_user(to, from, n);
	return n;
}

static inline__wsum csum_and_copy_from_user (const void __user *src, void *dst,
				      int len, __wsum sum, int *err_ptr)
{
	if (access_ok(VERIFY_READ, src, len))
	if (access_ok(src, len))
		return csum_partial_copy_from_user(src, dst, len, sum, err_ptr);

	if (len)

static __inline__ __wsum csum_and_copy_to_user{
	sum = csum_partial(src, len, sum);

	if (access_ok(VERIFY_WRITE, dst, len)) {
	if (access_ok(dst, len)) {
		if (copy_to_user(dst, src, len) == 0)
			return sum;
	}

int bpf_check_uarg_tail_zero(void __user *uaddr,	if (unlikely(actual_size > PAGE_SIZE))	/* silly large */
		return -E2BIG;

	if (unlikely(!access_ok(VERIFY_READ, uaddr, actual_size)))
	if (unlikely(!access_ok(uaddr, actual_size)))
		return -EFAULT;

	if (actual_size <= expected_size)

int compat_put_timex(struct compat_timex __user *utp, const struct timex *txc)
static int __compat_get_timeval(struct timeval *tv, const struct old_timeval32 __user *ctv)
{
	return (!access_ok(VERIFY_READ, ctv, sizeof(*ctv)) ||
	return (!access_ok(ctv, sizeof(*ctv)) ||
			__get_user(tv->tv_sec, &ctv->tv_sec) ||
			__get_user(tv->tv_usec, &ctv->tv_usec)) ? -EFAULT : 0;
}

static int __compat_put_timeval(const struct timeval *tv, struct old_timeval32 __user *ctv)
{
	return (!access_ok(VERIFY_WRITE, ctv, sizeof(*ctv)) ||
	return (!access_ok(ctv, sizeof(*ctv)) ||
			__put_user(tv->tv_sec, &ctv->tv_sec) ||
			__put_user(tv->tv_usec, &ctv->tv_usec)) ? -EFAULT : 0;
}

static int __compat_get_timespec(struct timespec *ts, const struct old_timespec32 __user *cts)
{
	return (!access_ok(VERIFY_READ, cts, sizeof(*cts)) ||
	return (!access_ok(cts, sizeof(*cts)) ||
			__get_user(ts->tv_sec, &cts->tv_sec) ||
			__get_user(ts->tv_nsec, &cts->tv_nsec)) ? -EFAULT : 0;
}

static int __compat_put_timespec(const struct timespec *ts, struct old_timespec32 __user *cts)
{
	return (!access_ok(VERIFY_WRITE, cts, sizeof(*cts)) ||
	return (!access_ok(cts, sizeof(*cts)) ||
			__put_user(ts->tv_sec, &cts->tv_sec) ||
			__put_user(ts->tv_nsec, &cts->tv_nsec)) ? -EFAULT : 0;
}

int get_compat_sigevent(struct sigevent *event,		const struct compat_sigevent __user *u_event)
{
	memset(event, 0, sizeof(*event));
	return (!access_ok(VERIFY_READ, u_event, sizeof(*u_event)) ||
	return (!access_ok(u_event, sizeof(*u_event)) ||
		__get_user(event->sigev_value.sival_int,
			&u_event->sigev_value.sival_int) ||
		__get_user(event->sigev_signo, &u_event->sigev_signo) ||

long compat_get_bitmap(unsigned long *mask, const compat_ulong_t __user *umask,	bitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);
	nr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);

	if (!access_ok(VERIFY_READ, umask, bitmap_size / 8))
	if (!access_ok(umask, bitmap_size / 8))
		return -EFAULT;

	user_access_begin();

long compat_put_bitmap(compat_ulong_t __user *umask, unsigned long *mask,	bitmap_size = ALIGN(bitmap_size, BITS_PER_COMPAT_LONG);
	nr_compat_longs = BITS_TO_COMPAT_LONGS(bitmap_size);

	if (!access_ok(VERIFY_WRITE, umask, bitmap_size / 8))
	if (!access_ok(umask, bitmap_size / 8))
		return -EFAULT;

	user_access_begin();

void __user *compat_alloc_user_space(unsigned long len)
	ptr = arch_compat_alloc_user_space(len);

	if (unlikely(!access_ok(VERIFY_WRITE, ptr, len)))
	if (unlikely(!access_ok(ptr, len)))
		return NULL;

	return ptr;

static int perf_copy_attr(struct perf_event_attr __user *uattr,	u32 size;
	int ret;

	if (!access_ok(VERIFY_WRITE, uattr, PERF_ATTR_SIZE_VER0))
	if (!access_ok(uattr, PERF_ATTR_SIZE_VER0))
		return -EFAULT;

	/*

SYSCALL_DEFINE5(waitid, int, which, pid_t, upid, struct siginfo __user *,	if (!infop)
		return err;

	if (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))
	if (!access_ok(infop, sizeof(*infop)))
		return -EFAULT;

	user_access_begin();

COMPAT_SYSCALL_DEFINE5(waitid,	if (!infop)
		return err;

	if (!access_ok(VERIFY_WRITE, infop, sizeof(*infop)))
	if (!access_ok(infop, sizeof(*infop)))
		return -EFAULT;

	user_access_begin();

static void drop_futex_key_refs(union futex_key *key)	}
}

enum futex_access {
	FUTEX_READ,
	FUTEX_WRITE
};

/**
 * get_futex_key() - Get parameters which are the keys for a futex
 * @uaddr:	virtual address of the futex
 * @fshared:	0 for a PROCESS_PRIVATE futex, 1 for PROCESS_SHARED
 * @key:	address where result is stored.
 * @rw:		mapping needs to be read/write (values: VERIFY_READ,
 *              VERIFY_WRITE)
 * @rw:		mapping needs to be read/write (values: FUTEX_READ,
 *              FUTEX_WRITE)
 *
 * Return: a negative error code or 0
 *

static void drop_futex_key_refs(union futex_key *key) * lock_page() might sleep, the caller should not hold a spinlock.
 */
static int
get_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, int rw)
get_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, enum futex_access rw)
{
	unsigned long address = (unsigned long)uaddr;
	struct mm_struct *mm = current->mm;

get_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, int rw)		return -EINVAL;
	address -= key->both.offset;

	if (unlikely(!access_ok(rw, uaddr, sizeof(u32))))
	if (unlikely(!access_ok(uaddr, sizeof(u32))))
		return -EFAULT;

	if (unlikely(should_fail_futex(fshared)))

get_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, int rw)	 * If write access is not required (eg. FUTEX_WAIT), try
	 * and get read-only access.
	 */
	if (err == -EFAULT && rw == VERIFY_READ) {
	if (err == -EFAULT && rw == FUTEX_READ) {
		err = get_user_pages_fast(address, 1, 0, &page);
		ro = 1;
	}

futex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)	if (!bitset)
		return -EINVAL;

	ret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, VERIFY_READ);
	ret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, FUTEX_READ);
	if (unlikely(ret != 0))
		goto out;


static int futex_atomic_op_inuser(unsigned int encoded_op, u32 __user *uaddr)		oparg = 1 << oparg;
	}

	if (!access_ok(VERIFY_WRITE, uaddr, sizeof(u32)))
	if (!access_ok(uaddr, sizeof(u32)))
		return -EFAULT;

	ret = arch_futex_atomic_op_inuser(op, oparg, &oldval, uaddr);

futex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,	DEFINE_WAKE_Q(wake_q);

retry:
	ret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, VERIFY_READ);
	ret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, FUTEX_READ);
	if (unlikely(ret != 0))
		goto out;
	ret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);
	ret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, FUTEX_WRITE);
	if (unlikely(ret != 0))
		goto out_put_key1;


static int futex_requeue(u32 __user *uaddr1, unsigned int flags,	}

retry:
	ret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, VERIFY_READ);
	ret = get_futex_key(uaddr1, flags & FLAGS_SHARED, &key1, FUTEX_READ);
	if (unlikely(ret != 0))
		goto out;
	ret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2,
			    requeue_pi ? VERIFY_WRITE : VERIFY_READ);
			    requeue_pi ? FUTEX_WRITE : FUTEX_READ);
	if (unlikely(ret != 0))
		goto out_put_key1;


static int futex_wait_setup(u32 __user *uaddr, u32 val, unsigned int flags,	 * while the syscall executes.
	 */
retry:
	ret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q->key, VERIFY_READ);
	ret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q->key, FUTEX_READ);
	if (unlikely(ret != 0))
		return ret;


static int futex_lock_pi(u32 __user *uaddr, unsigned int flags,	}

retry:
	ret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q.key, VERIFY_WRITE);
	ret = get_futex_key(uaddr, flags & FLAGS_SHARED, &q.key, FUTEX_WRITE);
	if (unlikely(ret != 0))
		goto out;


static int futex_unlock_pi(u32 __user *uaddr, unsigned int flags)	if ((uval & FUTEX_TID_MASK) != vpid)
		return -EPERM;

	ret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, VERIFY_WRITE);
	ret = get_futex_key(uaddr, flags & FLAGS_SHARED, &key, FUTEX_WRITE);
	if (ret)
		return ret;


static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,	 */
	rt_mutex_init_waiter(&rt_waiter);

	ret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, VERIFY_WRITE);
	ret = get_futex_key(uaddr2, flags & FLAGS_SHARED, &key2, FUTEX_WRITE);
	if (unlikely(ret != 0))
		goto out;


int do_syslog(int type, char __user *buf, int len, int source)			return -EINVAL;
		if (!len)
			return 0;
		if (!access_ok(VERIFY_WRITE, buf, len))
		if (!access_ok(buf, len))
			return -EFAULT;
		error = wait_event_interruptible(log_wait,
						 syslog_seq != log_next_seq);

int do_syslog(int type, char __user *buf, int len, int source)			return -EINVAL;
		if (!len)
			return 0;
		if (!access_ok(VERIFY_WRITE, buf, len))
		if (!access_ok(buf, len))
			return -EFAULT;
		error = syslog_print_all(buf, len, clear);
		break;

int ptrace_request(struct task_struct *child, long request,		struct iovec kiov;
		struct iovec __user *uiov = datavp;

		if (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))
		if (!access_ok(uiov, sizeof(*uiov)))
			return -EFAULT;

		if (__get_user(kiov.iov_base, &uiov->iov_base) ||

int compat_ptrace_request(struct task_struct *child, compat_long_t request,		compat_uptr_t ptr;
		compat_size_t len;

		if (!access_ok(VERIFY_WRITE, uiov, sizeof(*uiov)))
		if (!access_ok(uiov, sizeof(*uiov)))
			return -EFAULT;

		if (__get_user(ptr, &uiov->iov_base) ||

void __rseq_handle_notify_resume(struct ksignal *ksig, struct pt_regs *regs)
	if (unlikely(t->flags & PF_EXITING))
		return;
	if (unlikely(!access_ok(VERIFY_WRITE, t->rseq, sizeof(*t->rseq))))
	if (unlikely(!access_ok(t->rseq, sizeof(*t->rseq))))
		goto error;
	ret = rseq_ip_fixup(regs);
	if (unlikely(ret < 0))

void rseq_syscall(struct pt_regs *regs)
	if (!t->rseq)
		return;
	if (!access_ok(VERIFY_READ, t->rseq, sizeof(*t->rseq)) ||
	if (!access_ok(t->rseq, sizeof(*t->rseq)) ||
	    rseq_get_rseq_cs(t, &rseq_cs) || in_rseq_cs(ip, &rseq_cs))
		force_sig(SIGSEGV, t);
}

SYSCALL_DEFINE4(rseq, struct rseq __user *, rseq, u32, rseq_len,	if (!IS_ALIGNED((unsigned long)rseq, __alignof__(*rseq)) ||
	    rseq_len != sizeof(*rseq))
		return -EINVAL;
	if (!access_ok(VERIFY_WRITE, rseq, rseq_len))
	if (!access_ok(rseq, rseq_len))
		return -EFAULT;
	current->rseq = rseq;
	current->rseq_len = rseq_len;

static int sched_copy_attr(struct sched_attr __user *uattr, struct sched_attr *a	u32 size;
	int ret;

	if (!access_ok(VERIFY_WRITE, uattr, SCHED_ATTR_SIZE_VER0))
	if (!access_ok(uattr, SCHED_ATTR_SIZE_VER0))
		return -EFAULT;

	/* Zero the full structure, so that a short copy will be nice: */

static int sched_read_attr(struct sched_attr __user *uattr,{
	int ret;

	if (!access_ok(VERIFY_WRITE, uattr, usize))
	if (!access_ok(uattr, usize))
		return -EFAULT;

	/*

SYSCALL_DEFINE3(sigaction, int, sig,
	if (act) {
		old_sigset_t mask;
		if (!access_ok(VERIFY_READ, act, sizeof(*act)) ||
		if (!access_ok(act, sizeof(*act)) ||
		    __get_user(new_ka.sa.sa_handler, &act->sa_handler) ||
		    __get_user(new_ka.sa.sa_restorer, &act->sa_restorer) ||
		    __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||

SYSCALL_DEFINE3(sigaction, int, sig,	ret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);

	if (!ret && oact) {
		if (!access_ok(VERIFY_WRITE, oact, sizeof(*oact)) ||
		if (!access_ok(oact, sizeof(*oact)) ||
		    __put_user(old_ka.sa.sa_handler, &oact->sa_handler) ||
		    __put_user(old_ka.sa.sa_restorer, &oact->sa_restorer) ||
		    __put_user(old_ka.sa.sa_flags, &oact->sa_flags) ||

COMPAT_SYSCALL_DEFINE3(sigaction, int, sig,	compat_uptr_t handler, restorer;

	if (act) {
		if (!access_ok(VERIFY_READ, act, sizeof(*act)) ||
		if (!access_ok(act, sizeof(*act)) ||
		    __get_user(handler, &act->sa_handler) ||
		    __get_user(restorer, &act->sa_restorer) ||
		    __get_user(new_ka.sa.sa_flags, &act->sa_flags) ||

COMPAT_SYSCALL_DEFINE3(sigaction, int, sig,	ret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);

	if (!ret && oact) {
		if (!access_ok(VERIFY_WRITE, oact, sizeof(*oact)) ||
		if (!access_ok(oact, sizeof(*oact)) ||
		    __put_user(ptr_to_compat(old_ka.sa.sa_handler),
			       &oact->sa_handler) ||
		    __put_user(ptr_to_compat(old_ka.sa.sa_restorer),

COMPAT_SYSCALL_DEFINE1(sysinfo, struct compat_sysinfo __user *, info)		s.freehigh >>= bitcount;
	}

	if (!access_ok(VERIFY_WRITE, info, sizeof(struct compat_sysinfo)) ||
	if (!access_ok(info, sizeof(struct compat_sysinfo)) ||
	    __put_user(s.uptime, &info->uptime) ||
	    __put_user(s.loads[0], &info->loads[0]) ||
	    __put_user(s.loads[1], &info->loads[1]) ||

BPF_CALL_3(bpf_probe_write_user, void *, unsafe_ptr, const void *, src,		return -EPERM;
	if (unlikely(uaccess_kernel()))
		return -EPERM;
	if (!access_ok(VERIFY_WRITE, unsafe_ptr, size))
	if (!access_ok(unsafe_ptr, size))
		return -EPERM;

	return probe_kernel_write(unsafe_ptr, src, size);

int bitmap_parse_user(const char __user *ubuf,			unsigned int ulen, unsigned long *maskp,
			int nmaskbits)
{
	if (!access_ok(VERIFY_READ, ubuf, ulen))
	if (!access_ok(ubuf, ulen))
		return -EFAULT;
	return __bitmap_parse((const char __force *)ubuf,
				ulen, 1, maskp, nmaskbits);

int bitmap_parselist_user(const char __user *ubuf,			unsigned int ulen, unsigned long *maskp,
			int nmaskbits)
{
	if (!access_ok(VERIFY_READ, ubuf, ulen))
	if (!access_ok(ubuf, ulen))
		return -EFAULT;
	return __bitmap_parselist((const char __force *)ubuf,
					ulen, 1, maskp, nmaskbits);


static int copyout(void __user *to, const void *from, size_t n)
{
	if (access_ok(VERIFY_WRITE, to, n)) {
	if (access_ok(to, n)) {
		kasan_check_read(from, n);
		n = raw_copy_to_user(to, from, n);
	}

static int copyout(void __user *to, const void *from, size_t n)
static int copyin(void *to, const void __user *from, size_t n)
{
	if (access_ok(VERIFY_READ, from, n)) {
	if (access_ok(from, n)) {
		kasan_check_write(to, n);
		n = raw_copy_from_user(to, from, n);
	}

EXPORT_SYMBOL(_copy_to_iter);#ifdef CONFIG_ARCH_HAS_UACCESS_MCSAFE
static int copyout_mcsafe(void __user *to, const void *from, size_t n)
{
	if (access_ok(VERIFY_WRITE, to, n)) {
	if (access_ok(to, n)) {
		kasan_check_read(from, n);
		n = copy_to_user_mcsafe((__force void *) to, from, n);
	}

int import_single_range(int rw, void __user *buf, size_t len,{
	if (len > MAX_RW_COUNT)
		len = MAX_RW_COUNT;
	if (unlikely(!access_ok(!rw, buf, len)))
	if (unlikely(!access_ok(buf, len)))
		return -EFAULT;

	iov->iov_base = buf;

unsigned long _copy_from_user(void *to, const void __user *from, unsigned long n{
	unsigned long res = n;
	might_fault();
	if (likely(access_ok(VERIFY_READ, from, n))) {
	if (likely(access_ok(from, n))) {
		kasan_check_write(to, n);
		res = raw_copy_from_user(to, from, n);
	}

EXPORT_SYMBOL(_copy_from_user);unsigned long _copy_to_user(void __user *to, const void *from, unsigned long n)
{
	might_fault();
	if (likely(access_ok(VERIFY_WRITE, to, n))) {
	if (likely(access_ok(to, n))) {
		kasan_check_read(from, n);
		n = raw_copy_to_user(to, from, n);
	}

int __get_user_pages_fast(unsigned long start, int nr_pages, int write,	len = (unsigned long) nr_pages << PAGE_SHIFT;
	end = start + len;

	if (unlikely(!access_ok(write ? VERIFY_WRITE : VERIFY_READ,
					(void __user *)start, len)))
	if (unlikely(!access_ok((void __user *)start, len)))
		return 0;

	/*

int get_user_pages_fast(unsigned long start, int nr_pages, int write,	if (nr_pages <= 0)
		return 0;

	if (unlikely(!access_ok(write ? VERIFY_WRITE : VERIFY_READ,
					(void __user *)start, len)))
	if (unlikely(!access_ok((void __user *)start, len)))
		return -EFAULT;

	if (gup_fast_permitted(start, nr_pages, write)) {

SYSCALL_DEFINE3(mincore, unsigned long, start, size_t, len,		return -EINVAL;

	/* ..and we need to be passed a valid user-space range */
	if (!access_ok(VERIFY_READ, (void __user *) start, len))
	if (!access_ok((void __user *) start, len))
		return -ENOMEM;

	/* This also avoids any overflows on PAGE_ALIGN */
	pages = len >> PAGE_SHIFT;
	pages += (offset_in_page(len)) != 0;

	if (!access_ok(VERIFY_WRITE, vec, pages))
	if (!access_ok(vec, pages))
		return -EFAULT;

	tmp = (void *) __get_free_page(GFP_USER);

static ssize_t batadv_socket_read(struct file *file, char __user *buf,	if (!buf || count < sizeof(struct batadv_icmp_packet))
		return -EINVAL;

	if (!access_ok(VERIFY_WRITE, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;

	error = wait_event_interruptible(socket_client->queue_wait,

static ssize_t batadv_log_read(struct file *file, char __user *buf,	if (count == 0)
		return 0;

	if (!access_ok(VERIFY_WRITE, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;

	error = wait_event_interruptible(debug_log->queue_wait,

static int do_set_sock_timeout(struct socket *sock, int level,
	if (optlen < sizeof(*up))
		return -EINVAL;
	if (!access_ok(VERIFY_READ, up, sizeof(*up)) ||
	if (!access_ok(up, sizeof(*up)) ||
	    __get_user(ktime.tv_sec, &up->tv_sec) ||
	    __get_user(ktime.tv_usec, &up->tv_usec))
		return -EFAULT;

static int do_get_sock_timeout(struct socket *sock, int level, int optname,
	if (!err) {
		if (put_user(sizeof(*up), optlen) ||
		    !access_ok(VERIFY_WRITE, up, sizeof(*up)) ||
		    !access_ok(up, sizeof(*up)) ||
		    __put_user(ktime.tv_sec, &up->tv_sec) ||
		    __put_user(ktime.tv_usec, &up->tv_usec))
			err = -EFAULT;

int compat_mc_setsockopt(struct sock *sock, int level, int optname,			compat_alloc_user_space(sizeof(struct group_req));
		u32 interface;

		if (!access_ok(VERIFY_READ, gr32, sizeof(*gr32)) ||
		    !access_ok(VERIFY_WRITE, kgr, sizeof(struct group_req)) ||
		if (!access_ok(gr32, sizeof(*gr32)) ||
		    !access_ok(kgr, sizeof(struct group_req)) ||
		    __get_user(interface, &gr32->gr_interface) ||
		    __put_user(interface, &kgr->gr_interface) ||
		    copy_in_user(&kgr->gr_group, &gr32->gr_group,

int compat_mc_setsockopt(struct sock *sock, int level, int optname,			sizeof(struct group_source_req));
		u32 interface;

		if (!access_ok(VERIFY_READ, gsr32, sizeof(*gsr32)) ||
		    !access_ok(VERIFY_WRITE, kgsr,
		if (!access_ok(gsr32, sizeof(*gsr32)) ||
		    !access_ok(kgsr,
			sizeof(struct group_source_req)) ||
		    __get_user(interface, &gsr32->gsr_interface) ||
		    __put_user(interface, &kgsr->gsr_interface) ||

int compat_mc_setsockopt(struct sock *sock, int level, int optname,		struct group_filter __user *kgf;
		u32 interface, fmode, numsrc;

		if (!access_ok(VERIFY_READ, gf32, __COMPAT_GF0_SIZE) ||
		if (!access_ok(gf32, __COMPAT_GF0_SIZE) ||
		    __get_user(interface, &gf32->gf_interface) ||
		    __get_user(fmode, &gf32->gf_fmode) ||
		    __get_user(numsrc, &gf32->gf_numsrc))

int compat_mc_setsockopt(struct sock *sock, int level, int optname,		if (koptlen < GROUP_FILTER_SIZE(numsrc))
			return -EINVAL;
		kgf = compat_alloc_user_space(koptlen);
		if (!access_ok(VERIFY_WRITE, kgf, koptlen) ||
		if (!access_ok(kgf, koptlen) ||
		    __put_user(interface, &kgf->gf_interface) ||
		    __put_user(fmode, &kgf->gf_fmode) ||
		    __put_user(numsrc, &kgf->gf_numsrc) ||

int compat_mc_getsockopt(struct sock *sock, int level, int optname,		return getsockopt(sock, level, optname, optval, optlen);

	koptlen = compat_alloc_user_space(sizeof(*koptlen));
	if (!access_ok(VERIFY_READ, optlen, sizeof(*optlen)) ||
	if (!access_ok(optlen, sizeof(*optlen)) ||
	    __get_user(ulen, optlen))
		return -EFAULT;


int compat_mc_getsockopt(struct sock *sock, int level, int optname,	if (klen < GROUP_FILTER_SIZE(0))
		return -EINVAL;

	if (!access_ok(VERIFY_WRITE, koptlen, sizeof(*koptlen)) ||
	if (!access_ok(koptlen, sizeof(*koptlen)) ||
	    __put_user(klen, koptlen))
		return -EFAULT;

	/* have to allow space for previous compat_alloc_user_space, too */
	kgf = compat_alloc_user_space(klen+sizeof(*optlen));

	if (!access_ok(VERIFY_READ, gf32, __COMPAT_GF0_SIZE) ||
	if (!access_ok(gf32, __COMPAT_GF0_SIZE) ||
	    __get_user(interface, &gf32->gf_interface) ||
	    __get_user(fmode, &gf32->gf_fmode) ||
	    __get_user(numsrc, &gf32->gf_numsrc) ||

int compat_mc_getsockopt(struct sock *sock, int level, int optname,	if (err)
		return err;

	if (!access_ok(VERIFY_READ, koptlen, sizeof(*koptlen)) ||
	if (!access_ok(koptlen, sizeof(*koptlen)) ||
	    __get_user(klen, koptlen))
		return -EFAULT;

	ulen = klen - (sizeof(*kgf)-sizeof(*gf32));

	if (!access_ok(VERIFY_WRITE, optlen, sizeof(*optlen)) ||
	if (!access_ok(optlen, sizeof(*optlen)) ||
	    __put_user(ulen, optlen))
		return -EFAULT;

	if (!access_ok(VERIFY_READ, kgf, klen) ||
	    !access_ok(VERIFY_WRITE, gf32, ulen) ||
	if (!access_ok(kgf, klen) ||
	    !access_ok(gf32, ulen) ||
	    __get_user(interface, &kgf->gf_interface) ||
	    __get_user(fmode, &kgf->gf_fmode) ||
	    __get_user(numsrc, &kgf->gf_numsrc) ||

proc_dodebug(struct ctl_table *table, int write,	left = *lenp;

	if (write) {
		if (!access_ok(VERIFY_READ, buffer, left))
		if (!access_ok(buffer, left))
			return -EFAULT;
		p = buffer;
		while (left && __get_user(c, p) >= 0 && isspace(c))

ssize_t tomoyo_write_control(struct tomoyo_io_buffer *head,	int idx;
	if (!head->write)
		return -ENOSYS;
	if (!access_ok(VERIFY_READ, buffer, buffer_len))
	if (!access_ok(buffer, buffer_len))
		return -EFAULT;
	if (mutex_lock_interruptible(&head->io_sem))
		return -EINTR;

static ssize_t snd_seq_read(struct file *file, char __user *buf, size_t count,	if (!(snd_seq_file_flags(file) & SNDRV_SEQ_LFLG_INPUT))
		return -ENXIO;

	if (!access_ok(VERIFY_WRITE, buf, count))
	if (!access_ok(buf, count))
		return -EFAULT;

	/* check client structures are in place */

snd_emu8000_sample_new(struct snd_emux *rec, struct snd_sf_sample *sp,	}

	if (sp->v.mode_flags & SNDRV_SFNT_SAMPLE_8BITS) {
		if (!access_ok(VERIFY_READ, data, sp->v.size))
		if (!access_ok(data, sp->v.size))
			return -EFAULT;
	} else {
		if (!access_ok(VERIFY_READ, data, sp->v.size * 2))
		if (!access_ok(data, sp->v.size * 2))
			return -EFAULT;
	}



#define get_user	__get_user

#define access_ok(type, addr, size)	1
#define access_ok(addr, size)	1

#endif

int __kvm_set_memory_region(struct kvm *kvm,	/* We can read the guest memory with __xxx_user() later on. */
	if ((id < KVM_USER_MEM_SLOTS) &&
	    ((mem->userspace_addr & (PAGE_SIZE - 1)) ||
	     !access_ok(VERIFY_WRITE,
			(void __user *)(unsigned long)mem->userspace_addr,
	     !access_ok((void __user *)(unsigned long)mem->userspace_addr,
			mem->memory_size)))
		goto out;
	if (as_id >= KVM_ADDRESS_SPACE_NUM || id >= KVM_MEM_SLOTS_NUM)


