int bnxt_re_query_ah(struct ib_ah *ib_ah, struct rdma_ah_attr *ah_attr)	return 0;
}

static unsigned long bnxt_re_lock_cqs(struct bnxt_re_qp *qp)
unsigned long bnxt_re_lock_cqs(struct bnxt_re_qp *qp)
	__acquires(&qp->scq->cq_lock) __acquires(&qp->rcq->cq_lock)
{
	unsigned long flags;

static unsigned long bnxt_re_lock_cqs(struct bnxt_re_qp *qp)	return flags;
}

static void bnxt_re_unlock_cqs(struct bnxt_re_qp *qp,
			       unsigned long flags)
void bnxt_re_unlock_cqs(struct bnxt_re_qp *qp,
			unsigned long flags)
	__releases(&qp->scq->cq_lock) __releases(&qp->rcq->cq_lock)
{
	if (qp->rcq != qp->scq)

int bnxt_re_modify_qp(struct ib_qp *ib_qp, struct ib_qp_attr *qp_attr,	int status;
	union ib_gid sgid;
	struct ib_gid_attr sgid_attr;
	unsigned int flags;
	u8 nw_type;

	qp->qplib_qp.modify_flags = 0;

int bnxt_re_modify_qp(struct ib_qp *ib_qp, struct ib_qp_attr *qp_attr,			dev_dbg(rdev_to_dev(rdev),
				"Move QP = %p to flush list\n",
				qp);
			flags = bnxt_re_lock_cqs(qp);
			bnxt_qplib_add_flush_qp(&qp->qplib_qp);
			bnxt_re_unlock_cqs(qp, flags);
		}
		if (!qp->sumem &&
		    qp->qplib_qp.state == CMDQ_MODIFY_QP_NEW_STATE_RESET) {
			dev_dbg(rdev_to_dev(rdev),
				"Move QP = %p out of flush list\n",
				qp);
			flags = bnxt_re_lock_cqs(qp);
			bnxt_qplib_clean_qp(&qp->qplib_qp);
			bnxt_re_unlock_cqs(qp, flags);
		}
	}
	if (qp_attr_mask & IB_QP_EN_SQD_ASYNC_NOTIFY) {

struct ib_ucontext *bnxt_re_alloc_ucontext(struct ib_device *ibdev,					   struct ib_udata *udata);
int bnxt_re_dealloc_ucontext(struct ib_ucontext *context);
int bnxt_re_mmap(struct ib_ucontext *context, struct vm_area_struct *vma);

unsigned long bnxt_re_lock_cqs(struct bnxt_re_qp *qp);
void bnxt_re_unlock_cqs(struct bnxt_re_qp *qp, unsigned long flags);
#endif /* __BNXT_RE_IB_VERBS_H__ */

static int bnxt_re_handle_qp_async_event(struct creq_qp_event *qp_event,					 struct bnxt_re_qp *qp)
{
	struct ib_event event;
	unsigned int flags;

	if (qp->qplib_qp.state == CMDQ_MODIFY_QP_NEW_STATE_ERR) {
		flags = bnxt_re_lock_cqs(qp);
		bnxt_qplib_add_flush_qp(&qp->qplib_qp);
		bnxt_re_unlock_cqs(qp, flags);
	}

	memset(&event, 0, sizeof(event));
	if (qp->qplib_qp.srq) {

static void __bnxt_qplib_add_flush_qp(struct bnxt_qplib_qp *qp)	}
}

void bnxt_qplib_acquire_cq_locks(struct bnxt_qplib_qp *qp,
				 unsigned long *flags)
	__acquires(&qp->scq->hwq.lock) __acquires(&qp->rcq->hwq.lock)
static void bnxt_qplib_acquire_cq_flush_locks(struct bnxt_qplib_qp *qp,
				       unsigned long *flags)
	__acquires(&qp->scq->flush_lock) __acquires(&qp->rcq->flush_lock)
{
	spin_lock_irqsave(&qp->scq->hwq.lock, *flags);
	spin_lock_irqsave(&qp->scq->flush_lock, *flags);
	if (qp->scq == qp->rcq)
		__acquire(&qp->rcq->hwq.lock);
		__acquire(&qp->rcq->flush_lock);
	else
		spin_lock(&qp->rcq->hwq.lock);
		spin_lock(&qp->rcq->flush_lock);
}

void bnxt_qplib_release_cq_locks(struct bnxt_qplib_qp *qp,
				 unsigned long *flags)
	__releases(&qp->scq->hwq.lock) __releases(&qp->rcq->hwq.lock)
static void bnxt_qplib_release_cq_flush_locks(struct bnxt_qplib_qp *qp,
				       unsigned long *flags)
	__releases(&qp->scq->flush_lock) __releases(&qp->rcq->flush_lock)
{
	if (qp->scq == qp->rcq)
		__release(&qp->rcq->hwq.lock);
		__release(&qp->rcq->flush_lock);
	else
		spin_unlock(&qp->rcq->hwq.lock);
	spin_unlock_irqrestore(&qp->scq->hwq.lock, *flags);
}

static struct bnxt_qplib_cq *bnxt_qplib_find_buddy_cq(struct bnxt_qplib_qp *qp,
						      struct bnxt_qplib_cq *cq)
{
	struct bnxt_qplib_cq *buddy_cq = NULL;

	if (qp->scq == qp->rcq)
		buddy_cq = NULL;
	else if (qp->scq == cq)
		buddy_cq = qp->rcq;
	else
		buddy_cq = qp->scq;
	return buddy_cq;
}

static void bnxt_qplib_lock_buddy_cq(struct bnxt_qplib_qp *qp,
				     struct bnxt_qplib_cq *cq)
	__acquires(&buddy_cq->hwq.lock)
{
	struct bnxt_qplib_cq *buddy_cq = NULL;

	buddy_cq = bnxt_qplib_find_buddy_cq(qp, cq);
	if (!buddy_cq)
		__acquire(&cq->hwq.lock);
	else
		spin_lock(&buddy_cq->hwq.lock);
}

static void bnxt_qplib_unlock_buddy_cq(struct bnxt_qplib_qp *qp,
				       struct bnxt_qplib_cq *cq)
	__releases(&buddy_cq->hwq.lock)
{
	struct bnxt_qplib_cq *buddy_cq = NULL;

	buddy_cq = bnxt_qplib_find_buddy_cq(qp, cq);
	if (!buddy_cq)
		__release(&cq->hwq.lock);
	else
		spin_unlock(&buddy_cq->hwq.lock);
		spin_unlock(&qp->rcq->flush_lock);
	spin_unlock_irqrestore(&qp->scq->flush_lock, *flags);
}

void bnxt_qplib_add_flush_qp(struct bnxt_qplib_qp *qp)
{
	unsigned long flags;

	bnxt_qplib_acquire_cq_locks(qp, &flags);
	bnxt_qplib_acquire_cq_flush_locks(qp, &flags);
	__bnxt_qplib_add_flush_qp(qp);
	bnxt_qplib_release_cq_locks(qp, &flags);
	bnxt_qplib_release_cq_flush_locks(qp, &flags);
}

static void __bnxt_qplib_del_flush_qp(struct bnxt_qplib_qp *qp)

void bnxt_qplib_clean_qp(struct bnxt_qplib_qp *qp){
	unsigned long flags;

	bnxt_qplib_acquire_cq_locks(qp, &flags);
	bnxt_qplib_acquire_cq_flush_locks(qp, &flags);
	__clean_cq(qp->scq, (u64)(unsigned long)qp);
	qp->sq.hwq.prod = 0;
	qp->sq.hwq.cons = 0;

void bnxt_qplib_clean_qp(struct bnxt_qplib_qp *qp)	qp->rq.hwq.cons = 0;

	__bnxt_qplib_del_flush_qp(qp);
	bnxt_qplib_release_cq_locks(qp, &flags);
	bnxt_qplib_release_cq_flush_locks(qp, &flags);
}

static void bnxt_qpn_cqn_sched_task(struct work_struct *work)

void bnxt_qplib_mark_qp_error(void *qp_handle)	/* Must block new posting of SQ and RQ */
	qp->state = CMDQ_MODIFY_QP_NEW_STATE_ERR;
	bnxt_qplib_cancel_phantom_processing(qp);

	/* Add qp to flush list of the CQ */
	__bnxt_qplib_add_flush_qp(qp);
}

/* Note: SQE is valid from sw_sq_cons up to cqe_sq_cons (exclusive)

static int bnxt_qplib_cq_process_req(struct bnxt_qplib_cq *cq,				sw_sq_cons, cqe->wr_id, cqe->status);
			cqe++;
			(*budget)--;
			bnxt_qplib_lock_buddy_cq(qp, cq);
			bnxt_qplib_mark_qp_error(qp);
			bnxt_qplib_unlock_buddy_cq(qp, cq);
			/* Add qp to flush list of the CQ */
			bnxt_qplib_add_flush_qp(qp);
		} else {
			if (swq->flags & SQ_SEND_FLAGS_SIGNAL_COMP) {
				/* Before we complete, do WA 9060 */

static int bnxt_qplib_cq_process_res_rc(struct bnxt_qplib_cq *cq,		if (hwcqe->status != CQ_RES_RC_STATUS_OK) {
			qp->state = CMDQ_MODIFY_QP_NEW_STATE_ERR;
			/* Add qp to flush list of the CQ */
			bnxt_qplib_lock_buddy_cq(qp, cq);
			__bnxt_qplib_add_flush_qp(qp);
			bnxt_qplib_unlock_buddy_cq(qp, cq);
			bnxt_qplib_add_flush_qp(qp);
		}
	}


static int bnxt_qplib_cq_process_res_ud(struct bnxt_qplib_cq *cq,		if (hwcqe->status != CQ_RES_RC_STATUS_OK) {
			qp->state = CMDQ_MODIFY_QP_NEW_STATE_ERR;
			/* Add qp to flush list of the CQ */
			bnxt_qplib_lock_buddy_cq(qp, cq);
			__bnxt_qplib_add_flush_qp(qp);
			bnxt_qplib_unlock_buddy_cq(qp, cq);
			bnxt_qplib_add_flush_qp(qp);
		}
	}
done:

static int bnxt_qplib_cq_process_res_ud(struct bnxt_qplib_cq *cq,bool bnxt_qplib_is_cq_empty(struct bnxt_qplib_cq *cq)
{
	struct cq_base *hw_cqe, **hw_cqe_ptr;
	unsigned long flags;
	u32 sw_cons, raw_cons;
	bool rc = true;

	spin_lock_irqsave(&cq->hwq.lock, flags);
	raw_cons = cq->hwq.cons;
	sw_cons = HWQ_CMP(raw_cons, &cq->hwq);
	hw_cqe_ptr = (struct cq_base **)cq->hwq.pbl_ptr;

bool bnxt_qplib_is_cq_empty(struct bnxt_qplib_cq *cq)
	 /* Check for Valid bit. If the CQE is valid, return false */
	rc = !CQE_CMP_VALID(hw_cqe, raw_cons, cq->hwq.max_elements);
	spin_unlock_irqrestore(&cq->hwq.lock, flags);
	return rc;
}


static int bnxt_qplib_cq_process_res_raweth_qp1(struct bnxt_qplib_cq *cq,		if (hwcqe->status != CQ_RES_RC_STATUS_OK) {
			qp->state = CMDQ_MODIFY_QP_NEW_STATE_ERR;
			/* Add qp to flush list of the CQ */
			bnxt_qplib_lock_buddy_cq(qp, cq);
			__bnxt_qplib_add_flush_qp(qp);
			bnxt_qplib_unlock_buddy_cq(qp, cq);
			bnxt_qplib_add_flush_qp(qp);
		}
	}


static int bnxt_qplib_cq_process_terminal(struct bnxt_qplib_cq *cq,	 */

	/* Add qp to flush list of the CQ */
	bnxt_qplib_lock_buddy_cq(qp, cq);
	__bnxt_qplib_add_flush_qp(qp);
	bnxt_qplib_unlock_buddy_cq(qp, cq);
	bnxt_qplib_add_flush_qp(qp);
done:
	return rc;
}

int bnxt_qplib_process_flush_list(struct bnxt_qplib_cq *cq,	u32 budget = num_cqes;
	unsigned long flags;

	spin_lock_irqsave(&cq->hwq.lock, flags);
	spin_lock_irqsave(&cq->flush_lock, flags);
	list_for_each_entry(qp, &cq->sqf_head, sq_flush) {
		dev_dbg(&cq->hwq.pdev->dev,
			"QPLIB: FP: Flushing SQ QP= %p",

int bnxt_qplib_process_flush_list(struct bnxt_qplib_cq *cq,			qp);
		__flush_rq(&qp->rq, qp, &cqe, &budget);
	}
	spin_unlock_irqrestore(&cq->hwq.lock, flags);
	spin_unlock_irqrestore(&cq->flush_lock, flags);

	return num_cqes - budget;
}

int bnxt_qplib_poll_cq(struct bnxt_qplib_cq *cq, struct bnxt_qplib_cqe *cqe,		       int num_cqes, struct bnxt_qplib_qp **lib_qp)
{
	struct cq_base *hw_cqe, **hw_cqe_ptr;
	unsigned long flags;
	u32 sw_cons, raw_cons;
	int budget, rc = 0;

	spin_lock_irqsave(&cq->hwq.lock, flags);
	raw_cons = cq->hwq.cons;
	budget = num_cqes;


int bnxt_qplib_poll_cq(struct bnxt_qplib_cq *cq, struct bnxt_qplib_cqe *cqe,		bnxt_qplib_arm_cq(cq, DBR_DBR_TYPE_CQ);
	}
exit:
	spin_unlock_irqrestore(&cq->hwq.lock, flags);
	return num_cqes - budget;
}

void bnxt_qplib_req_notify_cq(struct bnxt_qplib_cq *cq, u32 arm_type)
{
	unsigned long flags;

	spin_lock_irqsave(&cq->hwq.lock, flags);
	if (arm_type)
		bnxt_qplib_arm_cq(cq, arm_type);
	/* Using cq->arm_state variable to track whether to issue cq handler */
	atomic_set(&cq->arm_state, 1);
	spin_unlock_irqrestore(&cq->hwq.lock, flags);
}

void bnxt_qplib_flush_cqn_wq(struct bnxt_qplib_qp *qp)

struct bnxt_qplib_cq {	struct list_head		sqf_head, rqf_head;
	atomic_t			arm_state;
	spinlock_t			compl_lock; /* synch CQ handlers */
/* Locking Notes:
 * QP can move to error state from modify_qp, async error event or error
 * CQE as part of poll_cq. When QP is moved to error state, it gets added
 * to two flush lists, one each for SQ and RQ.
 * Each flush list is protected by qplib_cq->flush_lock. Both scq and rcq
 * flush_locks should be acquired when QP is moved to error. The control path
 * operations(modify_qp and async error events) are synchronized with poll_cq
 * using upper level CQ locks (bnxt_re_cq->cq_lock) of both SCQ and RCQ.
 * The qplib_cq->flush_lock is required to synchronize two instances of poll_cq
 * of the same QP while manipulating the flush list.
 */
	spinlock_t			flush_lock; /* QP flush management */
};

#define BNXT_QPLIB_MAX_IRRQE_ENTRY_SIZE	sizeof(struct xrrq_irrq)

static int bnxt_qplib_process_qp_event(struct bnxt_qplib_rcfw *rcfw,			err_event->res_err_state_reason);
		if (!qp)
			break;
		bnxt_qplib_acquire_cq_locks(qp, &flags);
		bnxt_qplib_mark_qp_error(qp);
		bnxt_qplib_release_cq_locks(qp, &flags);
		rcfw->aeq_handler(rcfw, qp_event, qp);
		break;
	default:
		/* Command Response */


