tcp_app_win - INTEGER	buffer. Value 0 is special, it means that nothing is reserved.
	Default: 31

tcp_autocorking - BOOLEAN
	Enable TCP auto corking :
	When applications do consecutive small write()/sendmsg() system calls,
	we try to coalesce these small writes as much as possible, to lower
	total amount of sent packets. This is done if at least one prior
	packet for the flow is waiting in Qdisc queues or device transmit
	queue. Applications can still use TCP_CORK for optimal behavior
	when they know how/when to uncork their sockets.
	Default : 1

tcp_available_congestion_control - STRING
	Shows the available congestion control choices that are registered.
	More congestion control algorithms may be available as modules,

extern int sysctl_tcp_limit_output_bytes;extern int sysctl_tcp_challenge_ack_limit;
extern unsigned int sysctl_tcp_notsent_lowat;
extern int sysctl_tcp_min_tso_segs;
extern int sysctl_tcp_autocorking;

extern atomic_long_t tcp_memory_allocated;
extern struct percpu_counter tcp_sockets_allocated;

enum	LINUX_MIB_TCPFASTOPENCOOKIEREQD,	/* TCPFastOpenCookieReqd */
	LINUX_MIB_TCPSPURIOUS_RTX_HOSTQUEUES, /* TCPSpuriousRtxHostQueues */
	LINUX_MIB_BUSYPOLLRXPACKETS,		/* BusyPollRxPackets */
	LINUX_MIB_TCPAUTOCORKING,		/* TCPAutoCorking */
	__LINUX_MIB_MAX
};


static const struct snmp_mib snmp4_net_list[] = {	SNMP_MIB_ITEM("TCPFastOpenCookieReqd", LINUX_MIB_TCPFASTOPENCOOKIEREQD),
	SNMP_MIB_ITEM("TCPSpuriousRtxHostQueues", LINUX_MIB_TCPSPURIOUS_RTX_HOSTQUEUES),
	SNMP_MIB_ITEM("BusyPollRxPackets", LINUX_MIB_BUSYPOLLRXPACKETS),
	SNMP_MIB_ITEM("TCPAutoCorking", LINUX_MIB_TCPAUTOCORKING),
	SNMP_MIB_SENTINEL
};


static struct ctl_table ipv4_table[] = {		.extra1		= &zero,
		.extra2		= &gso_max_segs,
	},
	{
		.procname	= "tcp_autocorking",
		.data		= &sysctl_tcp_autocorking,
		.maxlen		= sizeof(int),
		.mode		= 0644,
		.proc_handler	= proc_dointvec_minmax,
		.extra1		= &zero,
		.extra2		= &one,
	},
	{
		.procname	= "udp_mem",
		.data		= &sysctl_udp_mem,

int sysctl_tcp_fin_timeout __read_mostly = TCP_FIN_TIMEOUT;
int sysctl_tcp_min_tso_segs __read_mostly = 2;

int sysctl_tcp_autocorking __read_mostly = 1;

struct percpu_counter tcp_orphan_count;
EXPORT_SYMBOL_GPL(tcp_orphan_count);


static inline void tcp_mark_urg(struct tcp_sock *tp, int flags)		tp->snd_up = tp->write_seq;
}

static inline void tcp_push(struct sock *sk, int flags, int mss_now,
			    int nonagle)
/* If a not yet filled skb is pushed, do not send it if
 * we have packets in Qdisc or NIC queues :
 * Because TX completion will happen shortly, it gives a chance
 * to coalesce future sendmsg() payload into this skb, without
 * need for a timer, and with no latency trade off.
 * As packets containing data payload have a bigger truesize
 * than pure acks (dataless) packets, the last check prevents
 * autocorking if we only have an ACK in Qdisc/NIC queues.
 */
static bool tcp_should_autocork(struct sock *sk, struct sk_buff *skb,
				int size_goal)
{
	if (tcp_send_head(sk)) {
		struct tcp_sock *tp = tcp_sk(sk);
	return skb->len < size_goal &&
	       sysctl_tcp_autocorking &&
	       atomic_read(&sk->sk_wmem_alloc) > skb->truesize;
}

static void tcp_push(struct sock *sk, int flags, int mss_now,
		     int nonagle, int size_goal)
{
	struct tcp_sock *tp = tcp_sk(sk);
	struct sk_buff *skb;

		if (!(flags & MSG_MORE) || forced_push(tp))
			tcp_mark_push(tp, tcp_write_queue_tail(sk));
	if (!tcp_send_head(sk))
		return;

	skb = tcp_write_queue_tail(sk);
	if (!(flags & MSG_MORE) || forced_push(tp))
		tcp_mark_push(tp, skb);

	tcp_mark_urg(tp, flags);

	if (tcp_should_autocork(sk, skb, size_goal)) {

		tcp_mark_urg(tp, flags);
		__tcp_push_pending_frames(sk, mss_now,
					  (flags & MSG_MORE) ? TCP_NAGLE_CORK : nonagle);
		/* avoid atomic op if TSQ_THROTTLED bit is already set */
		if (!test_bit(TSQ_THROTTLED, &tp->tsq_flags)) {
			NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPAUTOCORKING);
			set_bit(TSQ_THROTTLED, &tp->tsq_flags);
		}
		return;
	}

	if (flags & MSG_MORE)
		nonagle = TCP_NAGLE_CORK;

	__tcp_push_pending_frames(sk, mss_now, nonagle);
}

static int tcp_splice_data_recv(read_descriptor_t *rd_desc, struct sk_buff *skb,

static ssize_t do_tcp_sendpages(struct sock *sk, struct page *page, int offset,wait_for_sndbuf:
		set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
wait_for_memory:
		tcp_push(sk, flags & ~MSG_MORE, mss_now, TCP_NAGLE_PUSH);
		tcp_push(sk, flags & ~MSG_MORE, mss_now,
			 TCP_NAGLE_PUSH, size_goal);

		if ((err = sk_stream_wait_memory(sk, &timeo)) != 0)
			goto do_error;

static ssize_t do_tcp_sendpages(struct sock *sk, struct page *page, int offset,
out:
	if (copied && !(flags & MSG_SENDPAGE_NOTLAST))
		tcp_push(sk, flags, mss_now, tp->nonagle);
		tcp_push(sk, flags, mss_now, tp->nonagle, size_goal);
	return copied;

do_error:

int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,			set_bit(SOCK_NOSPACE, &sk->sk_socket->flags);
wait_for_memory:
			if (copied)
				tcp_push(sk, flags & ~MSG_MORE, mss_now, TCP_NAGLE_PUSH);
				tcp_push(sk, flags & ~MSG_MORE, mss_now,
					 TCP_NAGLE_PUSH, size_goal);

			if ((err = sk_stream_wait_memory(sk, &timeo)) != 0)
				goto do_error;

int tcp_sendmsg(struct kiocb *iocb, struct sock *sk, struct msghdr *msg,
out:
	if (copied)
		tcp_push(sk, flags, mss_now, tp->nonagle);
		tcp_push(sk, flags, mss_now, tp->nonagle, size_goal);
	release_sock(sk);
	return copied + copied_syn;



