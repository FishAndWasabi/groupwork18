ifdef CONFIG_SMPprepare: task_cpu_prepare

task_cpu_prepare: prepare0
	$(eval KBUILD_CFLAGS += -D_TASK_CPU=$(shell awk '{if ($$2 == "TI_CPU") print $$3;}' include/generated/asm-offsets.h))
	$(eval KBUILD_CFLAGS += -D_TASK_CPU=$(shell awk '{if ($$2 == "TASK_CPU") print $$3;}' include/generated/asm-offsets.h))
endif

# Check toolchain versions:


#ifdef CONFIG_PPC64
#define CURRENT_THREAD_INFO(dest, sp)	stringify_in_c(ld dest, PACACURRENT(r13))
#else
#define CURRENT_THREAD_INFO(dest, sp)	stringify_in_c(mr dest, r2)
#endif

#ifndef __ASSEMBLY__

int main(void)#endif /* CONFIG_PPC64 */
	OFFSET(TASK_STACK, task_struct, stack);
#ifdef CONFIG_SMP
	OFFSET(TI_CPU, task_struct, cpu);
	OFFSET(TASK_CPU, task_struct, cpu);
#endif

#ifdef CONFIG_LIVEPATCH

transfer_to_handler:	stw	r2,_XER(r11)
	mfspr	r12,SPRN_SPRG_THREAD
	addi	r2,r12,-THREAD
	tovirt(r2,r2)			/* set r2 to current */
	beq	2f			/* if from user, fix up THREAD.regs */
	addi	r11,r1,STACK_FRAME_OVERHEAD
	stw	r11,PT_REGS(r12)

transfer_to_handler:	lwz	r12,THREAD_DBCR0(r12)
	andis.	r12,r12,DBCR0_IDM@h
#endif
#ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
	CURRENT_THREAD_INFO(r9, r1)
	tophys(r9, r9)
	ACCOUNT_CPU_USER_ENTRY(r9, r11, r12)
#endif
	ACCOUNT_CPU_USER_ENTRY(r2, r11, r12)
#if defined(CONFIG_40x) || defined(CONFIG_BOOKE)
	beq+	3f
	/* From user and task is ptraced - load up global dbcr0 */

transfer_to_handler:	tophys(r11,r11)
	addi	r11,r11,global_dbcr0@l
#ifdef CONFIG_SMP
	CURRENT_THREAD_INFO(r9, r1)
	lwz	r9,TI_CPU(r9)
	lwz	r9,TASK_CPU(r2)
	slwi	r9,r9,3
	add	r11,r11,r9
#endif

transfer_to_handler:	ble-	stack_ovf		/* then the kernel stack overflowed */
5:
#if defined(CONFIG_PPC_BOOK3S_32) || defined(CONFIG_E500)
	CURRENT_THREAD_INFO(r9, r1)
	tophys(r9,r9)			/* check local flags */
	lwz	r12,TI_LOCAL_FLAGS(r9)
	lwz	r12,TI_LOCAL_FLAGS(r2)
	mtcrf	0x01,r12
	bt-	31-TLF_NAPPING,4f
	bt-	31-TLF_SLEEPING,7f

transfer_to_handler:transfer_to_handler_cont:
3:
	mflr	r9
	tovirt(r2, r2)			/* set r2 to current */
	lwz	r11,0(r9)		/* virtual address of handler */
	lwz	r9,4(r9)		/* where to go when done */
#if defined(CONFIG_PPC_8xx) && defined(CONFIG_PERF_EVENTS)

reenable_mmu:				/* re-enable mmu so we can */
#if defined (CONFIG_PPC_BOOK3S_32) || defined(CONFIG_E500)
4:	rlwinm	r12,r12,0,~_TLF_NAPPING
	stw	r12,TI_LOCAL_FLAGS(r9)
	stw	r12,TI_LOCAL_FLAGS(r2)
	b	power_save_ppc32_restore

7:	rlwinm	r12,r12,0,~_TLF_SLEEPING
	stw	r12,TI_LOCAL_FLAGS(r9)
	stw	r12,TI_LOCAL_FLAGS(r2)
	lwz	r9,_MSR(r11)		/* if sleeping, clear MSR.EE */
	rlwinm	r9,r9,0,~MSR_EE
	lwz	r12,_LINK(r11)		/* and return to address in LR */

_GLOBAL(DoSyscall)	mtmsr	r11
1:
#endif /* CONFIG_TRACE_IRQFLAGS */
	CURRENT_THREAD_INFO(r10, r1)
	lwz	r11,TI_FLAGS(r10)
	lwz	r11,TI_FLAGS(r2)
	andi.	r11,r11,_TIF_SYSCALL_DOTRACE
	bne-	syscall_dotrace
syscall_dotrace_cont:

ret_from_syscall:	lwz	r3,GPR3(r1)
#endif
	mr	r6,r3
	CURRENT_THREAD_INFO(r12, r1)
	/* disable interrupts so current_thread_info()->flags can't change */
	LOAD_MSR_KERNEL(r10,MSR_KERNEL)	/* doesn't include MSR_EE */
	/* Note: We don't bother telling lockdep about it */
	SYNC
	MTMSRD(r10)
	lwz	r9,TI_FLAGS(r12)
	lwz	r9,TI_FLAGS(r2)
	li	r8,-MAX_ERRNO
	andi.	r0,r9,(_TIF_SYSCALL_DOTRACE|_TIF_SINGLESTEP|_TIF_USER_WORK_MASK|_TIF_PERSYSCALL_MASK)
	bne-	syscall_exit_work

END_FTR_SECTION_IFSET(CPU_FTR_NEED_PAIRED_STWCX)#ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
	andi.	r4,r8,MSR_PR
	beq	3f
	CURRENT_THREAD_INFO(r4, r1)
	ACCOUNT_CPU_USER_EXIT(r4, r5, r7)
	ACCOUNT_CPU_USER_EXIT(r2, r5, r7)
3:
#endif
	lwz	r4,_LINK(r1)

syscall_exit_work:	/* Clear per-syscall TIF flags if any are set.  */

	li	r11,_TIF_PERSYSCALL_MASK
	addi	r12,r12,TI_FLAGS
	addi	r12,r2,TI_FLAGS
3:	lwarx	r8,0,r12
	andc	r8,r8,r11
#ifdef CONFIG_IBM405_ERR77

syscall_exit_work:#endif
	stwcx.	r8,0,r12
	bne-	3b
	subi	r12,r12,TI_FLAGS
	
4:	/* Anything which requires enabling interrupts? */
	andi.	r0,r9,(_TIF_SYSCALL_DOTRACE|_TIF_SINGLESTEP)

ret_from_except:
user_exc_return:		/* r10 contains MSR_KERNEL here */
	/* Check current_thread_info()->flags */
	CURRENT_THREAD_INFO(r9, r1)
	lwz	r9,TI_FLAGS(r9)
	lwz	r9,TI_FLAGS(r2)
	andi.	r0,r9,_TIF_USER_WORK_MASK
	bne	do_work


restore_user:	andis.	r10,r0,DBCR0_IDM@h
	bnel-	load_dbcr0
#endif
#ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
	CURRENT_THREAD_INFO(r9, r1)
	ACCOUNT_CPU_USER_EXIT(r9, r10, r11)
#endif
	ACCOUNT_CPU_USER_EXIT(r2, r10, r11)

	b	restore

/* N.B. the only way to get here is from the beq following ret_from_except. */
resume_kernel:
	/* check current_thread_info, _TIF_EMULATE_STACK_STORE */
	CURRENT_THREAD_INFO(r9, r1)
	lwz	r8,TI_FLAGS(r9)
	lwz	r8,TI_FLAGS(r2)
	andis.	r0,r8,_TIF_EMULATE_STACK_STORE@h
	beq+	1f


resume_kernel:
	/* Clear _TIF_EMULATE_STACK_STORE flag */
	lis	r11,_TIF_EMULATE_STACK_STORE@h
	addi	r5,r9,TI_FLAGS
	addi	r5,r2,TI_FLAGS
0:	lwarx	r8,0,r5
	andc	r8,r8,r11
#ifdef CONFIG_IBM405_ERR77

resume_kernel:
#ifdef CONFIG_PREEMPT
	/* check current_thread_info->preempt_count */
	lwz	r0,TI_PREEMPT(r9)
	lwz	r0,TI_PREEMPT(r2)
	cmpwi	0,r0,0		/* if non-zero, just restore regs and return */
	bne	restore
	andi.	r8,r8,_TIF_NEED_RESCHED

resume_kernel:	bl	trace_hardirqs_off
#endif
1:	bl	preempt_schedule_irq
	CURRENT_THREAD_INFO(r9, r1)
	lwz	r3,TI_FLAGS(r9)
	lwz	r3,TI_FLAGS(r2)
	andi.	r0,r3,_TIF_NEED_RESCHED
	bne-	1b
#ifdef CONFIG_TRACE_IRQFLAGS

load_dbcr0:	lis	r11,global_dbcr0@ha
	addi	r11,r11,global_dbcr0@l
#ifdef CONFIG_SMP
	CURRENT_THREAD_INFO(r9, r1)
	lwz	r9,TI_CPU(r9)
	lwz	r9,TASK_CPU(r2)
	slwi	r9,r9,3
	add	r11,r11,r9
#endif

recheck:	LOAD_MSR_KERNEL(r10,MSR_KERNEL)
	SYNC
	MTMSRD(r10)		/* disable interrupts */
	CURRENT_THREAD_INFO(r9, r1)
	lwz	r9,TI_FLAGS(r9)
	lwz	r9,TI_FLAGS(r2)
	andi.	r0,r9,_TIF_NEED_RESCHED
	bne-	do_resched
	andi.	r0,r9,_TIF_USER_WORK_MASK

#ifndef CONFIG_PPC64
/* epapr_ev_idle() was derived from e500_idle() */
_GLOBAL(epapr_ev_idle)
	CURRENT_THREAD_INFO(r3, r1)
	PPC_LL	r4, TI_LOCAL_FLAGS(r3)	/* set napping bit */
	PPC_LL	r4, TI_LOCAL_FLAGS(r2)	/* set napping bit */
	ori	r4, r4,_TLF_NAPPING	/* so when we take an exception */
	PPC_STL	r4, TI_LOCAL_FLAGS(r3)	/* it will return to our caller */
	PPC_STL	r4, TI_LOCAL_FLAGS(r2)	/* it will return to our caller */

	wrteei	1


set_ivor:	stwu	r0,THREAD_SIZE-STACK_FRAME_OVERHEAD(r1)

#ifdef CONFIG_SMP
	CURRENT_THREAD_INFO(r22, r1)
	stw	r24, TI_CPU(r22)
	stw	r24, TASK_CPU(r2)
#endif

	bl	early_init

finish_tlb_load:
	/* Get the next_tlbcam_idx percpu var */
#ifdef CONFIG_SMP
	lwz	r15, TI_CPU-THREAD(r12)
	lwz	r15, TASK_CPU-THREAD(r12)
	lis     r14, __per_cpu_offset@h
	ori     r14, r14, __per_cpu_offset@l
	rlwinm  r15, r15, 2, 0, 29

BEGIN_FTR_SECTION	DSSALL
	sync
END_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)
	CURRENT_THREAD_INFO(r9, r1)
	lwz	r8,TI_LOCAL_FLAGS(r9)	/* set napping bit */
	lwz	r8,TI_LOCAL_FLAGS(r2)	/* set napping bit */
	ori	r8,r8,_TLF_NAPPING	/* so when we take an exception */
	stw	r8,TI_LOCAL_FLAGS(r9)	/* it will return to our caller */
	stw	r8,TI_LOCAL_FLAGS(r2)	/* it will return to our caller */
	mfmsr	r7
	ori	r7,r7,MSR_EE
	oris	r7,r7,MSR_POW@h

_GLOBAL(power_save_ppc32_restore)	stw	r9,_NIP(r11)		/* make it do a blr */

#ifdef CONFIG_SMP
	CURRENT_THREAD_INFO(r12, r1)
	tophys(r12, r12)
	lwz	r11,TI_CPU(r12)		/* get cpu number * 4 */
	lwz	r11,TASK_CPU(r2)	/* get cpu number * 4 */
	slwi	r11,r11,2
#else
	li	r11,0

	.text

_GLOBAL(e500_idle)
	CURRENT_THREAD_INFO(r3, r1)
	lwz	r4,TI_LOCAL_FLAGS(r3)	/* set napping bit */
	lwz	r4,TI_LOCAL_FLAGS(r2)	/* set napping bit */
	ori	r4,r4,_TLF_NAPPING	/* so when we take an exception */
	stw	r4,TI_LOCAL_FLAGS(r3)	/* it will return to our caller */
	stw	r4,TI_LOCAL_FLAGS(r2)	/* it will return to our caller */

#ifdef CONFIG_PPC_E500MC
	wrteei	1

_GLOBAL(power_save_ppc32_restore)	stw	r9,_NIP(r11)		/* make it do a blr */

#ifdef CONFIG_SMP
	CURRENT_THREAD_INFO(r12, r1)
	lwz	r11,TI_CPU(r12)		/* get cpu number * 4 */
	lwz	r11,TASK_CPU(r2)		/* get cpu number * 4 */
	slwi	r11,r11,2
#else
	li	r11,0

_GLOBAL(low_choose_750fx_pll)
#ifdef CONFIG_SMP
	/* Store new HID1 image */
	CURRENT_THREAD_INFO(r6, r1)
	lwz	r6,TI_CPU(r6)
	lwz	r6,TASK_CPU(r2)
	slwi	r6,r6,2
#else
	li	r6, 0

_GLOBAL(add_hash_page)	add	r3,r3,r0		/* note create_hpte trims to 24 bits */

#ifdef CONFIG_SMP
	CURRENT_THREAD_INFO(r8, r1)	/* use cpu number to make tag */
	lwz	r8,TI_CPU(r8)		/* to go in mmu_hash_lock */
	lwz	r8,TASK_CPU(r2)		/* to go in mmu_hash_lock */
	oris	r8,r8,12
#endif /* CONFIG_SMP */


_GLOBAL(flush_hash_pages)#ifdef CONFIG_SMP
	lis	r9, (mmu_hash_lock - PAGE_OFFSET)@ha
	addi	r9, r9, (mmu_hash_lock - PAGE_OFFSET)@l
	CURRENT_THREAD_INFO(r8, r1)
	tophys(r8, r8)
	lwz	r8,TI_CPU(r8)
	lwz	r8,TASK_CPU(r2)
	oris	r8,r8,9
10:	lwarx	r0,0,r9
	cmpi	0,r0,0

EXPORT_SYMBOL(flush_hash_pages) */
_GLOBAL(_tlbie)
#ifdef CONFIG_SMP
	CURRENT_THREAD_INFO(r8, r1)
	lwz	r8,TI_CPU(r8)
	lwz	r8,TASK_CPU(r2)
	oris	r8,r8,11
	mfmsr	r10
	SYNC

_GLOBAL(_tlbie) */
_GLOBAL(_tlbia)
#if defined(CONFIG_SMP)
	CURRENT_THREAD_INFO(r8, r1)
	lwz	r8,TI_CPU(r8)
	lwz	r8,TASK_CPU(r2)
	oris	r8,r8,10
	mfmsr	r10
	SYNC

_GLOBAL(mpc6xx_enter_standby)	ori	r5, r5, ret_from_standby@l
	mtlr	r5

	CURRENT_THREAD_INFO(r5, r1)
	lwz	r6, TI_LOCAL_FLAGS(r5)
	lwz	r6, TI_LOCAL_FLAGS(r2)
	ori	r6, r6, _TLF_SLEEPING
	stw	r6, TI_LOCAL_FLAGS(r5)
	stw	r6, TI_LOCAL_FLAGS(r2)

	mfmsr	r5
	ori	r5, r5, MSR_EE


