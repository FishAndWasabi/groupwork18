static void __blk_mq_requeue_request(struct request *rq)
	if (blk_mq_request_started(rq)) {
		WRITE_ONCE(rq->state, MQ_RQ_IDLE);
		rq->rq_flags &= ~RQF_TIMED_OUT;
		if (q->dma_drain_size && blk_rq_bytes(rq))
			rq->nr_phys_segments--;
	}

EXPORT_SYMBOL(blk_mq_tag_to_rq);
static void blk_mq_rq_timed_out(struct request *req, bool reserved)
{
	req->rq_flags |= RQF_TIMED_OUT;
	if (req->q->mq_ops->timeout) {
		enum blk_eh_timer_return ret;


static void blk_mq_rq_timed_out(struct request *req, bool reserved)		WARN_ON_ONCE(ret != BLK_EH_RESET_TIMER);
	}

	req->rq_flags &= ~RQF_TIMED_OUT;
	blk_add_timer(req);
}


static bool blk_mq_req_expired(struct request *rq, unsigned long *next)
	if (blk_mq_rq_state(rq) != MQ_RQ_IN_FLIGHT)
		return false;
	if (rq->rq_flags & RQF_TIMED_OUT)
		return false;

	deadline = blk_rq_deadline(rq);
	if (time_after_eq(jiffies, deadline))

typedef __u32 __bitwise req_flags_t;#define RQF_ZONE_WRITE_LOCKED	((__force req_flags_t)(1 << 19))
/* already slept for hybrid poll */
#define RQF_MQ_POLL_SLEPT	((__force req_flags_t)(1 << 20))
/* ->timeout has been called, don't expire again */
#define RQF_TIMED_OUT		((__force req_flags_t)(1 << 21))

/* flags that prevent us from merging requests: */
#define RQF_NOMERGE_FLAGS \


